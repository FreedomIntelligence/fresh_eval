{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/milokarlsen/AI-trading-bot", "text_blocks": "### AL ###\n#Template for AL projects for Dynamics 365 Business Central\n#launch.json folder\n.vscode/\n#Cache folder\n.alcache/\n#Symbols folder\n.alpackages/\n#Snapshots folder\n.snapshots/\n#Testing Output folder\n.output/\n#Extension App-file\n*.app\n#Rapid Application Development File\nrad.json\n#Translation Base-file\n*.g.xlf\n#License-file\n*.flf\n#Test results file\nTestResults.xml\n\n\n# Development\n\nThis document describes the process for running this application on your local computer.\n\n## Getting started\n\nThis application is powered by Node.js & Docker compose!\n\nIt runs on macOS, and Linux environments. I did not test or run this application on Windows environment.\n\nYou'll need Node.js version 14. To install Node.js, [download the \"LTS\" installer from nodejs.org](https://nodejs.org). If you're using [`nodenv`](https://github.com/nodenv/nodenv), read the [`nodenv` docs](#nodenv) for instructions on switching Node.js versions.\n\nIn addition, you will need Docker Compose. To install Docker Compose, [refer steps from docker.com](https://docs.docker.com/compose/install/).\n\nOnce you've installed Node.js and Docker Compose, open Terminal and run the following:\n\n```sh\ngit clone https://github.com/chrisleekr/binance-trading-bot\nnpm install\ndocker-compose up -d --build\n```\n\nYou should now have a running server! Visit [localhost:8080](http://localhost:8080) in your browser. Unfortunately, it does not automatically refresh the change. Please refresh the browser to see the change.\n\nWhen you're ready to stop your local server, run the following:\n\n```sh\ndocker-compose down\n```\n\n## Branch naming conventions\n\nThe project enforces the branch name.\n\n```text\n  <type>/<branch name>\n     ‚îÇ      ‚îÇ\n     |      ‚îî‚îÄ> Summary in present tense. Not capitalized. No period at the end.\n     |\n     ‚îî‚îÄ> Type: chore, docs, feat, fix, refactor, style, or test.\n```\n\n### Example\n\n```text\n  feat/new-feature\n  fix/the-bug\n  docs/readme\n  style/update-button\n  refactor/change-variable\n  test/add-test\n  chore/some-small-thing\n  perf/improve-performance\n  build/update-build-step\n  ci/update-ci\n  revert/revert-commit\n  localize/add-korean-translation\n  bump/bump-version\n```\n\n| type        | description       |\n| ----------- | ----------------- |\n| `build`     | Changes that affect the build system or external dependencies (example scopes: gulp, broccoli, npm) |\n| `ci`        | Changes to our CI configuration files and scripts (example scopes: Travis, Circle, BrowserStack, SauceLabs) |\n| `chore`     | Updating grunt tasks etc; no production code change, Other changes that don't modify src or test files |\n| `docs`      | Changes to the documentation |\n| `feat`      | New feature for the user, not a new feature for build script |\n| `fix`       | Bug fix for the user, not a fix to a build script |\n| `perf`      | A code change that improves performance |\n| `refactor`  | Refactoring production code, eg. renaming a variable, A code change that neither fixes a bug nor adds a feature |\n| `revert`    | Reverts a previous commit |\n| `style`     | Formatting, missing semi colons, etc. (white-space, formatting, missing semi-colons, changes that do not affect the meaning of the code, etc); no production code change |\n| `test`      | Adding missing tests, refactoring tests; no production code change |\n\n## Commit message conventions\n\nThe project enforces commit message conventions. To know what patterns to use, please visit [commitlint](https://github.com/conventional-changelog/commitlint/#what-is-commitlint) and [conventional commits](https://www.conventionalcommits.org/en/v1.0.0/)\n\n```text\n  type(scope?): description  #scope is optional; multiple scopes are supported (current delimiter options: \"/\", \"\\\" and \",\")\n\n  [optional body]\n\n  [optional footer(s)]\n```\n\nAllowed types are:\n\n- `build`\n- `ci`\n- `chore`\n- `docs`\n- `feat`\n- `fix`\n- `perf`\n- `refactor`\n- `revert`\n- `style`\n- `test`\n\n\n\n# development stage\nFROM node:14-alpine AS dev-stage\n\nRUN apk add --no-cache make gcc g++ py-pip mongodb-tools redis\n\n# Add configuration files\nCOPY image-files/ /\n\nWORKDIR /srv\n\nCOPY package*.json ./\n\nRUN npm install\n\nCOPY . .\n\nARG PACKAGE_VERSION=untagged\nENV PACKAGE_VERSION=${PACKAGE_VERSION}\nLABEL com.chrisleekr.binance-trading-bot.package-version=${PACKAGE_VERSION}\n\nARG GIT_HASH=unspecified\nENV GIT_HASH=${GIT_HASH}\nLABEL com.chrisleekr.binance-trading-bot.git-hash=${GIT_HASH}\n\nARG NODE_ENV=development\nENV NODE_ENV=${NODE_ENV}\nLABEL com.chrisleekr.binance-trading-bot.node-env=${NODE_ENV}\n\nENTRYPOINT [ \"docker-entrypoint.sh\" ]\n\nCMD [ \"npm\", \"run\", \"dev\" ]\n\n# build stage\nFROM dev-stage AS build-stage\n\nRUN npm run build:webpack\n\nRUN npm run build:grunt\n\nRUN rm -rf node_modules\n\nRUN npm install --production\n\n# production stage\nFROM node:14-alpine AS production-stage\n\nRUN apk add --no-cache mongodb-tools redis\n\nARG PACKAGE_VERSION=untagged\nENV PACKAGE_VERSION=${PACKAGE_VERSION}\nLABEL com.chrisleekr.binance-trading-bot.package-version=${PACKAGE_VERSION}\n\nARG GIT_HASH=unspecified\nENV GIT_HASH=${GIT_HASH}\nLABEL com.chrisleekr.binance-trading-bot.git-hash=${GIT_HASH}\n\nARG NODE_ENV=production\nENV NODE_ENV=${NODE_ENV}\nLABEL com.chrisleekr.binance-trading-bot.node-env=${NODE_ENV}\n\n# Add configuration files\nCOPY image-files/ /\n\nWORKDIR /srv\n\nCOPY --from=build-stage /srv /srv\n\n# Copy index production HTML to index.html\nRUN cp /srv/public/index.html /srv/public/index.dev.html && \\\n  cp /srv/public/index.prod.html /srv/public/index.html\n\nENTRYPOINT [ \"docker-entrypoint.sh\" ]\n\nCMD [ \"npm\", \"start\"]\n\n\n\n/**\n * Gruntfile\n *  - compiling JSX files and concatenate them into a single file.\n *  - minifing CSS file\n *\n * @param {*} grunt\n */\nmodule.exports = grunt => {\n  // Project configuration.\n  grunt.initConfig({\n    // Compile Javascript files; so I don't need to use bable-standalone.\n    babel: {\n      options: {\n        sourceMap: false,\n        comments: false,\n        compact: true,\n        plugins: ['@babel/plugin-transform-react-jsx']\n      },\n      files: {\n        expand: true,\n        cwd: './public/js/',\n        src: ['**/*.js'],\n        dest: './public/dist/js/',\n        ext: '.min.js',\n        extDot: 'first'\n      }\n    },\n    // Concat all compiled files to single files.\n    //    All files in the src are not working.\n    //    The files should be listed in the src option in sequence.\n    //    Otherwise, it may not see undefined.\n    concat: {\n      options: {},\n      dist: {\n        src: [\n          './public/dist/js/Config.min.js',\n          './public/dist/js/HighlightChange.min.js',\n          './public/dist/js/CoinWrapperSellLastBuyPrice.min.js',\n          './public/dist/js/CoinWrapperSetting.min.js',\n          './public/dist/js/CoinWrapperSellOrders.min.js',\n          './public/dist/js/CoinWrapperSellSignal.min.js',\n          './public/dist/js/CoinWrapperBuyOrders.min.js',\n          './public/dist/js/CoinWrapperBuySignal.min.js',\n          './public/dist/js/SymbolManualTradeIcon.min.js',\n          './public/dist/js/CoinWrapperAction.min.js',\n          './public/dist/js/CoinWrapperBalance.min.js',\n          './public/dist/js/SymbolGridCalculator.min.js',\n          './public/dist/js/SymbolGridTradeArchiveIcon.min.js',\n          './public/dist/js/SymbolCancelIcon.min.js',\n          './public/dist/js/SymbolEnableActionIcon.min.js',\n          './public/dist/js/SymbolDeleteIcon.min.js',\n          './public/dist/js/SymbolEditLastBuyPriceIcon.min.js',\n          './public/dist/js/SymbolTriggerSellIcon.min.js',\n          './public/dist/js/SymbolTriggerBuyIcon.min.js',\n          './public/dist/js/SymbolSettingIconBotOptions.min.js',\n          './public/dist/js/SymbolSettingIconGridBuy.min.js',\n          './public/dist/js/SymbolSettingIconGridSell.min.js',\n          './public/dist/js/SymbolSettingIcon.min.js',\n          './public/dist/js/SymbolLogsIcon.min.js',\n          './public/dist/js/CoinWrapperSymbol.min.js',\n          './public/dist/js/CoinWrapperTradingView.min.js',\n          './public/dist/js/CoinWrapper.min.js',\n          './public/dist/js/DustTransferIcon.min.js',\n          './public/dist/js/ManualTradeIcon.min.js',\n          './public/dist/js/SettingIconActionBackupConfirmModal.min.js',\n          './public/dist/js/SettingIconActionRestoreConfirmModal.min.js',\n          './public/dist/js/SettingIconActionRestoreSuccessModal.min.js',\n          './public/dist/js/SettingIconActions.min.js',\n          './public/dist/js/SettingIconGridSell.min.js',\n          './public/dist/js/SettingIconGridBuy.min.js',\n          './public/dist/js/SettingIconBotOptions.min.js',\n          './public/dist/js/SettingIconLastBuyPriceRemoveThreshold.min.js',\n          './public/dist/js/SettingIcon.min.js',\n          './public/dist/js/AccountWrapperAsset.min.js',\n          './public/dist/js/AccountWrapper.min.js',\n          './public/dist/js/QuoteAssetGridTradeArchiveIcon.min.js',\n          './public/dist/js/ProfitLossWrapper.min.js',\n          './public/dist/js/OrderStats.min.js',\n          './public/dist/js/Status.min.js',\n          './public/dist/js/Footer.min.js',\n          './public/dist/js/FilterIcon.min.js',\n          './public/dist/js/LockIcon.min.js',\n          './public/dist/js/UnlockIcon.min.js',\n          './public/dist/js/Header.min.js',\n          './public/dist/js/APIError.min.js',\n          './public/dist/js/LockScreen.min.js',\n          './public/dist/js/AppSorting.min.js',\n          './public/dist/js/AppLoading.min.js',\n          './public/dist/js/App.min.js'\n        ],\n        dest: './public/dist/App.min.js'\n      }\n    },\n    cssmin: {\n      target: {\n        files: {\n          './public/dist/App.min.css': ['./public/css/App.css']\n        }\n      }\n    },\n    clean: ['./public/dist/js']\n  });\n\n  grunt.loadNpmTasks('grunt-babel');\n  grunt.loadNpmTasks('grunt-contrib-concat');\n  grunt.loadNpmTasks('grunt-contrib-cssmin');\n  grunt.loadNpmTasks('grunt-contrib-clean');\n\n  // Default task(s).\n  grunt.registerTask('default', ['babel', 'concat', 'cssmin', 'clean']);\n};\n\n\n\nMIT License\n\nCopyright (c) 2019 chrisleekr\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/keiyoushi/extensions", "text_blocks": "### Please give the repo a :star:\n\n| Build | Support Server |\n|-------|---------|\n| [![CI](https://github.com/keiyoushi/extensions-source/actions/workflows/build_push.yml/badge.svg)](https://github.com/keiyoushi/extensions-source/actions/workflows/build_push.yml) | [![Discord](https://img.shields.io/discord/1193460528052453448.svg?label=discord&labelColor=7289da&color=2c2f33&style=flat)](https://discord.gg/3FbCpdKbdY) |\n\n## Source Code\n\nhttps://github.com/keiyoushi/extensions-source\n\n## Report issues\n\nhttps://github.com/keiyoushi/extensions-source/issues/new/choose\n\n## Guide\n\n### One-click installation\nOne-click installation is only supported by these Tachiyomi versions:\n- Tachiyomi v0.15.2+\n- Tachiyomi Preview r6404+\n- TachiyomiSY v1.10.0+\n- TachiyomiSY Preview r539+\n\nNavigate to [the website](https://keiyoushi.github.io/extensions) and tap \"Add to Tachiyomi\", then restart the app.\n\n### TachiyomiAZ\n1. Go to Settings ‚Üí Browse\n2. Tap on \"Edit repos\" and then \"+\" button at bottom\n3. Input `keiyoushi/extensions`\n4. Enjoy!\n\n# Others\nIf you're not using any of the above fork then manually download and update extensions from [the listing page](https://keiyoushi.github.io/extensions/extensions)\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/linuxtips/MesDoKubernetes", "text_blocks": "# M√™s do Kubernetes - LINUXtips\n\nBem-vindo ao reposit√≥rio oficial do **M√™s do Kubernetes**, uma iniciativa da LINUXtips para proporcionar um m√™s inteiro de aprendizado intensivo, intera√ß√£o e crescimento profissional em torno do Kubernetes. Aqui, voc√™ encontrar√° todos os materiais de apoio, links para as grava√ß√µes das lives, desafios pr√°ticos e outros recursos √∫teis para maximizar sua experi√™ncia durante o evento.\n\n## Sobre o Evento\n\nO **M√™s do Kubernetes** √© um evento especial organizado pela LINUXtips, dedicado a explorar o universo do Kubernetes de forma pr√°tica e aprofundada. Durante este m√™s, voc√™ ter√° a oportunidade de:\n\n- Participar de lives interativas com especialistas no assunto.\n- Engajar-se em desafios pr√°ticos e testar suas habilidades em ambientes reais.\n- Ganhar certificados de conclus√£o ao assistir e participar ativamente das sess√µes.\n- Concorrer a sorteios e pr√™mios exclusivos.\n\n### Cronograma do Evento\n\n- **In√≠cio do Evento:** 6 de Janeiro de 2024, 10h (hor√°rio de Bras√≠lia)\n- **Sess√µes Semanais:** Todos os s√°bados de Janeiro, √†s 10h\n- **Sess√£o de Encerramento:** Data a ser anunciada\n\n## Materiais de Apoio\n\nAqui, voc√™ encontrar√° slides, c√≥digos-fonte, listas de leitura recomendadas e outros materiais de apoio usados durante as lives. Cada pasta corresponde a uma semana espec√≠fica do evento:\n\n- [Semana 1](/semana1)\n- [Semana 2](/semana2)\n- [Semana 3](/semana3)\n- [Semana 4](/semana4)\n\n## Desafios Pr√°ticos\n\nPara cada semana do evento, propomos desafios pr√°ticos para testar e aplicar seu aprendizado. Veja os detalhes dos desafios e como submeter suas solu√ß√µes:\n\n- [Desafio Semana 1](/desafios/semana1)\n\n## Certifica√ß√£o\n\nInstru√ß√µes sobre como obter seu certificado de conclus√£o estar√£o dispon√≠veis aqui ao final do evento.\nLembrando que somente ter√° acesso ao certificado quem participar ativamente das sess√µes ao vivo e assinar a lista de presen√ßa.\n\n## Suporte e Comunidade\n\nJunte-se ao nosso grupo do Telegram para discutir d√∫vidas, compartilhar insights e interagir com a comunidade:\n\n- [Grupo do Telegram da LINUXtips](https://t.me/canalLINUXtips)\n\n## Mantenha-se Conectado\n\nN√£o esque√ßa de seguir a LINUXtips nas redes sociais para atualiza√ß√µes cont√≠nuas e recursos adicionais:\n\n- [YouTube](https://www.youtube.com/linuxtips)\n- [Twitch](https://www.twitch.tv/linuxtips)\n- [Twitter](https://twitter.com/linuxtipsbr)\n- [Instagram](https://www.instagram.com/linuxtipsbr)\n- [Twitter do Jeferson Fernando](https://twitter.com/badtux_)\n- [Twitter do Jo√£o \"P0ssuidao\" Freire](https://twitter.com/p0ssuidao)\n- [LinkedIn do Jeferson Fernando](https://www.linkedin.com/in/jefersonfernando/)\n- [LinkedIn do Jo√£o \"P0ssuidao\" Freire](https://www.linkedin.com/in/joaopaulocunhafreire/)\n---\n\nAgradecemos sua participa√ß√£o e estamos empolgados para ver seu crescimento e engajamento durante o **M√™s do Kubernetes**. #VAIIII\n\n*Equipe LINUXtips*\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/hpcaitech/SwiftInfer", "text_blocks": "# üöÄ SwiftInfer\n\n## üîó Table of Contents\n\n- [üöÄ SwiftInfer](#-swiftinfer)\n  - [üîó Table of Contents](#-table-of-contents)\n  - [üìå Overview](#-overview)\n  - [üöó Quick Start](#-quick-start)\n    - [üõ† Installation](#-installation)\n    - [üïπ Run Llama example](#-run-llama-example)\n  - [‚öñÔ∏è Benchmark](#-benchmark)\n  - [üó∫ Roadmap](#-roadmap)\n  - [üìÉ Acknowledgement](#-acknowledgement)\n  - [üìù Citation](#-citation)\n\n## üìå Overview\n\n[**Streaming-LLM**](https://github.com/mit-han-lab/streaming-llm) is a technique to support infinite input length for LLM inference. It leverages [**Attention Sink**](https://arxiv.org/abs/2309.17453) to prevent the model collapse when the attention window shifts. The original work is implemented in PyTorch, we offer **SwiftInfer**, a TensorRT implementation to make StreamingLLM more production-grade. Our implementation was built upon the recently released [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) project.\n\n## üöó Quick Start\n\n### üõ† Installation\n\nWe use the API in [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) to construct the model and run inference. As the API of TensorRT-LLM is not stable and changing rapidly, we bind our implementation with the `42af740db51d6f11442fd5509ef745a4c043ce51` commit whose version is `v0.6.0`. We may upgrade this repository as TensorRT-LLM's APIs become more stable.\n\nIf you have build **TensorRT-LLM V0.6.0**, simply run:\n\n```bash\ngit clone https://github.com/hpcaitech/SwiftInfer.git\ncd SwiftInfer\npip install .\n```\n\nOtherwise, you should install TensorRT-LLM first.\n\n#### Install TensorRT-LLM with Docker\n\nIf using docker, you can follow [TensorRT-LLM Installation](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation.md) to install **TensorRT-LLM V0.6.0**.\n\nBy using docker, you can install SwiftInfer by simply running:\n\n```bash\ngit clone https://github.com/hpcaitech/SwiftInfer.git\ncd SwiftInfer\npip install .\n```\n\n#### Install TensorRT-LLM without Docker\n\nIf not using docker, we provide a script to install TensorRT-LLM automatically.\n\n**Prerequisites**\n\nPlease ensure that you have installed the following packages:\n\n- python\n- build essentials, including gcc/g++, make, cmake\n- CUDA toolkit\n- cuDNN\n- NCCL\n- TensorRT\n- PyTorch\n\nMake sure the version of TensorRT >= 9.1.0 and CUDA toolkit >= 12.2.\n\nTo install tensorrt:\n\n```bash\nARCH=$(uname -m)\nif [ \"$ARCH\" = \"arm64\" ];then ARCH=\"aarch64\";fi\nif [ \"$ARCH\" = \"amd64\" ];then ARCH=\"x86_64\";fi\nif [ \"$ARCH\" = \"aarch64\" ];then OS=\"ubuntu-22.04\"; else OS=\"linux\";fi\nwget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/secure/9.1.0/tars/tensorrt-9.1.0.4.$OS.$ARCH-gnu.cuda-12.2.tar.gz\ntar xzvf tensorrt-9.1.0.4.linux.x86_64-gnu.cuda-12.2.tar.gz\nPY_VERSION=$(python -c 'import sys; print(\".\".join(map(str, sys.version_info[0:2])))')\nPARSED_PY_VERSION=$(echo \"${PY_VERSION//./}\")\npip install TensorRT-9.1.0.4/python/tensorrt-*-cp${PARSED_PY_VERSION}-*.whl\nexport TRT_ROOT=$(realpath TensorRT-9.1.0.4)\n```\n\nTo download nccl, follow [NCCL download page](https://developer.nvidia.com/nccl/nccl-download).\n\nTo download cudnn, follow [cuDNN download page](https://developer.nvidia.com/rdp/cudnn-download).\n\n**Commands**\n\nBefore running the following commands, please ensure that you have set `nvcc` correctly. To check it, run:\n\n```bash\nnvcc --version\n```\n\nTo install TensorRT-LLM and SwiftInfer, run:\n\n```bash\ngit clone https://github.com/hpcaitech/SwiftInfer.git\ncd SwiftInfer\nTRT_ROOT=xxx NCCL_ROOT=xxx CUDNN_ROOT=xxx pip install .\n```\n\n### üïπ Run Llama example\n\nTo run the Llama example, you need to first clone the Hugging Face repository for the [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model or other Llama-based variants such as [lmsys/vicuna-7b-v1.3](https://huggingface.co/lmsys/vicuna-7b-v1.3). Then, you can run the following command to build the TensorRT engine. **You need to replace `<model-dir>` with the actual path to the Llama model.**\n\n```bash\ncd examples/llama\n\npython build.py \\\n--model_dir <model-dir> \\\n--dtype float16 \\\n--enable_context_fmha \\\n--use_gemm_plugin float16 \\\n--max_input_len 2048 \\\n--max_output_len 1024 \\\n--output_dir ./output/7B-streaming-8k-1k-4-2000/trt_engines/fp16/1-gpu/ \\\n--max_batch_size 1\n```\n\nNext, you need to download the [MT-Bench](https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md#mt-bench) data provided by [LMSYS-FastChat](https://github.com/lm-sys/FastChat).\n\n```bash\nmkdir mt_bench_data\nwget -P ./mt_bench_data https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/mt_bench/question.jsonl\n```\n\nFinally, you are ready to run the Llama example with the following command.\n\n‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è **Before that, please note that:**\n1. The `only_n_first` argument is used to control the number of samples to be evaluated. If you want to evaluate all samples, please remove this argument.\n\n```bash\npython ../run_conversation.py \\\n--max_input_length 2048 \\\n--max_output_len 1024 \\\n--tokenizer_dir <model-dir> \\\n--engine_dir ./output/7B-streaming-8k-1k-4-2000/trt_engines/fp16/1-gpu/ \\\n--input_file ./mt_bench_data/question.jsonl \\\n--streaming_llm_start_size 4 \\\n--only_n_first 5\n```\n\nYou should expect to see the generation out as follows:\n\n![generation output](./assets/inference-result.png)\n\n## ‚öñÔ∏è Benchmark\n\nWe have benchmarked our implementations of Streaming-LLM with the [original PyTorch version](https://github.com/mit-han-lab/streaming-llm). The benchmark command for our implementation is given in the [Run Llama Example](#üïπ-run-llama-example) section while that for the original PyTorch implementation is given in the [torch_streamingllm](./examples/torch_streamingllm/) folder. The hardware used is listed below:\n\n- GPU: Nvidia H800 (80GB)\n- CPU: Intel(R) Xeon(R) Platinum 8468\n- RAM: 2TB\n\nThe results (20 rounds of conversations) are:\n\n![performance](./assets/performance.jpg)\n\nWe are still working on further performance improvement and adapting to the TensorRT V0.7.1 APIs. We also notice that TensorRT-LLM has integrated StreamingLLM in their [example](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm) but it seems it is more suitable for single text generation instead of multi-round conversations. \n\n## üó∫ Roadmap\n\n- [x] Streaming-LLM attention implementation based on TRT-LLM APIs\n- [x] KV cache adaptation\n- [x] Early stop adaptation\n- [x] Contiguous tensor fix\n- [x] Llama example for multi-round conversation\n\n## üìÉ Acknowledgement\n\nThis work is inspired by Streaming-LLM to make it usable for production. Throughout development, we have referenced the following materials and we wish to acknowledge their efforts and contribution to the open-source community and academia.\n\n- Streaming-LLM\n    - [Paper](https://arxiv.org/abs/2309.17453)\n    - [Slides](https://github.com/mit-han-lab/streaming-llm/blob/main/assets/StreamingLLM.pdf)\n    - [GitHub Repository](https://github.com/mit-han-lab/streaming-llm)\n- TensorRT-LLM\n    - [Documentation](https://nvidia.github.io/TensorRT-LLM/)\n    - [GitHub Repository](https://github.com/NVIDIA/TensorRT-LLM)\n\n\n## üìù Citation\n\nIf you find StreamingLLM and our TensorRT implementation useful, please kindly cite our repository and the original work proposed by [Xiao et al.](https://github.com/Guangxuan-Xiao) from [MIT Han Lab](https://github.com/mit-han-lab).\n\n```bibtex\n# our repository\n# NOTE: the listed authors have equal contribution\n@misc{streamingllmtrt2023,\n  title = {SwiftInfer},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/hpcaitech/SwiftInfer}},\n}\n\n# Xiao's original paper\n@article{xiao2023streamingllm,\n        title={Efficient Streaming Language Models with Attention Sinks},\n        author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},\n        journal={arXiv},\n        year={2023}\n        }\n\n# TensorRT-LLM repo\n# as TensorRT-LLM team does not provide a bibtex\n# please let us know if there is any change needed\n@misc{trtllm2023,\n  title = {TensorRT-LLM},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/NVIDIA/TensorRT-LLM}},\n}\n```\nBasedOnStyle: Google\n\n\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/.build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# IDE\n.idea/\n.vscode/\n\n# macos\n*.DS_Store\n#data/\n\ndocs/.build\n\n# pytorch checkpoint\n*.pt\n\n# ignore version.py generated by setup.py\nswiftinfer/version.py\n\n# ignore any kernel build files\n.o\n.so\n\n# ignore python interface defition file\n.pyi\n\n# ignore coverage test file\ncoverage.lcov\ncoverage.xml\n\n# ignore testmon and coverage files\n.coverage\n.testmondata*\n\n############\n# ignore c++\n############\n# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n############\n# ignore cuda\n############\n*.i\n*.ii\n*.gpu\n*.ptx\n*.cubin\n*.fatbin\n\n############\n# ignore cmake\n############\nCMakeLists.txt.user\nCMakeCache.txt\nCMakeFiles\nCMakeScripts\nTesting\nMakefile\ncmake_install.cmake\ninstall_manifest.txt\ncompile_commands.json\nCTestTestfile.cmake\n_deps\n\n# cache\nexamples/*/output/\nexamples/*/benchmark/\nexamples/*/mt_bench_data/\nexamples/*/data/\n\n\n\n[submodule \"3rdparty/TensorRT-LLM\"]\n\tpath = 3rdparty/TensorRT-LLM\n\turl = https://github.com/NVIDIA/TensorRT-LLM.git\n\n\n\n[settings]\nline_length = 120\nmulti_line_output=3\ninclude_trailing_comma = true\nignore_comments = true\nprofile = black\nhonor_noqa = true\n\n\n\nrepos:\n\n  - repo: https://github.com/PyCQA/autoflake\n    rev: v2.2.1\n    hooks:\n      - id: autoflake\n        name: autoflake (python)\n        args: ['--in-place', '--remove-unused-variables', '--remove-all-unused-imports', '--ignore-init-module-imports']\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        name: sort all imports (python)\n\n  - repo: https://github.com/psf/black-pre-commit-mirror\n    rev: 23.9.1\n    hooks:\n    - id: black\n      name: black formatter\n      args: ['--line-length=120', '--target-version=py37', '--target-version=py38', '--target-version=py39','--target-version=py310']\n\n  - repo: https://github.com/pre-commit/mirrors-clang-format\n    rev: v13.0.1\n    hooks:\n    - id: clang-format\n      name: clang formatter\n      types_or: [c++, c]\n\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n      - id: check-yaml\n      - id: check-merge-conflict\n      - id: check-case-conflict\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n        args: ['--fix=lf']\n\n\n\ninclude *.txt README.md\nrecursive-include requirements *.txt\nrecursive-include swiftinfer/lib *.so *.dll\n\n\n\n[pytest]\naddopts = --ignore=3rdparty\n\n\n\nimport os\nimport subprocess\nfrom typing import List\n\nfrom setuptools import find_packages, setup\n\n\ndef fetch_requirements(path) -> List[str]:\n    \"\"\"\n    This function reads the requirements file.\n\n    Args:\n        path (str): the path to the requirements file.\n\n    Returns:\n        The lines in the requirements file.\n    \"\"\"\n    with open(path, \"r\") as fd:\n        return [r.strip() for r in fd.readlines()]\n\n\ndef fetch_readme() -> str:\n    \"\"\"\n    This function reads the README.md file in the current directory.\n\n    Returns:\n        The lines in the README file.\n    \"\"\"\n    with open(\"README.md\", encoding=\"utf-8\") as f:\n        return f.read()\n\n\ndef get_version() -> str:\n    \"\"\"\n    This function reads the version.txt and generates the colossalai/version.py file.\n\n    Returns:\n        The library version stored in version.txt.\n    \"\"\"\n\n    setup_file_path = os.path.abspath(__file__)\n    project_path = os.path.dirname(setup_file_path)\n    version_txt_path = os.path.join(project_path, \"version.txt\")\n    version_py_path = os.path.join(project_path, \"swiftinfer/version.py\")\n\n    with open(version_txt_path) as f:\n        version = f.read().strip()\n\n    # write version into version.py\n    with open(version_py_path, \"w\") as f:\n        f.write(f\"__version__ = '{version}'\\n\")\n\n    return version\n\n\ndef build_trt_llm() -> None:\n    try:\n        import tensorrt_llm # noqa\n    except ImportError:\n        print(\"TensorRT-LLM is not installed. Building it now...\")\n        script_path = os.path.join(os.path.dirname(__file__), \"scripts/build_trt_llm.sh\")\n        subprocess.run(f\"bash {script_path}\", shell=True, check=True)\n\n\nbuild_trt_llm()\n\nversion = get_version()\npackage_name = \"swiftinfer\"\n\nsetup(\n    name=package_name,\n    version=version,\n    packages=find_packages(\n        exclude=(\n            \"tests\",\n            \"build\",\n            \"dist\",\n            \"scripts\",\n            \"requirements\",\n            \"docs\",\n            \"examples\",\n            \"*.egg-info\",\n            \"docker\",\n        )\n    ),\n    description=\"An efficient inference system based on TensorRT.\",\n    long_description=fetch_readme(),\n    long_description_content_type=\"text/markdown\",\n    license=\"Apache Software License 2.0\",\n    url=\"https://www.colossalai.org\",\n    project_urls={\n        \"Github\": \"https://github.com/hpcaitech/ColossalAI-Inference\",\n    },\n    install_requires=fetch_requirements(\"requirements/requirements.txt\"),\n    python_requires=\">=3.6\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Environment :: GPU :: NVIDIA CUDA\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: System :: Distributed Computing\",\n    ],\n)\n\n\n\n0.0.1\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/keiyoushi/extensions-source", "text_blocks": "### Please give the repo a :star:\n\n| Build | Support Server |\n|-------|---------|\n| [![CI](https://github.com/keiyoushi/extensions-source/actions/workflows/build_push.yml/badge.svg)](https://github.com/keiyoushi/extensions-source/actions/workflows/build_push.yml) | [![Discord](https://img.shields.io/discord/1193460528052453448.svg?label=discord&labelColor=7289da&color=2c2f33&style=flat)](https://discord.gg/3FbCpdKbdY) |\n\n# Usage\n\nhttps://github.com/keiyoushi/extensions/blob/main/README.md\n\n# Contributing\n\nContributions are welcome!\n\nCheck out the repo's [issue backlog](https://github.com/keiyoushi/extensions-source/issues) for source requests and bug reports.\n\n## License\n\n    Copyright 2015 Javier Tom√°s\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n## Disclaimer\n\nThis project is not affiliated with Tachiyomi. Don't ask for help about these extensions at the official support means of Tachiyomi. All credits to the codebase goes to the original contributors.\n\n# Editor configuration, see https://editorconfig.org\nroot = true\n\n[*.kt]\ncharset = utf-8\nend_of_line = lf\nindent_size = 4\nindent_style = space\ninsert_final_newline = true\ntrim_trailing_whitespace = true\nij_kotlin_allow_trailing_comma = true\nij_kotlin_allow_trailing_comma_on_call_site = true\nij_kotlin_name_count_to_use_star_import = 2147483647\nij_kotlin_name_count_to_use_star_import_for_members = 2147483647\n\n[*.properties]\ncharset = utf-8\nend_of_line = lf\ninsert_final_newline = true\n\n\n\n.gradle\n/local.properties\n/.idea/workspace.xml\n.DS_Store\nbuild/\n/captures\n.idea/\n*.iml\nrepo/\napk/\ngen\ngenerated-src/\n\n\n\nbuildscript {\n    repositories {\n        mavenCentral()\n        google()\n        maven(url = \"https://plugins.gradle.org/m2/\")\n    }\n    dependencies {\n        classpath(libs.gradle.agp)\n        classpath(libs.gradle.kotlin)\n        classpath(libs.gradle.serialization)\n        classpath(libs.gradle.kotlinter)\n    }\n}\n\nallprojects {\n    repositories {\n        mavenCentral()\n        google()\n        maven(url = \"https://jitpack.io\")\n    }\n}\n\ntasks.register<Delete>(\"clean\") {\n    delete(rootProject.layout.buildDirectory.asFile.get())\n}\n\n\n\napply plugin: 'org.jmailen.kotlinter'\n\nandroid {\n    compileSdkVersion AndroidConfig.compileSdk\n\n    namespace \"eu.kanade.tachiyomi.extension\"\n    sourceSets {\n        main {\n            manifest.srcFile \"AndroidManifest.xml\"\n            java.srcDirs = ['src']\n            res.srcDirs = ['res']\n            assets.srcDirs = ['assets']\n        }\n        release {\n            manifest.srcFile \"AndroidManifest.xml\"\n        }\n        debug {\n            manifest.srcFile \"AndroidManifest.xml\"\n        }\n    }\n\n    defaultConfig {\n        minSdkVersion AndroidConfig.minSdk\n        targetSdkVersion AndroidConfig.targetSdk\n        applicationIdSuffix pkgNameSuffix\n        versionCode extVersionCode\n        versionName project.ext.properties.getOrDefault(\"libVersion\", \"1.4\") + \".$extVersionCode\"\n        setProperty(\"archivesBaseName\", \"tachiyomi-$pkgNameSuffix-v$versionName\")\n        def readmes = project.projectDir.listFiles({ File file ->\n            file.name == \"README.md\" || file.name == \"CHANGELOG.md\"\n        } as FileFilter)\n        def hasReadme = readmes != null && readmes.any { File file ->\n            file.name.startsWith(\"README\")\n        }\n        def hasChangelog = readmes != null && readmes.any { File file ->\n            file.name.startsWith(\"CHANGELOG\")\n        }\n        manifestPlaceholders = [\n                appName : \"Tachiyomi: $extName\",\n                extClass: extClass,\n                extFactory: project.ext.properties.getOrDefault(\"extFactory\", \"\"),\n                nsfw: project.ext.properties.getOrDefault(\"isNsfw\", false) ? 1 : 0,\n                hasReadme: hasReadme ? 1 : 0,\n                hasChangelog: hasChangelog ? 1 : 0,\n        ]\n    }\n\n    signingConfigs {\n        release {\n            storeFile rootProject.file(\"signingkey.jks\")\n            storePassword System.getenv(\"KEY_STORE_PASSWORD\")\n            keyAlias System.getenv(\"ALIAS\")\n            keyPassword System.getenv(\"KEY_PASSWORD\")\n        }\n    }\n\n    buildTypes {\n        release {\n            signingConfig signingConfigs.release\n            minifyEnabled false\n        }\n    }\n\n    dependenciesInfo {\n        includeInApk = false\n    }\n\n    buildFeatures {\n        // Disable unused AGP features\n        aidl false\n        renderScript false\n        resValues false\n        shaders false\n    }\n\n    compileOptions {\n        sourceCompatibility = JavaVersion.VERSION_1_8\n        targetCompatibility = JavaVersion.VERSION_1_8\n    }\n\n    kotlinOptions {\n        jvmTarget = JavaVersion.VERSION_1_8.toString()\n        freeCompilerArgs += \"-opt-in=kotlinx.serialization.ExperimentalSerializationApi\"\n    }\n\n    kotlinter {\n        experimentalRules = true\n        disabledRules = [\n            \"experimental:argument-list-wrapping\", // Doesn't play well with Android Studio\n            \"experimental:comment-wrapping\",\n        ]\n    }\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation(project(\":core\"))\n    compileOnly(libs.bundles.common)\n}\n\npreBuild.dependsOn(lintKotlin)\nlintKotlin.dependsOn(formatKotlin)\n\n\n\n# Project-wide Gradle settings.\n\n# IDE (e.g. Android Studio) users:\n# Gradle settings configured through the IDE *will override*\n# any settings specified in this file.\n\n# For more details on how to configure your build environment visit\n# http://www.gradle.org/docs/current/userguide/build_environment.html\n\n# Specifies the JVM arguments used for the daemon process.\n# The setting is particularly useful for tweaking memory settings.\norg.gradle.jvmargs=-Xmx6144m\n\n# When configured, Gradle will run in incubating parallel mode.\n# This option should only be used with decoupled projects. More details, visit\n# http://www.gradle.org/docs/current/userguide/multi_project_builds.html#sec:decoupled_projects\norg.gradle.parallel=true\norg.gradle.workers.max=5\n\norg.gradle.caching=true\n\n# Enable AndroidX dependencies\nandroid.useAndroidX=true\n\n\n\n@rem\n@rem Copyright 2015 the original author or authors.\n@rem\n@rem Licensed under the Apache License, Version 2.0 (the \"License\");\n@rem you may not use this file except in compliance with the License.\n@rem You may obtain a copy of the License at\n@rem\n@rem      https://www.apache.org/licenses/LICENSE-2.0\n@rem\n@rem Unless required by applicable law or agreed to in writing, software\n@rem distributed under the License is distributed on an \"AS IS\" BASIS,\n@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n@rem See the License for the specific language governing permissions and\n@rem limitations under the License.\n@rem\n\n@if \"%DEBUG%\"==\"\" @echo off\n@rem ##########################################################################\n@rem\n@rem  Gradle startup script for Windows\n@rem\n@rem ##########################################################################\n\n@rem Set local scope for the variables with windows NT shell\nif \"%OS%\"==\"Windows_NT\" setlocal\n\nset DIRNAME=%~dp0\nif \"%DIRNAME%\"==\"\" set DIRNAME=.\n@rem This is normally unused\nset APP_BASE_NAME=%~n0\nset APP_HOME=%DIRNAME%\n\n@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\nfor %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\n\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nset DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\n\n@rem Find java.exe\nif defined JAVA_HOME goto findJavaFromJavaHome\n\nset JAVA_EXE=java.exe\n%JAVA_EXE% -version >NUL 2>&1\nif %ERRORLEVEL% equ 0 goto execute\n\necho.\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\necho.\necho Please set the JAVA_HOME variable in your environment to match the\necho location of your Java installation.\n\ngoto fail\n\n:findJavaFromJavaHome\nset JAVA_HOME=%JAVA_HOME:\"=%\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\n\nif exist \"%JAVA_EXE%\" goto execute\n\necho.\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%\necho.\necho Please set the JAVA_HOME variable in your environment to match the\necho location of your Java installation.\n\ngoto fail\n\n:execute\n@rem Setup the command line\n\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\n\n\n@rem Execute Gradle\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %*\n\n:end\n@rem End local scope for the variables with windows NT shell\nif %ERRORLEVEL% equ 0 goto mainEnd\n\n:fail\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\nrem the _cmd.exe /c_ return code!\nset EXIT_CODE=%ERRORLEVEL%\nif %EXIT_CODE% equ 0 set EXIT_CODE=1\nif not \"\"==\"%GRADLE_EXIT_CONSOLE%\" exit %EXIT_CODE%\nexit /b %EXIT_CODE%\n\n:mainEnd\nif \"%OS%\"==\"Windows_NT\" endlocal\n\n:omega\n\n\n\n<code_scheme name=\"Project\" version=\"173\">\n  <JetCodeStyleSettings>\n    <option name=\"PACKAGES_TO_USE_STAR_IMPORTS\">\n      <value>\n        <package name=\"kotlinx.android.synthetic\" withSubpackages=\"true\" static=\"false\" />\n      </value>\n    </option>\n    <option name=\"NAME_COUNT_TO_USE_STAR_IMPORT\" value=\"2147483647\" />\n    <option name=\"NAME_COUNT_TO_USE_STAR_IMPORT_FOR_MEMBERS\" value=\"2147483647\" />\n    <option name=\"CODE_STYLE_DEFAULTS\" value=\"KOTLIN_OFFICIAL\" />\n  </JetCodeStyleSettings>\n  <codeStyleSettings language=\"XML\">\n    <indentOptions>\n      <option name=\"CONTINUATION_INDENT_SIZE\" value=\"4\" />\n    </indentOptions>\n    <arrangement>\n      <rules>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>xmlns:android</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>xmlns:.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>BY_NAME</order>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*:id</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>http://schemas.android.com/apk/res/android</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*:name</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>http://schemas.android.com/apk/res/android</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>name</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>style</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>BY_NAME</order>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>http://schemas.android.com/apk/res/android</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>ANDROID_ATTRIBUTE_ORDER</order>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>.*</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>BY_NAME</order>\n          </rule>\n        </section>\n      </rules>\n    </arrangement>\n  </codeStyleSettings>\n  <codeStyleSettings language=\"kotlin\">\n    <option name=\"CODE_STYLE_DEFAULTS\" value=\"KOTLIN_OFFICIAL\" />\n    <option name=\"LINE_COMMENT_AT_FIRST_COLUMN\" value=\"false\" />\n    <option name=\"LINE_COMMENT_ADD_SPACE\" value=\"true\" />\n    <option name=\"KEEP_BLANK_LINES_IN_DECLARATIONS\" value=\"1\" />\n    <option name=\"KEEP_BLANK_LINES_IN_CODE\" value=\"1\" />\n    <option name=\"KEEP_BLANK_LINES_BEFORE_RBRACE\" value=\"0\" />\n    <option name=\"ALIGN_MULTILINE_PARAMETERS\" value=\"false\" />\n    <indentOptions>\n      <option name=\"CONTINUATION_INDENT_SIZE\" value=\"4\" />\n    </indentOptions>\n  </codeStyleSettings>\n</code_scheme>\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/kyegomez/MultiModalMamba", "text_blocks": "[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n# Multi Modal Mamba - [MMM]\nMulti Modal Mamba (MMM) is an all-new AI model that integrates Vision Transformer (ViT) and Mamba, creating a high-performance multi-modal model. MMM is built on Zeta, a minimalist yet powerful AI framework, designed to streamline and enhance machine learning model management. \n\nThe capacity to process and interpret multiple data types concurrently is essential, the world isn't 1dimensional. MMM addresses this need by leveraging the capabilities of Vision Transformer and Mamba, enabling efficient handling of both text and image data. This makes MMM a versatile solution for a broad spectrum of AI tasks. MMM stands out for its significant speed and efficiency improvements over traditional transformer architectures, such as GPT-4 and LLAMA. This enhancement allows MMM to deliver high-quality results without sacrificing performance, making it an optimal choice for real-time data processing and complex AI algorithm execution. A key feature of MMM is its proficiency in processing extremely long sequences.\n\nThis capability is particularly beneficial for tasks that involve substantial data volumes or necessitate a comprehensive understanding of context, such as natural language processing or image recognition. With MMM, you're not just adopting a state-of-the-art AI model. You're integrating a fast, efficient, and robust tool that is equipped to meet the demands of contemporary AI tasks. Experience the power and versatility of Multi Modal Mamba - MMM now!\n\n## Install\n`pip3 install mmm-zeta`\n\n\n## Usage\n\n### `MultiModalMambaBlock`\n\n\n```python\n# Import the necessary libraries\nimport torch \nfrom torch import nn\nfrom mm_mamba import MultiModalMambaBlock\n\n# Create some random input tensors\nx = torch.randn(1, 16, 64)  # Tensor with shape (batch_size, sequence_length, feature_dim)\ny = torch.randn(1, 3, 64, 64)  # Tensor with shape (batch_size, num_channels, image_height, image_width)\n\n# Create an instance of the MultiModalMambaBlock model\nmodel = MultiModalMambaBlock(\n    dim = 64,  # Dimension of the token embeddings\n    depth = 5,  # Number of transformer layers\n    dropout = 0.1,  # Dropout probability\n    heads = 4,  # Number of attention heads\n    d_state = 16,  # Dimension of the state embeddings\n    image_size = 64,  # Size of the input image\n    patch_size = 16,  # Size of each image patch\n    encoder_dim = 64,  # Dimension of the encoder token embeddings\n    encoder_depth = 5,  # Number of encoder transformer layers\n    encoder_heads = 4,  # Number of encoder attention heads\n    fusion_method=\"mlp\",\n)\n\n# Pass the input tensors through the model\nout = model(x, y)\n\n# Print the shape of the output tensor\nprint(out.shape)\n\n```\n\n\n### `MMM`, Ready to Train Model\n- Flexibility in Data Types: The MMM model can handle both text and image data simultaneously. This allows it to be trained on a wider variety of datasets and tasks, including those that require understanding of both text and image data.\n\n- Customizable Architecture: The MMM model has numerous parameters such as depth, dropout, heads, d_state, image_size, patch_size, encoder_dim, encoder_depth, encoder_heads, and fusion_method. These parameters can be tuned according to the specific requirements of the task at hand, allowing for a high degree of customization in the model architecture.\n\n- Option to Return Embeddings: The MMM model has a return_embeddings option. When set to True, the model will return the embeddings instead of the final output. This can be useful for tasks that require access to the intermediate representations learned by the model, such as transfer learning or feature extraction tasks.\n\n```python\nimport torch  # Import the torch library\n\n# Import the MMM model from the mm_mamba module\nfrom mm_mamba import MMM\n\n# Generate a random tensor 'x' of size (1, 224) with random elements between 0 and 10000\nx = torch.randint(0, 10000, (1, 196))\n\n# Generate a random image tensor 'img' of size (1, 3, 224, 224)\nimg = torch.randn(1, 3, 224, 224)\n\n# Create a MMM model object with the following parameters:\nmodel = MMM(\n    vocab_size=10000,\n    dim=512,\n    depth=6,\n    dropout=0.1,\n    heads=8,\n    d_state=512,\n    image_size=224,\n    patch_size=16,\n    encoder_dim=512,\n    encoder_depth=6,\n    encoder_heads=8,\n    fusion_method=\"mlp\",\n    return_embeddings=False,\n    post_fuse_norm=True,\n)\n\n# Pass the tensor 'x' and 'img' through the model and store the output in 'out'\nout = model(x, img)\n\n# Print the shape of the output tensor 'out'\nprint(out.shape)\n\n\n# After much training\nmodel.eval()\n\n# Tokenize texts\ntext_tokens = tokenize(text)\n\n# Send text tokens to the model\nlogits = model(text_tokens)\n\ntext = detokenize(logits)\n```\n\n# Real-World Deployment\n\nAre you an enterprise looking to leverage the power of AI? Do you want to integrate state-of-the-art models into your workflow? Look no further!\n\nMulti Modal Mamba (MMM) is a cutting-edge AI model that fuses Vision Transformer (ViT) with Mamba, providing a fast, agile, and high-performance solution for your multi-modal needs. \n\nBut that's not all! With Zeta, our simple yet powerful AI framework, you can easily customize and fine-tune MMM to perfectly fit your unique quality standards. \n\nWhether you're dealing with text, images, or both, MMM has got you covered. With its deep configuration and multiple fusion layers, you can handle complex AI tasks with ease and efficiency.\n\n### :star2: Why Choose Multi Modal Mamba?\n\n- **Versatile**: Handle both text and image data with a single model.\n- **Powerful**: Leverage the power of Vision Transformer and Mamba.\n- **Customizable**: Fine-tune the model to your specific needs with Zeta.\n- **Efficient**: Achieve high performance without compromising on speed.\n\nDon't let the complexities of AI slow you down. Choose Multi Modal Mamba and stay ahead of the curve!\n\n[Contact us here](https://calendly.com/swarm-corp/30min) today to learn how you can integrate Multi Modal Mamba into your workflow and supercharge your AI capabilities!\n\n---\n\n\n# License\nMIT\n\n\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n.vscode/\n.vscode\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\n.ruff_cache/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n\n\nrepos:\n  - repo: https://github.com/ambv/black\n    rev: 22.3.0\n    hooks:\n    - id: black\n  - repo: https://github.com/charliermarsh/ruff-pre-commit\n    rev: 'v0.0.255'\n    hooks:\n      - id: ruff\n        args: [--fix]\n  - repo: https://github.com/nbQA-dev/nbQA\n    rev: 1.6.3\n    hooks:\n    - id: nbqa-black\n      additional_dependencies: [ipython==8.12, black]\n    - id: nbqa-ruff \n      args: [\"--ignore=I001\"]\n      additional_dependencies: [ipython==8.12, ruff]\n\n\nMIT License\n\nCopyright (c) 2023 Eternal Reclaimer\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n\n# Import the necessary libraries\nimport torch\nfrom mm_mamba import MultiModalMambaBlock\n\n# Create some random input tensors\nx = torch.randn(\n    1, 16, 64\n)  # Tensor with shape (batch_size, sequence_length, feature_dim)\ny = torch.randn(\n    1, 3, 64, 64\n)  # Tensor with shape (batch_size, num_channels, image_height, image_width)\n\n# Create an instance of the MultiModalMambaBlock model\nmodel = MultiModalMambaBlock(\n    dim=64,  # Dimension of the token embeddings\n    depth=5,  # Number of transformer layers\n    dropout=0.1,  # Dropout probability\n    heads=4,  # Number of attention heads\n    d_state=16,  # Dimension of the state embeddings\n    image_size=64,  # Size of the input image\n    patch_size=16,  # Size of each image patch\n    encoder_dim=64,  # Dimension of the encoder token embeddings\n    encoder_depth=5,  # Number of encoder transformer layers\n    encoder_heads=4,  # Number of encoder attention heads\n    fusion_method=\"mlp\",\n)\n\n# Pass the input tensors through the model\nout = model(x, y)\n\n# Print the shape of the output tensor\nprint(out.shape)\n\n\n\nimport torch  # Import the torch library\n\n# Import the MMM model from the mm_mamba module\nfrom mm_mamba import MMM\n\n# Generate a random tensor 'x' of size (1, 224) with random elements between 0 and 10000\nx = torch.randint(0, 10000, (1, 196))\n\n# Generate a random image tensor 'img' of size (1, 3, 224, 224)\nimg = torch.randn(1, 3, 224, 224)\n\n# Create a MMM model object with the following parameters:\nmodel = MMM(\n    vocab_size=10000,\n    dim=512,\n    depth=6,\n    dropout=0.1,\n    heads=8,\n    d_state=512,\n    image_size=224,\n    patch_size=16,\n    encoder_dim=512,\n    encoder_depth=6,\n    encoder_heads=8,\n    fusion_method=\"mlp\",\n    return_embeddings=False,\n    post_fuse_norm=True,\n)\n\n# Pass the tensor 'x' and 'img' through the model and store the output in 'out'\nout = model(x, img)\n\n# Print the shape of the output tensor 'out'\nprint(out.shape)\n\n\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry]\nname = \"mmm-zeta\"\nversion = \"0.1.0\"\ndescription = \"MMM - Pytorch\"\nlicense = \"MIT\"\nauthors = [\"Kye Gomez <kye@apac.ai>\"]\nhomepage = \"https://github.com/kyegomez/MultiModalMamba\"\ndocumentation = \"https://github.com/kyegomez/MultiModalMamba\"  # Add this if you have documentation.\nreadme = \"README.md\"  # Assuming you have a README.md\nrepository = \"https://github.com/kyegomez/MultiModalMamba\"\nkeywords = [\"artificial intelligence\", \"deep learning\", \"optimizers\", \"Prompt Engineering\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3.9\"\n]\npackages = [\n    { include = \"mm_mamba\" },\n    { include = \"mm_mamba/**/*.py\" },\n]\n\n[tool.poetry.dependencies]\npython = \"^3.6\"\ntorch = \"2.1.2\"\nzetascale = \"1.4.0\"\neinops = \"*\"\n\n\n[tool.poetry.group.lint.dependencies]\nruff = \"^0.1.6\"\ntypes-toml = \"^0.10.8.1\"\ntypes-redis = \"^4.3.21.6\"\ntypes-pytz = \"^2023.3.0.0\"\nblack = \"^23.1.0\"\ntypes-chardet = \"^5.0.4.6\"\nmypy-protobuf = \"^3.0.0\"\n\n\n[tool.autopep8]\nmax_line_length = 80\nignore = \"E501,W6\"  # or [\"E501\", \"W6\"]\nin-place = true\nrecursive = true\naggressive = 3\n\n\n[tool.ruff]\nline-length = 70\n\n[tool.black]\nline-length = 70\ntarget-version = ['py38']\npreview = true\n\n\n\ntorch==2.1.2\nzetascale==1.4.0\nswarms==3.4.2\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/ProgrammingHero1/B9A1-New-Year-New-Mission", "text_blocks": ""}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/linkdd/aitoolkit", "text_blocks": "# AI Toolkit\n\n<center>\n\n![tests](https://img.shields.io/github/actions/workflow/status/linkdd/aitoolkit/tests.yml?style=flat-square&logo=github&label=tests)\n![docs](https://img.shields.io/github/actions/workflow/status/linkdd/aitoolkit/docs.yml?style=flat-square&logo=github&label=docs)\n![license](https://img.shields.io/github/license/linkdd/aitoolkit?style=flat-square&color=blue)\n![version](https://img.shields.io/github/v/release/linkdd/aitoolkit?style=flat-square&color=red)\n\n</center>\n\n**AI Toolkit** is a header-only C++ library which provides tools for building\nthe brain of your game's NPCs.\n\nIt provides:\n\n - Finite State Machines\n - Behavior Tree\n - Utility AI\n - Goal Oriented Action Planning\n\nWhy this project? Well, I wrote about it [here](https://david-delassus.medium.com/ai-toolkit-give-a-brain-to-your-npcs-a-header-only-c-library-02a50ae9faed?sk=011cd1ed8e61d22f1be6b6430847f430).\n\n## Installation\n\nAdd the `include` folder of this repository to your include paths.\n\nOr add it as a submodule:\n\n```\n$ git submodule add https://github.com/linkdd/aitoolkit.git\n$ g++ -std=c++23 -Iaiotoolkit/include main.cpp -o mygame\n```\n\n> **NB:** This library is compatible with C++20.\n\n## Usage\n\n### Finite State Machine\n\nFirst, include the header:\n\n```cpp\n#include <aitoolkit/fsm.hpp>\n\nusing namespace aitoolkit::fsm;\n```\n\nThen, create your blackboard type:\n\n```cpp\nstruct blackboard_type {\n  // ...\n};\n```\n\nThen, create a state type for each of your states:\n\n```cpp\nclass state_dummy final : public state<blackboard_type> {\n  public:\n    virtual void enter(blackboard_type& blackboard) override {\n      // ...\n    }\n\n    virtual void exit(blackboard_type& blackboard) override {\n      // ...\n    }\n\n    virtual void pause(blackboard_type& blackboard) override {\n      // ...\n    }\n\n    virtual void resume(blackboard_type& blackboard) override {\n      // ...\n    }\n\n    virtual void update(blackboard_type& blackboard) override {\n      // ...\n    }\n};\n```\n\nCreate your simple state machine:\n\n```cpp\nauto simple_bb = blackboard_type{};\nauto simple_fsm = simple_machine<blackboard_type>();\n\nsimple_fsm.set_state(std::make_shared<state_dummy>(), simple_bb);\nsimple_fsm.pause(simple_bb);\nsimple_fsm.resume(simple_bb);\nsimple_fsm.update(simple_bb);\n```\n\nOr with a stack state machine:\n\n```cpp\nauto stack_bb = blackboard_type{};\nauto stack_fsm = stack_machine<blackboard_type>{};\n\nstack_fsm.push_state(std::make_shared<state_dummy>(), stack_bb);\nstack_fsm.push_state(std::make_shared<state_dummy>(), stack_bb);\n\nstack_fsm.update(stack_bb);\n\nstack_fsm.pop_state(stack_bb);\nstack_fsm.pop_state(stack_bb);\n```\n\n### Behavior Tree\n\nFirst, include the header:\n\n```cpp\n#include <aitoolkit/behtree.hpp>\n\nusing namespace aitoolkit::bt;\n```\n\nThen, create your blackboard type:\n\n```cpp\nstruct blackboard_type {\n  // ...\n};\n```\n\nThen, create your tree:\n\n```cpp\nauto tree = seq<blackboard_type>::make({\n  check<blackboard_type>::make([](const blackboard_type& bb) {\n    // check some condition\n    return true;\n  }),\n  task<blackboard_type>::make([](blackboard_type& bb) {\n    // perform some action\n    return execution_state::success;\n  })\n});\n```\n\nFinally, evaluate it:\n\n```cpp\nauto blackboard = blackboard_type{\n  // ...\n};\n\nauto state = tree->evaluate(blackboard);\n```\n\nFor more informations, consult the\n[documentation](https://linkdd.github.io/aitoolkit/group__behtree.html).\n\n### Utility AI\n\nFirst, include the header file:\n\n```cpp\n#include <aitoolkit/utility.hpp>\n\nusing namespace aitoolkit::utility;\n```\n\nThen, create a blackboard type:\n\n```cpp\nstruct blackboard_type {\n  int food{0};\n  int wood{0};\n  int stone{0};\n  int gold{0};\n};\n```\n\nNext, create a class for each action that you want to be able to perform:\n\n```cpp\nclass collect_food final : public action<blackboard_type> {\n  public:\n    virtual float score(const blackboard_type& blackboard) const override {\n      return 50.0f;\n    }\n\n    virtual void apply(blackboard_type& blackboard) const override {\n      blackboard.food += 1;\n    }\n};\n\nclass collect_wood final : public action<blackboard_type> {\n  public:\n    virtual float score(const blackboard_type& blackboard) const override {\n      return 150.0f;\n    }\n\n    virtual void apply(blackboard_type& blackboard) const override {\n      blackboard.wood += 1;\n    }\n};\n\nclass collect_stone final : public action<blackboard_type> {\n  public:\n    virtual float score(const blackboard_type& blackboard) const override {\n      return -10.0f;\n    }\n\n    virtual void apply(blackboard_type& blackboard) const override {\n      blackboard.stone += 1;\n    }\n};\n\nclass collect_gold final : public action<blackboard_type> {\n  public:\n    virtual float score(const blackboard_type& blackboard) const override {\n      return 75.0f;\n    }\n\n    virtual void apply(blackboard_type& blackboard) const override {\n      blackboard.gold += 1;\n    }\n};\n```\n\nFinally, create an evaluator and run it:\n\n```cpp\nauto evaluator = evaluator<blackboard_type>{\n  std::make_shared<collect_food>(),\n  std::make_shared<collect_wood>(),\n  std::make_shared<collect_stone>(),\n  std::make_shared<collect_gold>()\n};\n\nauto blackboard = blackboard_type{};\nevaluator.run(blackboard);\n```\n\n### Goal Oriented Action Planning\n\nFirst, include the header file:\n\n```cpp\n#include <aitoolkit/goap.hpp>\n\nusing namespace aitoolkit::goap;\n```\n\nThen, create a blackboard class that will hold the state of the planner:\n\n```cpp\nstruct blackboard_type {\n  bool has_axe{false};\n  int wood{0};\n};\n```\n\n> **NB:** The blackboard needs to be comparable (`a == b`) and hashable.\n\nNext, create a class for each action that you want to be able to perform:\n\n```cpp\nclass get_axe final : public action<blackboard_type> {\n  public:\n    virtual float cost(const blackboard_type& blackboard) const override {\n      return 1.0f;\n    }\n\n    virtual bool check_preconditions(const blackboard_type& blackboard) const override {\n      return !blackboard.has_axe;\n    }\n\n    virtual void apply_effects(blackboard_type& blackboard, bool dry_run) const override {\n      blackboard.has_axe = true;\n    }\n};\n\nclass chop_tree final : public action<blackboard_type> {\n  public:\n    virtual float cost(const blackboard_type& blackboard) const override {\n      return 1.0f;\n    }\n\n    virtual bool check_preconditions(const blackboard_type& blackboard) const override {\n      return blackboard.has_axe;\n    }\n\n    virtual void apply_effects(blackboard_type& blackboard, bool dry_run) const override {\n      blackboard.wood += 1;\n    }\n};\n```\n\nFinally, create a plan and run it:\n\n```cpp\nauto actions = std::vector<action_ptr<blackboard_type>>{\n  std::make_shared<get_axe>(),\n  std::make_shared<chop_tree>()\n};\nauto initial = blackboard_type{};\nauto goal = blackboard_type{\n  .has_axe = true,\n  .wood = 3\n};\n\nauto p = planner<blackboard_type>(actions, initial, goal);\n\nauto blackboard = initial;\nwhile (p) {\n  p.run_next(blackboard); // will mutate the blackboard\n}\n```\n\nFor more informations, consult the\n[documentation](https://linkdd.github.io/aitoolkit/group__goap.html).\n\n## Documentation\n\nThe documentation is available online [here](https://linkdd.github.io/aitoolkit).\n\nYou can build it locally using [doxygen](https://www.doxygen.nl/):\n\n```\n$ make docs\n```\n\n## License\n\nThis library is released under the terms of the [MIT License](./LICENSE.txt).\n/build\n/.vscode\n\n\n\n[submodule \"docs/theme\"]\n\tpath = docs/theme\n\turl = https://github.com/jothepro/doxygen-awesome-css.git\n\n\n\nCopyright 2024 David Delassus <david.jose.delassus@gmail.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n.PHONY: test\ntest:\n\t@make -C tests all\n\n.PHONY: docs\ndocs:\n\t@make -C docs all\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/rauchg/next-ai-news", "text_blocks": "## Next AI News\n\nA full-stack replica of HN using Next.js and AI generated content.\n\n### Design notes\n\n- Uses [Next.js 14](https://nextjs.org/) with [App Router](https://nextjs.org/docs/app/building-your-application/routing) and [RSC](https://nextjs.org/docs/app/building-your-application/rendering/server-components) on the Node.js runtime\n  - All pages are server-rendered and dynamic, with no data caching\n  - All mutations are done via [Server Actions](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations)\n  - [Streaming](https://nextjs.org/docs/app/building-your-application/routing/loading-ui-and-streaming) is used throughout to maximize speed and concurrency\n- Uses [pnpm](https://pnpm.io/installation) for package management\n- Uses [Drizzle ORM](https://orm.drizzle.team/docs/overview) and Zod as the data layer\n- Uses [Auth.js](https://authjs.dev/)'s [Next-Auth](https://next-auth.js.org/) for password authentication\n- Used [v0](https://v0.dev) to generate all initial UIs with\n  [Tailwind](https://tailwindcss.com/), [Shadcn UI](https://ui.shadcn.com/) and [Radix UI](https://www.radix-ui.com/)\n- Developed entirely and tested with the new Next.js `--turbo` Rust compiler\n- Uses [react-highlight-words](https://bvaughn.github.io/react-highlight-words/) for search highlights\n- [PPR](https://vercel.com/blog/partial-prerendering-with-next-js-creating-a-new-default-rendering-model) _(experimental)_ is used to precompute the shells of pages\n  - When deployed, these are served statically from the edge\n  - This makes TTFB faster and speeds up CSS/fonts while origin streams\n- Deployed serverlessly on Vercel's Edge Network using:\n  - [Cron Jobs](https://vercel.com/guides/how-to-setup-cron-jobs-on-vercel) for AI generation\n  - [Serverless Functions](https://vercel.com/docs/functions/serverless-functions) (Node.js) for SSR (`iad1` / `us-east-1`)\n  - [KV](https://vercel.com/docs/storage/vercel-kv) (Upstash) for rate-limiting (`iad1` / `us-east-1`)\n  - [Postgres](https://vercel.com/docs/storage/vercel-postgres) (Neon) for core storage and search with [`pg_trgm`](https://www.postgresql.org/docs/current/pgtrgm.html) (`iad1` / `us-east-1`)\n\n#### AI\n\n- Uses [Mixtral](https://mistral.ai/) `mixtral-8x7b-32kseqlen` as the LLM for generated content\n- Uses [Anyscale](https://www.anyscale.com/)'s finetune for [Tools support](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features)\n- Uses [openai-zod-functions](https://www.npmjs.com/package/openai-zod-functions) for structured and runtime-validated generation\n\n### Deployment\n\n- Make sure the Vercel project is connected to a Vercel Postgres (Neon) database\n- Optionally, for rate limiting, add a Vercel KV (Upstash) database\n- Run `pnpm drizzle-kit push:pg`\n- Update `metadataBase` in `app/layout.tsx` to match your target domain\n\n### Local dev\n\n- Run `vc env pull` to get a `.env.local` file with your db credentials.\n- Run `pnpm dev` to start developing\n- For DB migrations with `drizzle-kit`:\n  - Make sure `?sslmode=required` is added to the `POSTGRES_URL` env for dev\n  - Run `pnpm drizzle-kit generate:pg` to generate migrations\n  - Run `pnpm drizzle-kit push:pg` to apply them\n\n### Performance\n\n[PageSpeed report](https://pagespeed.web.dev/analysis/https-next-ai-news-vercel-app/x55es0m0ya?form_factor=mobile) for Emulated Moto G Power with Lighthouse 11.0.0, Slow 4G Throttling:\n\n[![](https://h2rsi9anqnqbkvkf.public.blob.vercel-storage.com/perf-LAbwq5HsiimvbRrNSUV9JAGCATsBMs.png)](https://pagespeed.web.dev/analysis/https-next-ai-news-vercel-app/x55es0m0ya?form_factor=mobile)\n<sup>&nbsp;&nbsp;&nbsp;üí© The SEO `98` score cannot be `100` without sacrificing stylistic fidelity to the original HN navigation</sup>\n\n### Codebase notes\n\n- Auth is initialized in `app/auth.tsx`, Drizzle in `app/db.tsx`.\n- Shared components are in `./components` (exposed as `@/components`)\n- Only one component was not reused from npm / shadcn ([`components/time-ago.tsx`](components/time-ago.tsx))\n  - I couldn't find something very light that worked well with server-rendering (takes a `now` prop with a timestamp)\n- The following db migrations were added manually:\n  - `CREATE EXTENSION IF NOT EXISTS pg_trgm;` as part of #13\n  - `USING GIN (title gin_trgm_ops);` as part of #13\n\n### TODO\n\nThis project is unique in that it's a full-stack replica of HN, with quite a few features. It'd be great\nfor the community to fill in some important gaps, however:\n\n- [ ] Inline comment replies\n- [ ] \"Forgot password\" flow\n- [ ] Voting and ranking\n- [ ] \"Next\" and \"Prev\" comment links\n- [ ] Comment toggling\n- [ ] Flagging submissions and comments\n- [ ] Improve search further\n- [ ] Comment pagination\n- [ ] More efficient comment datastructures\n- [ ] Optimistic comments with `useOptimistic`\n- [ ] Local storage of comment and submission drafts\n- [ ] Improve the `/next` implementationa after login\n- [ ] Add support for passkeys\n- [ ] A basic admin panel\n- [ ] User profiles\n\n### License\n\nMIT\n{\n  \"extends\": \"next/core-web-vitals\",\n  \"rules\": {\n    \"import/no-relative-parent-imports\": [\n      \"error\",\n      {\n        \"ignore\": [\"@/app\", \"@/components\", \"@/lib\"]\n      }\n    ]\n  }\n}\n\n\n\n# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.\n\n# dependencies\n/node_modules\n/.pnp\n.pnp.js\n.yarn/install-state.gz\n\n# testing\n/coverage\n\n# next.js\n/.next/\n/out/\n\n# production\n/build\n\n# misc\n.DS_Store\n*.pem\n\n# debug\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n# local env files\n.env*.local\n\n# vercel\n.vercel\n\n# typescript\n*.tsbuildinfo\nnext-env.d.ts\ntest.sh\nTODO.md\n\npublic/test.html\ndrizzle\n\n\n\n{\n  \"$schema\": \"https://ui.shadcn.com/schema.json\",\n  \"style\": \"new-york\",\n  \"rsc\": true,\n  \"tsx\": true,\n  \"tailwind\": {\n    \"config\": \"tailwind.config.ts\",\n    \"css\": \"app/globals.css\",\n    \"baseColor\": \"zinc\",\n    \"cssVariables\": false\n  },\n  \"aliases\": {\n    \"utils\": \"@/lib/utils\",\n    \"components\": \"@/components\"\n  }\n}\n\n\nimport type { Config } from \"drizzle-kit\";\nimport * as dotenv from \"dotenv\";\ndotenv.config({ path: \".env.local\" });\n\nif (!process.env.POSTGRES_URL) {\n  throw new Error(\n    \"POSTGRES_URL is not defined. Make sure to `vc env pull` to get `.env.local`\"\n  );\n}\n\nexport default {\n  schema: \"./app/db.ts\",\n  out: \"./drizzle\",\n  driver: \"pg\",\n  dbCredentials: {\n    connectionString: process.env.POSTGRES_URL,\n  },\n} satisfies Config;\n\n\n\n/** @type {import('next').NextConfig} */\nexport default {\n  experimental: {\n    ppr: true,\n  },\n  rewrites: () => [\n    {\n      source: \"/newest\",\n      destination: \"/?newest=1\",\n    },\n    {\n      source: \"/newcomments\",\n      destination: \"/threads?new=1\",\n    },\n    {\n      source: \"/ask\",\n      destination: \"/?type=ask\",\n    },\n    {\n      source: \"/show\",\n      destination: \"/?type=show\",\n    },\n    {\n      source: \"/jobs\",\n      destination: \"/?type=jobs\",\n    },\n  ],\n};\n\n\n\n{\n  \"name\": \"next-ppr-news\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev --turbo\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\"\n  },\n  \"dependencies\": {\n    \"@auth/drizzle-adapter\": \"^0.3.12\",\n    \"@fontsource/inter\": \"^5.0.16\",\n    \"@neondatabase/serverless\": \"^0.6.1\",\n    \"@radix-ui/react-slot\": \"^1.0.2\",\n    \"@upstash/ratelimit\": \"^1.0.0\",\n    \"@upstash/redis\": \"^1.28.0\",\n    \"bcrypt\": \"^5.1.1\",\n    \"class-variance-authority\": \"^0.7.0\",\n    \"clsx\": \"^2.0.0\",\n    \"drizzle-orm\": \"^0.29.1\",\n    \"javascript-time-ago\": \"^2.5.9\",\n    \"lucide-react\": \"^0.303.0\",\n    \"nanoid\": \"^5.0.4\",\n    \"nanoid-dictionary\": \"^4.3.0\",\n    \"next-auth\": \"5.0.0-beta.4\",\n    \"openai\": \"^4.24.1\",\n    \"openai-zod-functions\": \"^0.1.2\",\n    \"psl\": \"^1.9.0\",\n    \"react\": \"^18\",\n    \"react-dom\": \"^18\",\n    \"react-highlight-words\": \"^0.20.0\",\n    \"sonner\": \"^1.3.1\",\n    \"zod\": \"^3.22.4\"\n  },\n  \"devDependencies\": {\n    \"@types/bcrypt\": \"^5.0.2\",\n    \"@types/nanoid-dictionary\": \"^4.2.3\",\n    \"@types/node\": \"^20\",\n    \"@types/psl\": \"^1.1.3\",\n    \"@types/react\": \"^18\",\n    \"@types/react-dom\": \"^18\",\n    \"@types/react-highlight-words\": \"^0.16.7\",\n    \"autoprefixer\": \"^10.0.1\",\n    \"dotenv\": \"^16.3.1\",\n    \"drizzle-kit\": \"^0.20.9\",\n    \"eslint\": \"^8\",\n    \"eslint-config-next\": \"14.0.2\",\n    \"next\": \"14.0.5-canary.41\",\n    \"pg\": \"^8.11.3\",\n    \"postcss\": \"^8\",\n    \"tailwind-merge\": \"^2.0.0\",\n    \"tailwindcss\": \"^3.4.0\",\n    \"tailwindcss-animate\": \"^1.0.7\",\n    \"typescript\": \"^5\"\n  }\n}\n\n\n\nmodule.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n};\n\n\n\nimport type { Config } from 'tailwindcss'\n\nconst config: Config = {\n  content: [\n    './pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './components/**/*.{js,ts,jsx,tsx,mdx}',\n    './app/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {\n      backgroundImage: {\n        'gradient-radial': 'radial-gradient(var(--tw-gradient-stops))',\n        'gradient-conic':\n          'conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))',\n      },\n    },\n  },\n  plugins: [require(\"tailwindcss-animate\")],\n}\nexport default config\n\n\n\n{\n  \"compilerOptions\": {\n    \"target\": \"es2020\",\n    \"lib\": [\"dom\", \"dom.iterable\", \"esnext\"],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"strict\": true,\n    \"noEmit\": true,\n    \"esModuleInterop\": true,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"bundler\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"jsx\": \"preserve\",\n    \"incremental\": true,\n    \"plugins\": [\n      {\n        \"name\": \"next\"\n      }\n    ],\n    \"paths\": {\n      \"@/*\": [\"./*\"]\n    }\n  },\n  \"include\": [\"next-env.d.ts\", \"**/*.ts\", \"**/*.tsx\", \".next/types/**/*.ts\"],\n  \"exclude\": [\"node_modules\"]\n}\n\n\n\n{\n  \"crons\": [\n    {\n      \"path\": \"/cron\",\n      \"schedule\": \"*/5 * * * *\"\n    }\n  ]\n}\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/epiresdasilva/aws-minecraft-server", "text_blocks": "# Minecraft Server on AWS\n\nMy 7 years old son is a fan of Minecraft. He started playing last year (2023) and just talk about that. Now, he want to play with us, me and my wife, as well with some cousins.\n\nIt's available lots of servers on the internet, both paid or free, but those server don't have the security I want to my kid.\n\nThat's because I had the idea of creating my own Minecraft Server on AWS. A server it will be used only by us.\n\nI'm creating this repository to share how I did that and maybe can help other parents (or not) with the same necessity.\n\n## Steps\n\nIt's very simple to create. To do this faster and reproduceble, I create a CloudFormation code for that.\n\nYou'll find a file called `minecraft-server.yaml`.\n\nBasically, in this file I create a Security Groud to allow only few ports I want to expose to the internet, like SSH and the port 25565 that is the port Minecraft Launcher connects.\n\n```yaml\n  MinecraftSecurityGroup:\n    Type: 'AWS::EC2::SecurityGroup'\n    Properties:\n      GroupDescription: 'Enable SSH and Minecraft server port'\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: 22\n          ToPort: 22\n          CidrIp: 0.0.0.0/0\n        - IpProtocol: tcp\n          FromPort: 25565\n          ToPort: 25565\n          CidrIp: 0.0.0.0/0\n```\n\nAnother important part is creating the EC2 instance. Basically, you need to choose the Linux image you want, as well the instance type. I choose a `t4g.small` and it's running very well. I just need to understand if can be a smaller instance yet.\n\n```yaml\n      InstanceType: t4g.small\n      ImageId: ami-02cd6549baea35b55\n      KeyName: Minecraft Server\n```\n\nThe instance type you can find in the EC2 page or using the AWS Calculator, so doing that it's easier to understand how much it will costs.\n\nThe image ID you'll find on the AMI Marketplace on the EC2 section on your AWS Management Console.\n\nYou need to create a KeyPair also on the EC2 section. After creating, just put the name in the \"KeyName\" on the YAML file.\n\nFinally, you just need to run the following command:\n`aws cloudformation create-stack --stack-name MinecraftServerStack --template-body file://minecraft-server.yaml --region us-east-1`\n\n## Security\n\nRemember, DO NOT create that stack using your root user. Create an especific one to that. On doing that, remember to give only the necessary permissions you need. For instance, I used `AmazonEC2FullAccess` and `AWSCloudFormationFullAccess` and worked fine, but maybe can be more restricted yet.\n\nThis template doesn't create an advanced security for that server, so if you need to improve security it's important working on new layers of security in that template.\n\n## Reboot\n\nThe script for running the Minecraft will run only for the first time when the EC2 instance is created. So, if your instance reboot the Minecraft Server won't start again.\n\nAfter creating my EC2 instance, I create a `systemd` to handle with that, but I didn't add that to my template.\n\n1. Create a systemd service file:\n```bash\nsudo vi /etc/systemd/system/minecraft.service\n```\n\n2. Create the content:\n```ini\n[Unit]\nDescription=Minecraft Server\nAfter=network.target\n\n[Service]\nUser=root\nNice=5\nKillMode=none\nSuccessExitStatus=0 1\nInaccessibleDirectories=/root /sys /srv /media -/lost+found\nNoNewPrivileges=true\nWorkingDirectory=/\nExecStart=/opt/jdk-17.0.9/bin/java -Xmx1024M -Xms1024M -jar minecraft_server.jar nogui\nExecStop=/bin/kill -SIGINT $MAINPID\n\n[Install]\nWantedBy=multi-user.target\n```\n\n3. Reload the systemd:\n\n```bash\nsudo systemctl daemon-reload\n```\n\n4. Enable the service:\n\n```bash\nsudo systemctl enable minecraft.service\n```\n\n5. Start the service:\n\n```bash\nsudo systemctl start minecraft.service\n```\n\n6. Check the service status:\n\n```bash\nsudo systemctl status minecraft.service\n```\n\n7. Service logs:\n\n```bash\nsudo journalctl -u minecraft.service\n```\n## Playing Minecraft\n\nNow, you have your own Minecraft Server and you are able to play with your friends and family.AWSTemplateFormatVersion: '2010-09-09'\nDescription: 'Minecraft Server on AWS EC2'\nResources:\n  MinecraftSecurityGroup:\n    Type: 'AWS::EC2::SecurityGroup'\n    Properties:\n      GroupDescription: 'Enable SSH and Minecraft server port'\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: 22\n          ToPort: 22\n          CidrIp: 0.0.0.0/0\n        - IpProtocol: tcp\n          FromPort: 25565\n          ToPort: 25565\n          CidrIp: 0.0.0.0/0\n  MinecraftServer:\n    Type: 'AWS::EC2::Instance'\n    Properties:\n      InstanceType: t4g.small\n      ImageId: ami-02cd6549baea35b55\n      KeyName: Minecraft Server\n      SecurityGroups:\n        - Ref: MinecraftSecurityGroup\n      UserData:\n        Fn::Base64: !Sub |\n          #!/bin/bash\n          yum update -y\n          wget https://download.oracle.com/java/17/latest/jdk-17_linux-aarch64_bin.tar.gz\n          sudo tar xvf jdk-17_linux-aarch64_bin.tar.gz -C /opt\n          wget https://piston-data.mojang.com/v1/objects/8dd1a28015f51b1803213892b50b7b4fc76e594d/server.jar -O minecraft_server.jar\n          echo 'eula=true' > eula.txt\n          /opt/jdk-17.0.9/bin/java -Xmx1024M -Xms1024M -jar minecraft_server.jar nogui\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/ubuygold/go-noss", "text_blocks": "1. Â∞Ü.env.exampleÊîπ‰∏∫.env\n2. Âú®.envÈáåÈÖçÁΩÆÂÖ¨ÁßÅÈí•„ÄÇNPUB/NSECÁöÑËé∑ÂèñÂèØ‰ª•‰ΩøÁî®OKXÈí±ÂåÖÁöÑnostrÈìæËé∑ÂèñÂú∞ÂùÄ/ÂØºÂá∫ÁßÅÈí•Ëé∑ÂèñÔºåÊöÇÊó∂‰∏çÁü•ÈÅìÂ¶Ç‰Ωï‰ªéTPÈí±ÂåÖËé∑ÂèñNPUB/NSEC„ÄÇÊ≥®ÊÑènpub... nsec..ÂºÄÂ§¥ÁöÑÈúÄË¶ÅËΩ¨Êç¢‰∏Ä‰∏ãÊâçËÉΩ‰ΩøÁî®ÔºåÂèØ‰ª•Âéªhttps://nostrcheck.me/converter/ ËΩ¨Êç¢\n3. ‰ΩøÁî®golangÁºñËØëËÑöÊú¨ÔºåÊàñËÄÖ‰ΩøÁî®release‰∏≠ÁºñËØëÂ•ΩÁöÑÁâàÊú¨\n4. ËøêË°å\n5. ÂÜçÂä†‰∏ÄÂè•ËØùÔºöÂà§Êñ≠‰Ω†ÊòØÂê¶ÊàêÂäüËøêË°åËÑöÊú¨Ôºö1. ÊàêÂäüËøêË°åÊ≤°Èó™ÈÄÄ 2. Âú®Ëπ¶Â≠ó 3. ËøîÂõûÂÄºResponseÊòæÁ§∫ÊòØ200 4. Publish toÂêéÈù¢ÊúâidÔºåÂπ∂‰∏îidÂâç5‰Ωç‰∏∫0\n\n## FAQ\n1. ÊàëËøêË°åÊó∂Èó™ÈÄÄ‰∫ÜÔºöARBËäÇÁÇπÊ≤°ÈÖçÁΩÆÂ•ΩÔºåÊàñËÄÖËäÇÁÇπË¢´ÂÜ≤ÁÉÇ‰∫ÜÔºåÊç¢‰∏Ä‰∏™„ÄÇ\n2. ËøîÂõûÂÄº‰∏ÄÁõ¥ÊòØ429ÔºöÁÆóÂäõËøáÂâ©ÔºåÈ°πÁõÆÊñπÁöÑÊúçÂä°Âô®Â´å‰Ω†ÁÉ¶‰∫ÜÔºå‰∏¥Êó∂Â∞ÅÁ¶Å‰∫Ü‰Ω†ÁöÑIPÔºåÁ≠â‰∏Ä‰ºöÂÑø„ÄÇ\n3. ÊâìÂá∫Êù•ÁöÑID‰∏∫Á©∫ÊàñËÄÖÂâç5‰Ωç‰∏çÊòØ0ÔºöÂÖ¨ÁßÅÈí•ÈÖçÁΩÆÈîôËØØÔºå‰ªîÁªÜÈòÖËØª‰∏äÈù¢ÁöÑÁ¨¨2Êù°„ÄÇ\n\n## 2024.1.5Êõ¥Êñ∞\n‰øÆÊîπ‰∫ÜÁî±‰∫éÈ°πÁõÆÊñπ‰∏çËÆ§ËØÜÁæéÂåñÂêéJsonÂØºËá¥ÁöÑÈóÆÈ¢òÔºåÊú¨‰∫∫Â∑≤Áªè‰ΩøÁî®Â∑•ÂÖ∑Âá∫ÂùóÔºåÊïàÁéá‰∏çÊòéÔºåËá™Â∑±Â∞ùËØïÔºå‰∏ç‰øùËØÅËÉΩÂá∫Âùó;\nÂè¶Â§ñÔºåÁî±‰∫éNossÈöæÂ∫¶‰∏∫21ÔºåÂõ†Ê≠§Â§ßÂÆ∂ÁöÑÁÆóÂäõÈÉΩÊòØËøáÂâ©ÁöÑÔºåÈ¢ëÁπÅÊèê‰∫§POSTËØ∑Ê±Ç‰ºöÂØºËá¥ÊúçÂä°Âô®ËøîÂõû429ÔºàToo Many Requests/Â§™Â§öËØ∑Ê±ÇÔºâÈîôËØØÔºå‰∏éÊú¨Á®ãÂ∫èÊó†ÂÖ≥Ôºõ\nÂÜÖÂ≠òÈóÆÈ¢òÊåÅÁª≠‰ºòÂåñ‰∏≠„ÄÇpk=\nsk=\nnumberOfWorkers=8\narbRpcUrl=https://rpc.ankr.com/arbitrum\n\n\n*.exe\n.env\n*.7z\nnostr\nmain.go1\ngo-noss.7z\ngo-noss.zip\n\n\n\nmodule nostr\n\ngo 1.21.5\n\nrequire (\n\tgithub.com/ethereum/go-ethereum v1.13.8\n\tgithub.com/nbd-wtf/go-nostr v0.27.5\n)\n\nrequire (\n\tgithub.com/Microsoft/go-winio v0.6.1 // indirect\n\tgithub.com/StackExchange/wmi v1.2.1 // indirect\n\tgithub.com/bits-and-blooms/bitset v1.10.0 // indirect\n\tgithub.com/btcsuite/btcd/btcec/v2 v2.3.2 // indirect\n\tgithub.com/btcsuite/btcd/btcutil v1.1.3 // indirect\n\tgithub.com/btcsuite/btcd/chaincfg/chainhash v1.0.2 // indirect\n\tgithub.com/consensys/bavard v0.1.13 // indirect\n\tgithub.com/consensys/gnark-crypto v0.12.1 // indirect\n\tgithub.com/crate-crypto/go-kzg-4844 v0.7.0 // indirect\n\tgithub.com/deckarep/golang-set/v2 v2.1.0 // indirect\n\tgithub.com/decred/dcrd/crypto/blake256 v1.0.1 // indirect\n\tgithub.com/decred/dcrd/dcrec/secp256k1/v4 v4.2.0 // indirect\n\tgithub.com/ethereum/c-kzg-4844 v0.4.0 // indirect\n\tgithub.com/go-ole/go-ole v1.2.5 // indirect\n\tgithub.com/gobwas/httphead v0.1.0 // indirect\n\tgithub.com/gobwas/pool v0.2.1 // indirect\n\tgithub.com/gobwas/ws v1.2.0 // indirect\n\tgithub.com/gorilla/websocket v1.5.1 // indirect\n\tgithub.com/holiman/uint256 v1.2.4 // indirect\n\tgithub.com/joho/godotenv v1.5.1 // indirect\n\tgithub.com/josharian/intern v1.0.0 // indirect\n\tgithub.com/klauspost/cpuid/v2 v2.2.3 // indirect\n\tgithub.com/mailru/easyjson v0.7.7 // indirect\n\tgithub.com/minio/sha256-simd v1.0.1 // indirect\n\tgithub.com/mmcloughlin/addchain v0.4.0 // indirect\n\tgithub.com/puzpuzpuz/xsync/v2 v2.5.1 // indirect\n\tgithub.com/shirou/gopsutil v3.21.4-0.20210419000835-c7a38de76ee5+incompatible // indirect\n\tgithub.com/supranational/blst v0.3.11 // indirect\n\tgithub.com/tidwall/gjson v1.14.4 // indirect\n\tgithub.com/tidwall/match v1.1.1 // indirect\n\tgithub.com/tidwall/pretty v1.2.0 // indirect\n\tgithub.com/tklauser/go-sysconf v0.3.12 // indirect\n\tgithub.com/tklauser/numcpus v0.6.1 // indirect\n\tgolang.org/x/crypto v0.17.0 // indirect\n\tgolang.org/x/exp v0.0.0-20231110203233-9a3e6036ecaa // indirect\n\tgolang.org/x/mod v0.14.0 // indirect\n\tgolang.org/x/net v0.18.0 // indirect\n\tgolang.org/x/sync v0.5.0 // indirect\n\tgolang.org/x/sys v0.15.0 // indirect\n\tgolang.org/x/tools v0.15.0 // indirect\n\trsc.io/tmplfunc v0.0.3 // indirect\n)\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/HarborYuan/ovsam", "text_blocks": "# Open-Vocabulary SAM\n\n[Haobo Yuan<sup>1</sup>](https://yuanhaobo.me), \n[Xiangtai Li<sup>1</sup>](https://lxtgh.github.io), \n[Chong Zhou<sup>1</sup>](https://chongzhou96.github.io), \n[Yining Li<sup>2</sup>](https://scholar.google.com/citations?user=y_cp1sUAAAAJ), \n[Kai Chen<sup>2</sup>](https://chenkai.site), \n[Chen Change Loy<sup>1</sup>](https://www.mmlab-ntu.com/person/ccloy/). \n\n[<sup>1</sup>S-Lab, Nanyang Technological University](https://www.mmlab-ntu.com/), \n[<sup>2</sup>Shanghai Artificial Intelligence Laboratory](https://www.shlab.org.cn/)\n\n[[`Paper`](http://arxiv.org/abs/2401.02955)] \n[[`Project Page`](https://www.mmlab-ntu.com/project/ovsam)]\n[[`Hugging Face Demo`](https://huggingface.co/spaces/HarborYuan/ovsam)]\n\n## üëÄ Overview\nWe introduce the Open-Vocabulary SAM, a SAM-inspired model designed for simultaneous interactive segmentation and recognition, leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The former adapts SAM's knowledge into the CLIP via distillation and learnable transformer adapters, while the latter transfers CLIP knowledge into SAM, enhancing its recognition capabilities.\n\n<p>\n  <img src=\"https://www.mmlab-ntu.com/project/ovsam/img/ovsam_teaser.jpg\" alt=\"OVSAM overview\">\n</p>\n\n## üîßUsage\nTo play with Open-Vocabulary SAM, you can:\n1. Try the online demo on the [ü§óHugging Face Space](https://huggingface.co/spaces/HarborYuan/ovsam). Thanks for the generous support of the Hugging Face team.\n2. Run the gradio demo locally by cloning and running the [repo](https://huggingface.co/spaces/HarborYuan/ovsam/tree/main) on ü§óHugging Face:\n    ```commandline\n    git lfs install\n    git clone https://huggingface.co/spaces/HarborYuan/ovsam ovsam_demo\n    cd ovsam_demo\n    conda create -n ovsam_demo python=3.10  && conda activate ovsam_demo\n    python -m pip install gradio==4.7.1\n    python -m pip install -r requirements.txt\n    python main.py\n    ```\n3. Try to train or evaluate in this repo following the instructions below.\n\n## ‚öôÔ∏è Installation\nWe use conda to manage the environment.\n\nPytorch installation:\n```commandline\nconda install pytorch torchvision torchaudio cuda-toolkit pytorch-cuda==12.1 -c pytorch -c \"nvidia/label/cuda-12.1.0\"\n```\n\nmmengine installation:\n```commandline\npython -m pip install https://github.com/open-mmlab/mmengine/archive/refs/tags/v0.8.5.zip\n```\n\nmmcv installation (note that older version mmcv before this commit may cause bugs):\n```commandline\nTORCH_CUDA_ARCH_LIST=\"{COMCAP}\" TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\" CUDA_HOME=$(dirname $(dirname $(which nvcc))) LD_LIBRARY_PATH=$(dirname $(dirname $(which nvcc)))/lib MMCV_WITH_OPS=1 FORCE_CUDA=1 python -m pip install git+https://github.com/open-mmlab/mmcv.git@4f65f91db6502d990ce2ee5de0337441fb69dd10\n```\nPlease ask ChatGPT to get `COMCAP`:\n```text\nWhat is the `Compute Capability` of NVIDIA {YOUR GPU MODEL}? Please only output the number, without text.\n```\n\nOther OpenMMLab packages:\n```commandline\npython -m pip install \\\nhttps://github.com/open-mmlab/mmdetection/archive/refs/tags/v3.1.0.zip \\\nhttps://github.com/open-mmlab/mmsegmentation/archive/refs/tags/v1.1.1.zip \\\nhttps://github.com/open-mmlab/mmpretrain/archive/refs/tags/v1.0.1.zip\n```\n\nExtra packages:\n```commandline\npython -m pip install git+https://github.com/cocodataset/panopticapi.git \\\ngit+https://github.com/HarborYuan/lvis-api.git \\\ntqdm terminaltables pycocotools scipy tqdm ftfy regex timm scikit-image kornia\n```\n\n## üìà Datasets\nDatasets should be put in the `data/` folder of this project similar to [mmdet](https://mmdetection.readthedocs.io/en/latest/user_guides/tracking_dataset_prepare.html). Please prepare dataset in the following format.\n### COCO dataset\n```text\n‚îú‚îÄ‚îÄ coco\n‚îÇ   ‚îú‚îÄ‚îÄ annotations\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ panoptic_{train,val}2017.json\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ instance_{train,val}2017.json\n‚îÇ   ‚îú‚îÄ‚îÄ train2017\n‚îÇ   ‚îú‚îÄ‚îÄ val2017\n‚îÇ   ‚îú‚îÄ‚îÄ panoptic_{train,val}2017/  # png annotations\n```\n### SAM dataset\n```text\n‚îú‚îÄ‚îÄ sam\n‚îÇ   ‚îú‚îÄ‚îÄ train.txt\n‚îÇ   ‚îú‚îÄ‚îÄ val.txt\n‚îÇ   ‚îú‚îÄ‚îÄ sa_000020\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sa_223750.jpg\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sa_223750.json\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...\n‚îÇ   ‚îú‚îÄ‚îÄ ...\n```\n`train.txt` and `val.txt` should contain all the folders you need:\n```text\nsa_000020\nsa_000021\n...\n```\n\n## üöÄ Training\nPlease extract the language embeddings first.\n```commandline\nbash tools/dist.sh gen_cls seg/configs/ovsam/ovsam_coco_rn50x16_point.py 8\n```\n\n### SAM2CLIP\nSAM feature extraction:\n```commandline\nbash tools/dist.sh test seg/configs/sam2clip/sam_vith_dump.py 8\n```\nSAM2CLIP training:\n```commandline\nbash tools/dist.sh train seg/configs/sam2clip/sam2clip_vith_rn50x16.py 8\n```\n\n### CLIP2SAM\nCLIP2SAM training:\n```commandline\nbash tools/dist.sh train seg/configs/clip2sam/clip2sam_coco_rn50x16.py 8\n```\n\n## üèÉ‚Äç‚ôÄÔ∏èInference\n```commandline\nbash tools/dist.sh test seg/configs/ovsam/ovsam_coco_rn50x16_point.py 8\n```\nPlease refer to [ü§óHugging Face](https://huggingface.co/HarborYuan/ovsam_models) to get the pre-trained weights:\n```commandline\ngit clone https://huggingface.co/HarborYuan/ovsam_models models\n```\n\n## üìö Citation\n```bibtex\n@article{yuan2024ovsam,\n    title={Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively},\n    author={Yuan, Haobo and Li, Xiangtai and Zhou, Chong and Li, Yining and Chen, Kai and Loy, Chen Change},\n    journal={arXiv preprint},\n    year={2024}\n}\n```\n## License <a name=\"license\"></a>\n\nThis project is licensed under <a rel=\"license\" href=\"https://github.com/HarborYuan/ovsam/blob/master/LICENSE\">NTU S-Lab License 1.0</a>. Redistribution and use should follow this license.\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n/models/\n\n\n\nS-Lab License 1.0\n\nCopyright 2022 S-Lab\n\nRedistribution and use for non-commercial purpose in source and\nbinary forms, with or without modification, are permitted provided\nthat the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in\n   the documentation and/or other materials provided with the\n   distribution.\n\n3. Neither the name of the copyright holder nor the names of its\n   contributors may be used to endorse or promote products derived\n   from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nHOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nIn the event that redistribution and/or use for commercial purpose in\nsource or binary forms, with or without modification is required,\nplease contact the contributor(s) of the work.\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/praeclarum/1brc", "text_blocks": "Ôªø<Project Sdk=\"Microsoft.NET.Sdk\">\r\n\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFramework>net8.0</TargetFramework>\r\n    <RootNamespace>_1brc</RootNamespace>\r\n    <AllowUnsafeBlocks>True</AllowUnsafeBlocks>\r\n    <PublishAot>true</PublishAot>\r\n    <OptimizationPreference>Speed</OptimizationPreference>\r\n    <IlcInstructionSet>native</IlcInstructionSet>\r\n    <PublishReadyToRun>true</PublishReadyToRun>\r\n  </PropertyGroup>\r\n\r\n  <ItemGroup>\r\n    <Compile Include=\"Baseline.fs\" />\r\n    <Compile Include=\"LexedAndHashed.fs\" />\r\n    <Compile Include=\"Multithreaded.fs\" />\r\n    <Compile Include=\"Program.fs\" />\r\n  </ItemGroup>\r\n\r\n</Project>\r\n\n\n\n// 1 Billion Row Challenge in F#\n// 1 Billion Row Challenge in F#\n\n\nmodule Baseline\n\nopen System\nopen System.Collections.Generic\nopen System.IO\n\ntype StationDataObject = {\n    mutable Min : double\n    mutable Max : double\n    mutable Sum : double\n    mutable Count : int\n}\n\nlet run (measurementsPath : string) =\n    let bufferSize = 64*1024\n    use measurementsStream = new FileStream(measurementsPath, FileMode.Open, FileAccess.Read, FileShare.Read, bufferSize, FileOptions.SequentialScan)\n    use measurements = new StreamReader(measurementsStream, System.Text.Encoding.UTF8, true, bufferSize)\n    let stations = Dictionary<string, StationDataObject>()\n    let mutable entry: StationDataObject = {\n        Min = 0.0\n        Max = 0.0\n        Sum = 0.0\n        Count = 0\n    }\n    let mutable count = 0\n    let mutable line = measurements.ReadLine()\n    let stopwatch = System.Diagnostics.Stopwatch.StartNew()\n    while line <> null do\n        let parts = line.Split(';')\n        let station = parts.[0]\n        let temp = double(parts.[1])\n        match stations.TryGetValue(station, &entry) with\n        | true ->\n            entry.Min <- min entry.Min temp\n            entry.Max <- max entry.Max temp\n            entry.Sum <- entry.Sum + temp\n            entry.Count <- entry.Count + 1\n        | false ->\n            stations.[station] <- {\n                Min = temp\n                Max = temp\n                Sum = temp\n                Count = 1\n            }\n        count <- count + 1\n        line <- measurements.ReadLine()\n        if (count % 50_000_000) = 0 then\n            let entriesPerSecond = (float count) / stopwatch.Elapsed.TotalSeconds\n            let estimatedTotalTime = TimeSpan.FromSeconds (1.0e9 / entriesPerSecond)\n            printfn $\"Processed %d{count} lines (est {estimatedTotalTime})\"\n    let sortedStations =\n        stations\n        |> Seq.sortBy (fun (kv : KeyValuePair<string, StationDataObject>) -> kv.Key)\n    let mutable head = \"{\"\n    for station in sortedStations do\n        let e = station.Value\n        printf $\"%s{head}%s{station.Key}=%.1f{e.Min}/%.1f{e.Sum / double e.Count}/%.1f{e.Max}\"\n        head <- \", \"\n    printfn \"}\"\n\n\n\n// 1 Billion Row Challenge in F#\n// Lexer and Hashed Station Names\n\nmodule LexedAndHashed\n\nopen System\nopen System.Buffers.Text\nopen System.Collections.Generic\nopen System.IO\nopen Microsoft.FSharp.NativeInterop\n\nopen Baseline\n\n#nowarn \"9\"\n\ntype StationDataFixed = {\n    mutable Min : int\n    mutable Max : int\n    mutable Sum : int\n    mutable Count : int\n}\n\nlet private utf8 = System.Text.Encoding.UTF8\n\ntype ChunkProcessor(chunkPtr: nativeptr<byte>, chunkLength: int64) =\n    let stations = Dictionary<int, StationDataFixed>(1024)\n    let stationNames = Dictionary<int, string>(1024)\n    member this.Length = chunkLength\n    member this.Stations =\n        stations\n        |> Seq.map (fun kv -> stationNames.[kv.Key], kv.Value)\n    member this.Run() =\n        let mutable count: int64 = 0\n        let mutable entry: StationDataFixed = {\n            Min = 0\n            Max = 0\n            Sum = 0\n            Count = 0\n        }\n        let mutable p = chunkPtr\n        let mutable index: int64 = 0\n        while index < chunkLength do\n            // 1. Read and hash station name\n            let mutable nameLength = 0\n            let mutable nameHash = 0\n            let mutable b = NativePtr.read p\n            while b <> ';'B do\n                nameHash <- nameHash * 311 + int b\n                nameLength <- nameLength + 1\n                b <- NativePtr.get p nameLength\n            // 2. Read temperature\n            let tempPtr = NativePtr.add p (nameLength + 1)\n            let mutable tempLength = 0\n            b <- NativePtr.read tempPtr\n            let isNeg = b = '-'B\n            if isNeg then\n                tempLength <- tempLength + 1\n                b <- NativePtr.get tempPtr tempLength\n            let mutable temp = 0\n            while b <> '.'B do\n                tempLength <- tempLength + 1\n                temp <- temp * 10 + int (b - '0'B)\n                b <- NativePtr.get tempPtr tempLength\n            let dec = NativePtr.get tempPtr (tempLength + 1)\n            tempLength <- tempLength + 2\n            temp <- temp * 10 + int (dec - '0'B)\n            if isNeg then\n                temp <- -temp    \n            // 3. Update station data\n            if stations.TryGetValue(nameHash, &entry) then\n                entry.Min <- min entry.Min temp\n                entry.Max <- max entry.Max temp\n                entry.Sum <- entry.Sum + temp\n                entry.Count <- entry.Count + 1\n            else\n                stations.Add(nameHash, {\n                    Min = temp\n                    Max = temp\n                    Sum = temp\n                    Count = 1\n                })\n                stationNames.Add(nameHash, utf8.GetString (p, nameLength))\n            count <- count + 1L\n            // 4. Skip to next line\n            let newlineOffset = nameLength + tempLength + 2\n            p <- NativePtr.add p newlineOffset\n            index <- index + int64 newlineOffset\n\nlet run (measurementsPath : string) =\n    let mmap = MemoryMappedFiles.MemoryMappedFile.CreateFromFile(measurementsPath, FileMode.Open)\n    let mmapA = mmap.CreateViewAccessor()\n    let mutable filePtr: nativeptr<byte> = NativePtr.nullPtr<byte>\n    mmapA.SafeMemoryMappedViewHandle.AcquirePointer(&filePtr)\n    let fileLength = mmapA.Capacity\n    let processor = ChunkProcessor(filePtr, fileLength)\n    processor.Run()\n    let sortedStations =\n        processor.Stations\n        |> Seq.sortBy fst\n    let mutable head = \"{\"\n    for station in sortedStations do\n        let name, e = station\n        printf $\"%s{head}%s{name}=%.1f{float e.Min * 0.1}/%.1f{float e.Sum * 0.1 / float e.Count}/%.1f{float e.Max * 0.1}\"\n        head <- \", \"\n    printfn \"}\"\n\n\n\n// 1 Billion Row Challenge in F#\n// One thread per 256 MB chunk\n\nmodule Multithreaded\n\nopen System\nopen System.Buffers.Text\nopen System.Collections.Generic\nopen System.IO\nopen Microsoft.FSharp.NativeInterop\n\nopen Baseline\nopen LexedAndHashed\n\n#nowarn \"9\"\n\nlet chunkify (chunkPtr : nativeptr<byte>) (length : int64) : ResizeArray<ChunkProcessor> =\n    let idealChunkLength = 256 * 1024 * 1024\n    let chunks = ResizeArray<ChunkProcessor>()\n    let mutable offset = 0L\n    let mutable p = chunkPtr\n    while offset < length do\n        let mutable chunkLength = min (length - offset) (int64 idealChunkLength - 256L) |> int\n        let mutable pNextChunk = NativePtr.add p chunkLength\n        offset <- offset + int64 chunkLength\n        // Find next '\\n'\n        while (offset < length) && (NativePtr.read pNextChunk <> '\\n'B) do\n            pNextChunk <- NativePtr.add pNextChunk 1\n            chunkLength <- chunkLength + 1\n            offset <- offset + 1L\n        // Move beyond the '\\n'\n        if offset < length then\n            pNextChunk <- NativePtr.add pNextChunk 1\n            chunkLength <- chunkLength + 1\n            offset <- offset + 1L\n        // Output previous chunk\n        chunks.Add(ChunkProcessor(p, chunkLength))\n        p <- pNextChunk\n#if DEBUG\n    let totalChunkLength = chunks |> Seq.sumBy (fun c -> c.Length)\n    if totalChunkLength <> length then\n        failwithf $\"Chunk lengths don't add up: {totalChunkLength} <> {length}\"\n#endif\n    chunks\n\nlet mergeResults (results : ResizeArray<ChunkProcessor>) : Dictionary<string, StationDataFixed> =\n    let merged = Dictionary<string, StationDataFixed>(1024)\n    for result in results do\n        for name, e in result.Stations do\n            match merged.TryGetValue name with\n            | false, _ ->\n                merged.Add(name, e)\n            | true, e2 ->\n                e2.Count <- e2.Count + e.Count\n                e2.Sum <- e2.Sum + e.Sum\n                e2.Min <- min e2.Min e.Min\n                e2.Max <- max e2.Max e.Max\n    merged\n\nlet run (measurementsPath : string) =\n    let stopwatch = System.Diagnostics.Stopwatch.StartNew()\n    let mmap = MemoryMappedFiles.MemoryMappedFile.CreateFromFile(measurementsPath, FileMode.Open)\n    let mmapA = mmap.CreateViewAccessor()\n    let mutable filePtr: nativeptr<byte> = NativePtr.nullPtr<byte>\n    mmapA.SafeMemoryMappedViewHandle.AcquirePointer(&filePtr)\n    let fileLength = mmapA.Capacity\n    \n    let chunks = chunkify filePtr fileLength\n    \n    let threads =\n        chunks\n        |> Seq.map (fun c -> new System.Threading.Thread(fun () -> c.Run()))\n        |> Array.ofSeq\n    for thread in threads do\n        thread.Start()\n    let out = System.Console.Out\n    out.Write \"{\"\n    for thread in threads do\n        thread.Join()\n    \n    let sortedStations =\n        mergeResults chunks\n        |> Seq.map (fun kv -> kv.Key, kv.Value)\n        |> Seq.sortBy fst\n    let mutable head = \"\"\n    for station in sortedStations do\n        let name, e = station\n        out.Write head\n        out.Write name\n        out.Write $\"=%.1f{float e.Min * 0.1}/%.1f{float e.Sum * 0.1 / float e.Count}/%.1f{float e.Max * 0.1}\"\n        head <- \", \"\n    out.WriteLine(\"}\")\n    stopwatch.Stop ()\n    printfn $\"ELAPSED: {stopwatch}\"\n\n\n\nÔªø// 1 Billion Row Challenge in F#\r\n\r\n[<EntryPoint>]\r\nlet main argv =\r\n    let measurementsPath =\r\n        argv\r\n        |> Seq.filter (fun arg -> arg.EndsWith(\".txt\", System.StringComparison.OrdinalIgnoreCase))\r\n        |> Seq.tryHead\r\n        |> Option.defaultValue \"measurements-20.txt\"\r\n    \r\n    // Baseline.run measurementsPath\r\n    // LexedAndHashed.run measurementsPath\r\n    Multithreaded.run measurementsPath\r\n    0\r\n\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/ThePBone/tachiyomi-extensions-revived", "text_blocks": "# ![app icon](./.github/readme-images/app-icon.png)Tachiyomi Extensions Revived\n\nThis repository contains several source extensions that have been removed from the official repository.\n\nI'll probably keep maintaining the MangaDex extension here and push fixes if it breaks at some point in future.\n\nOther sources are likely not going to be maintained actively by me and are stored here for archival purposes instead. Feel free to send pull requests for those, though.\n\nThis repo is a flattened & detached fork because the git history of the upstream repo is over 400 megabytes big.\n\n### Extensions in this repo\n\n* MangaDex\n* Bato.to (fixed, works again)\n* NewToki / ManaToki\n* S2Manga\n\n\n# Usage\n\nExtension sources can be downloaded, installed, and uninstalled via the main Tachiyomi app. They are installed and uninstalled like regular apps, in `.apk` format.\n\nGoogle Play Protect may show a warning screen mentioning that the app is published by an unknown developer. This is because I had to use my own freshly created signing key that is not yet recognized by Google. This warning screen should stop showing up after my signing keys have built up enough reputation.\n\n> [!IMPORTANT]\n> Tachiyomi does not enable unofficial extension apps by default.\n> **You must manually press the 'TRUST' button** after installing the extension on Tachiyomi's extension management screen!\n\n\n## Downloads\n\n### Add this repo to Tachiyomi (v0.15.2 and later)\n\nStarting with the latest [Tachiyomi version](https://tachiyomi.org/download/) (v0.15.2 and later), you can now add external extension repos without having to re-approve unofficial extensions after each app launch!\n\n \n1. Update to the latest version of [Tachiyomi](https://tachiyomi.org/download/)\n2. Go to Settings > Browse > Extension repos > Add\n3. Enter the following URL and accept: `https://raw.githubusercontent.com/ThePBone/tachiyomi-extensions-revived/repo/index.min.json`\n4. Go to the extension management screen, refresh it, and you can now download the removed extensions.\n5. After installing an extension, you need to approve it by tapping on the 'Trust' button.\n\n> [!NOTE]\n> Unofficial extensions must be manually re-approved on Tachiyomi's extension management screen after installing an update for an extension!\n>\n> If Tachiyomi suddenly complains about a missing source, then there probably was an extension update that needs to be approved.\n\n### Direct APK downloads\nYou can also directly download the APK files in this GitHub repository in the [`repo` branch](https://github.com/ThePBone/tachiyomi-extensions-revived/tree/repo/apk).\n\nAfter installing any unofficial extension, you must **manually** enable the extension in Tachiyomi.\n\n## Disclaimer\n\nThe developer of this application does not have any affiliation with the content providers available.\n\n\n## License\n\n    Copyright 2015 Javier Tom√°s\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n# Editor configuration, see https://editorconfig.org\nroot = true\n\n[*.kt]\ncharset = utf-8\nend_of_line = lf\nindent_size = 4\nindent_style = space\ninsert_final_newline = true\ntrim_trailing_whitespace = true\nij_kotlin_allow_trailing_comma = true\nij_kotlin_allow_trailing_comma_on_call_site = true\nij_kotlin_name_count_to_use_star_import = 2147483647\nij_kotlin_name_count_to_use_star_import_for_members = 2147483647\n\n[*.properties]\ncharset = utf-8\nend_of_line = lf\ninsert_final_newline = true\n\n\n\n.gradle\n/local.properties\n/.idea/workspace.xml\n.DS_Store\nbuild/\n/captures\n.idea/\n*.iml\nrepo/\napk/\ngen\ngenerated-src/\n\n\n\nbuildscript {\n    repositories {\n        mavenCentral()\n        google()\n        maven(url = \"https://plugins.gradle.org/m2/\")\n    }\n    dependencies {\n        classpath(libs.gradle.agp)\n        classpath(libs.gradle.kotlin)\n        classpath(libs.gradle.serialization)\n        classpath(libs.gradle.kotlinter)\n    }\n}\n\nallprojects {\n    repositories {\n        mavenCentral()\n        google()\n        maven(url = \"https://jitpack.io\")\n    }\n}\n\ntasks.register<Delete>(\"clean\") {\n    delete(rootProject.layout.buildDirectory.asFile.get())\n}\n\n\n\napply plugin: 'org.jmailen.kotlinter'\n\nandroid {\n    compileSdkVersion AndroidConfig.compileSdk\n\n    namespace \"eu.kanade.tachiyomi.revived\"\n    sourceSets {\n        main {\n            manifest.srcFile \"AndroidManifest.xml\"\n            java.srcDirs = ['src']\n            res.srcDirs = ['res']\n            assets.srcDirs = ['assets']\n        }\n        release {\n            manifest.srcFile \"AndroidManifest.xml\"\n        }\n        debug {\n            manifest.srcFile \"AndroidManifest.xml\"\n        }\n    }\n\n    defaultConfig {\n        minSdkVersion AndroidConfig.minSdk\n        targetSdkVersion AndroidConfig.targetSdk\n        applicationIdSuffix pkgNameSuffix\n        versionCode extVersionCode\n        versionName project.ext.properties.getOrDefault(\"libVersion\", \"1.4\") + \".$extVersionCode\"\n        setProperty(\"archivesBaseName\", \"tachiyomi-$pkgNameSuffix-revived-v$versionName\")\n        def readmes = project.projectDir.listFiles({ File file ->\n            file.name == \"README.md\" || file.name == \"CHANGELOG.md\"\n        } as FileFilter)\n        def hasReadme = readmes != null && readmes.any { File file ->\n            file.name.startsWith(\"README\")\n        }\n        def hasChangelog = readmes != null && readmes.any { File file ->\n            file.name.startsWith(\"CHANGELOG\")\n        }\n        manifestPlaceholders = [\n                appName : \"Tachiyomi: $extName (Revived)\",\n                extClass: extClass,\n                extFactory: project.ext.properties.getOrDefault(\"extFactory\", \"\"),\n                nsfw: project.ext.properties.getOrDefault(\"isNsfw\", false) ? 1 : 0,\n                hasReadme: hasReadme ? 1 : 0,\n                hasChangelog: hasChangelog ? 1 : 0,\n        ]\n    }\n\n    signingConfigs {\n        release {\n            storeFile rootProject.file(\"signingkey.jks\")\n            storePassword System.getenv(\"KEY_STORE_PASSWORD\")\n            keyAlias System.getenv(\"ALIAS\")\n            keyPassword System.getenv(\"KEY_PASSWORD\")\n        }\n    }\n\n    buildTypes {\n        release {\n            signingConfig signingConfigs.release\n            minifyEnabled false\n        }\n    }\n\n    dependenciesInfo {\n        includeInApk = false\n    }\n\n    buildFeatures {\n        // Disable unused AGP features\n        aidl false\n        renderScript false\n        resValues false\n        shaders false\n    }\n\n    compileOptions {\n        sourceCompatibility = JavaVersion.VERSION_1_8\n        targetCompatibility = JavaVersion.VERSION_1_8\n    }\n\n    kotlinOptions {\n        jvmTarget = JavaVersion.VERSION_1_8.toString()\n        freeCompilerArgs += \"-opt-in=kotlinx.serialization.ExperimentalSerializationApi\"\n    }\n\n    kotlinter {\n        experimentalRules = true\n        disabledRules = [\n            \"experimental:argument-list-wrapping\", // Doesn't play well with Android Studio\n            \"experimental:comment-wrapping\",\n        ]\n    }\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation(project(\":core\"))\n    compileOnly(libs.bundles.common)\n}\n\npreBuild.dependsOn(lintKotlin)\nlintKotlin.dependsOn(formatKotlin)\n\n\n\n# Project-wide Gradle settings.\n\n# IDE (e.g. Android Studio) users:\n# Gradle settings configured through the IDE *will override*\n# any settings specified in this file.\n\n# For more details on how to configure your build environment visit\n# http://www.gradle.org/docs/current/userguide/build_environment.html\n\n# Specifies the JVM arguments used for the daemon process.\n# The setting is particularly useful for tweaking memory settings.\norg.gradle.jvmargs=-Xmx6144m\n\n# When configured, Gradle will run in incubating parallel mode.\n# This option should only be used with decoupled projects. More details, visit\n# http://www.gradle.org/docs/current/userguide/multi_project_builds.html#sec:decoupled_projects\norg.gradle.parallel=true\norg.gradle.workers.max=5\n\norg.gradle.caching=true\n\n# Enable AndroidX dependencies\nandroid.useAndroidX=true\n\n\n\n@rem\n@rem Copyright 2015 the original author or authors.\n@rem\n@rem Licensed under the Apache License, Version 2.0 (the \"License\");\n@rem you may not use this file except in compliance with the License.\n@rem You may obtain a copy of the License at\n@rem\n@rem      https://www.apache.org/licenses/LICENSE-2.0\n@rem\n@rem Unless required by applicable law or agreed to in writing, software\n@rem distributed under the License is distributed on an \"AS IS\" BASIS,\n@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n@rem See the License for the specific language governing permissions and\n@rem limitations under the License.\n@rem\n\n@if \"%DEBUG%\"==\"\" @echo off\n@rem ##########################################################################\n@rem\n@rem  Gradle startup script for Windows\n@rem\n@rem ##########################################################################\n\n@rem Set local scope for the variables with windows NT shell\nif \"%OS%\"==\"Windows_NT\" setlocal\n\nset DIRNAME=%~dp0\nif \"%DIRNAME%\"==\"\" set DIRNAME=.\n@rem This is normally unused\nset APP_BASE_NAME=%~n0\nset APP_HOME=%DIRNAME%\n\n@rem Resolve any \".\" and \"..\" in APP_HOME to make it shorter.\nfor %%i in (\"%APP_HOME%\") do set APP_HOME=%%~fi\n\n@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.\nset DEFAULT_JVM_OPTS=\"-Xmx64m\" \"-Xms64m\"\n\n@rem Find java.exe\nif defined JAVA_HOME goto findJavaFromJavaHome\n\nset JAVA_EXE=java.exe\n%JAVA_EXE% -version >NUL 2>&1\nif %ERRORLEVEL% equ 0 goto execute\n\necho.\necho ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.\necho.\necho Please set the JAVA_HOME variable in your environment to match the\necho location of your Java installation.\n\ngoto fail\n\n:findJavaFromJavaHome\nset JAVA_HOME=%JAVA_HOME:\"=%\nset JAVA_EXE=%JAVA_HOME%/bin/java.exe\n\nif exist \"%JAVA_EXE%\" goto execute\n\necho.\necho ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%\necho.\necho Please set the JAVA_HOME variable in your environment to match the\necho location of your Java installation.\n\ngoto fail\n\n:execute\n@rem Setup the command line\n\nset CLASSPATH=%APP_HOME%\\gradle\\wrapper\\gradle-wrapper.jar\n\n\n@rem Execute Gradle\n\"%JAVA_EXE%\" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% \"-Dorg.gradle.appname=%APP_BASE_NAME%\" -classpath \"%CLASSPATH%\" org.gradle.wrapper.GradleWrapperMain %*\n\n:end\n@rem End local scope for the variables with windows NT shell\nif %ERRORLEVEL% equ 0 goto mainEnd\n\n:fail\nrem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of\nrem the _cmd.exe /c_ return code!\nset EXIT_CODE=%ERRORLEVEL%\nif %EXIT_CODE% equ 0 set EXIT_CODE=1\nif not \"\"==\"%GRADLE_EXIT_CONSOLE%\" exit %EXIT_CODE%\nexit /b %EXIT_CODE%\n\n:mainEnd\nif \"%OS%\"==\"Windows_NT\" endlocal\n\n:omega\n\n\n\n<code_scheme name=\"Project\" version=\"173\">\n  <JetCodeStyleSettings>\n    <option name=\"PACKAGES_TO_USE_STAR_IMPORTS\">\n      <value>\n        <package name=\"kotlinx.android.synthetic\" withSubpackages=\"true\" static=\"false\" />\n      </value>\n    </option>\n    <option name=\"NAME_COUNT_TO_USE_STAR_IMPORT\" value=\"2147483647\" />\n    <option name=\"NAME_COUNT_TO_USE_STAR_IMPORT_FOR_MEMBERS\" value=\"2147483647\" />\n    <option name=\"CODE_STYLE_DEFAULTS\" value=\"KOTLIN_OFFICIAL\" />\n  </JetCodeStyleSettings>\n  <codeStyleSettings language=\"XML\">\n    <indentOptions>\n      <option name=\"CONTINUATION_INDENT_SIZE\" value=\"4\" />\n    </indentOptions>\n    <arrangement>\n      <rules>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>xmlns:android</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>xmlns:.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>BY_NAME</order>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*:id</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>http://schemas.android.com/apk/res/android</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*:name</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>http://schemas.android.com/apk/res/android</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>name</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>style</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>^$</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>BY_NAME</order>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>http://schemas.android.com/apk/res/android</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>ANDROID_ATTRIBUTE_ORDER</order>\n          </rule>\n        </section>\n        <section>\n          <rule>\n            <match>\n              <AND>\n                <NAME>.*</NAME>\n                <XML_ATTRIBUTE />\n                <XML_NAMESPACE>.*</XML_NAMESPACE>\n              </AND>\n            </match>\n            <order>BY_NAME</order>\n          </rule>\n        </section>\n      </rules>\n    </arrangement>\n  </codeStyleSettings>\n  <codeStyleSettings language=\"kotlin\">\n    <option name=\"CODE_STYLE_DEFAULTS\" value=\"KOTLIN_OFFICIAL\" />\n    <option name=\"LINE_COMMENT_AT_FIRST_COLUMN\" value=\"false\" />\n    <option name=\"LINE_COMMENT_ADD_SPACE\" value=\"true\" />\n    <option name=\"KEEP_BLANK_LINES_IN_DECLARATIONS\" value=\"1\" />\n    <option name=\"KEEP_BLANK_LINES_IN_CODE\" value=\"1\" />\n    <option name=\"KEEP_BLANK_LINES_BEFORE_RBRACE\" value=\"0\" />\n    <option name=\"ALIGN_MULTILINE_PARAMETERS\" value=\"false\" />\n    <indentOptions>\n      <option name=\"CONTINUATION_INDENT_SIZE\" value=\"4\" />\n    </indentOptions>\n  </codeStyleSettings>\n</code_scheme>\n\n\n"}
{"date": "2024-01-10-21-07", "error": false, "url": "https://github.com/adrianhajdin/docker-course", "text_blocks": "<div align=\"center\">\n  <br />\n    <a href=\"https://youtu.be/GFgJkfScVNU?feature=shared\" target=\"_blank\">\n      <img src=\"https://github.com/JavaScript-Mastery-Pro/docker-course/assets/151519281/983e334b-6f3a-47d1-8ea6-52139796da20\" alt=\"Project Banner\">\n    </a>\n  <br />\n\n  <div>\n    <img src=\"https://img.shields.io/badge/-Node_JS-black?style=for-the-badge&logoColor=white&logo=nodedotjs&color=339933\" alt=\"nodedotjs\" />\n    <img src=\"https://img.shields.io/badge/-Next_JS-black?style=for-the-badge&logoColor=white&logo=nextdotjs&color=000000\" alt=\"nextdotjs\" />\n    <img src=\"https://img.shields.io/badge/-Docker-black?style=for-the-badge&logoColor=white&logo=docker&color=2496ED\" alt=\"docker\" />\n    <img src=\"https://img.shields.io/badge/-MongoDB-black?style=for-the-badge&logoColor=white&logo=mongodb&color=47A248\" alt=\"mongodb\" />\n    <img src=\"https://img.shields.io/badge/-Vite-black?style=for-the-badge&logoColor=white&logo=vite&color=646CFF\" alt=\"vite\" />\n  </div>\n\n  <h3 align=\"center\">Docker Crash Course</h3>\n\n   <div align=\"center\">\n     Learn how to Dockerize various applications step by step with our detailed tutorial \n     on <a href=\"https://www.youtube.com/@javascriptmastery/videos\" target=\"_blank\"><b>JavaScript Mastery</b></a> YouTube. Join the JSM family!\n    </div>\n</div>\n\n<br />\n\n## üìã <a name=\"table\">Table of Contents</a>\n\n1. ü§ñ [Introduction](#introduction)\n2. ‚öôÔ∏è [Tech Stack](#tech-stack)\n3. üîã [Features](#features)\n4. ü§∏ [Quick Start](#quick-start)\n6. üì¶ [Starter Kit](#starter-kits)\n7. üï∏Ô∏è [Code Snippets](#code-snippets)\n8. üöÄ [More](#more)\n\n<br />\n\n## üö® Tutorial\n\nThis repository contains the code corresponding to an in-depth tutorial available on our YouTube channel, <a href=\"https://www.youtube.com/@javascriptmastery/videos\" target=\"_blank\"><b>JavaScript Mastery</b></a>. \n\nIf you prefer visual learning, this is the perfect resource for you. Follow our tutorial to learn how to build projects like these step-by-step in a beginner-friendly manner!\n\n<a href=\"https://youtu.be/GFgJkfScVNU?feature=shared\" target=\"_blank\"><img src=\"https://github.com/sujatagunale/EasyRead/assets/151519281/1736fca5-a031-4854-8c09-bc110e3bc16d\" /></a>\n\n## <a name=\"introduction\">ü§ñ Introduction</a>\n\nLearn the process of containerizing frontend, backend, and database applications built with diverse tech stacks like React, Vue, Svelte, or any Vite projects. \nAdditionally, it covers examples of the containerization of complete full-stack applications, including MERN setups or the popular Monorepo full-stack applications using Next.js 14+. \n\nThis repository contains the corresponding code for all these dockerized applications using the latest Docker features, including docker-compose watch and init.\n\nIf you're getting started and need assistance or face any bugs, join our active Discord community with over 27k+ members. It's a place where people help each other out.\n\n<a href=\"https://discord.com/invite/n6EdbFJ\" target=\"_blank\"><img src=\"https://github.com/sujatagunale/EasyRead/assets/151519281/618f4872-1e10-42da-8213-1d69e486d02e\" /></a>\n\n## <a name=\"tech-stack\">‚öôÔ∏è Tech Stack</a>\n\n- Docker\n- Node.js\n- React.js\n- Vite\n- MongoDB\n- Express.js\n- Next.js\n- Tailwind CSS\n\n## <a name=\"features\">üîã Features</a>\n\nüëâ **Fundamentals of Docker**: Understand the fundamentals of Docker, its purpose, and advantages.\n\nüëâ **Managing Images and Containers with Docker Compose**: Explore Docker Compose for orchestrating multiple images and containers efficiently.\n\nüëâ **Latest Docker Features**: Learn new features such as docker init, docker scout, and docker compose watch for enhanced development workflows.\n\nüëâ **Working with Volumes**: Learn how to use volumes for persistent data management in Docker containers\n\nüëâ **Port Mapping with Network**: Implement port mapping using Docker's networking capabilities\n\nüëâ **Dockerizing React Applications with Vite**: Step-by-step guide on Dockerizing React applications built with Vite.\n\nüëâ **Dockerizing Vite Applications (Vue or Svelte)**: Extend the knowledge to Dockerizing Vite applications, supporting Vue or Svelte frameworks.\n\nüëâ **Dockerizing Full Stack Applications**: Dockerize a complete MERN stack application, covering frontend, backend, and database.\n\nüëâ **Dockerizing Monorepo Full Stack Applications (Next.js 14+)**: Explore Dockerizing Monorepo full-stack applications using the latest features of Next.js (version 14 and above).\n\nüëâ **Publishing Docker Images**: Learn the steps to publish Docker images, making your applications accessible to a broader audience.\n\n...and much more, covering the best practices and usage of different commands in üê≥\n\n## <a name=\"quick-start\">ü§∏ Quick Start</a>\n\nFollow these steps to set up the project locally on your machine.\n\n**Prerequisites**\n\nMake sure you have the following installed on your machine:\n\n- [Git](https://git-scm.com/)\n- [Node.js](https://nodejs.org/en)\n- [npm](https://www.npmjs.com/) (Node Package Manager)\n- [Docker](https://www.docker.com/products/docker-desktop/)\n- [MongoDB Compass](https://www.mongodb.com/products/tools/compass)\n\n**Cloning the Repository**\n\n```bash\ngit clone https://github.com/your-username/your-project.git\ncd your-project\n```\n\n**Installation**\n\nInstall the project dependencies using npm:\n\n```bash\nnpm install\n```\n\n**Set Up Environment Variables**\n\nFor a few specific applications, we require environment variables. I've included a sample .env.example file for these essential projects. \nHowever, one crucial element needed for these projects is,\n\n```env\nDB_URL=\n```\n\nFor full stack applications, we'll be using MongoDB as a database. So do create an account on [MongoDB Atlas](https://www.mongodb.com/) as well as \ninstall [MongoDB Compass](https://www.mongodb.com/products/tools/compass) for creating local database instance for the project. \n\n**Running the Project**\n\n```bash\nnpm start\n```\n\nOpen [http://localhost:3000](http://localhost:3000) in your browser to view the project.\n\n## <a name=\"starter-kits\">üì¶ Starter Kits</a>\n\nGet the starter kits for a few corresponding applications used in the project\n* MERN Demo - [Download Kit](https://drive.google.com/file/d/15Yqkb6rNPv6DEfT6zuIHDKchzGkOUblZ/view?usp=sharing)\n* Next.js Demo - [Download Kit](https://drive.google.com/file/d/1edSiwP0AwtKblE5y-ZlzIGfq2lcDJBPF/view?usp=sharing)\n\n## <a name=\"code-snippets\">üï∏Ô∏è Code Snippets</a>\n\n<details>\n<summary><code>hello-docker/Dockerfile</code></summary>\n\n```dockerfile\n# select the base image to run the app. We want to run a javascript app, so we use the node runtime image from docker hub\n# we can use any image from docker hub. We can also use a custom image that we have created\n# node:20-alpine -> node is the image name, 20-alpine is the tag\n# alpine is a lightweight version of linux\n# we can see complete list of node image tags here: https://hub.docker.com/_/node\nFROM node:20-alpine\n\n# set the working directory to /app. This is the directory where the commands will be run. We can use any directory name but /app is a standard convention\nWORKDIR /app\n\n# copy everything from the current directory to the PWD (Present Working Directory) inside the container. \n# First `.` is the path to the current directory on the host machine. Second `.` is the path to the current directory inside the container i.e., source and destination\n# source - current directory on the host machine\n# destination - current directory inside the container (/app)\nCOPY . .\n\n# commands to run the app\nCMD node hello.js\n\n# build the image\n# docker build -t hello-docker .\n    # -t -> tag the image with a name\n    # hello-docker -> name of the image\n    # . -> path to the Dockerfile\n```\n\n</details>\n\n<details>\n<summary><code>react-docker/Dockerfile</code></summary>\n\n```dockerfile\n# set the base image to create the image for react app\nFROM node:20-alpine\n\n# create a user with permissions to run the app\n# -S -> create a system user\n# -G -> add the user to a group\n# This is done to avoid running the app as root\n# If the app is run as root, any vulnerability in the app can be exploited to gain access to the host system\n# It's a good practice to run the app as a non-root user\nRUN addgroup app && adduser -S -G app app\n\n# set the user to run the app\nUSER app\n\n# set the working directory to /app\nWORKDIR /app\n\n# copy package.json and package-lock.json to the working directory\n# This is done before copying the rest of the files to take advantage of Docker‚Äôs cache\n# If the package.json and package-lock.json files haven‚Äôt changed, Docker will use the cached dependencies\nCOPY package*.json ./\n\n# sometimes the ownership of the files in the working directory is changed to root\n# and thus the app can't access the files and throws an error -> EACCES: permission denied\n# to avoid this, change the ownership of the files to the root user\nUSER root\n\n# change the ownership of the /app directory to the app user\n# chown -R <user>:<group> <directory>\n# chown command changes the user and/or group ownership of for given file.\nRUN chown -R app:app .\n\n# change the user back to the app user\nUSER app\n\n# install dependencies\nRUN npm install\n\n# copy the rest of the files to the working directory\nCOPY . .\n\n# expose port 5173 to tell Docker that the container listens on the specified network ports at runtime\nEXPOSE 5173\n\n# command to run the app\nCMD npm run dev\n```\n\n</details>\n\n<details>\n<summary><code>vite-docker/Dockerfile</code></summary>\n\n```dockerfile\n# set the base image to create the image for react app\nFROM node:20-alpine\n\n# create a user with permissions to run the app\n# -S -> create a system user\n# -G -> add the user to a group\n# This is done to avoid running the app as root\n# If the app is run as root, any vulnerability in the app can be exploited to gain access to the host system\n# It's a good practice to run the app as a non-root user\nRUN addgroup app && adduser -S -G app app\n\n# set the user to run the app\nUSER app\n\n# set the working directory to /app\nWORKDIR /app\n\n# copy package.json and package-lock.json to the working directory\n# This is done before copying the rest of the files to take advantage of Docker‚Äôs cache\n# If the package.json and package-lock.json files haven‚Äôt changed, Docker will use the cached dependencies\nCOPY package*.json ./\n\n# sometimes the ownership of the files in the working directory is changed to root\n# and thus the app can't access the files and throws an error -> EACCES: permission denied\n# to avoid this, change the ownership of the files to the root user\nUSER root\n\n# change the ownership of the /app directory to the app user\n# chown -R <user>:<group> <directory>\n# chown command changes the user and/or group ownership of for given file.\nRUN chown -R app:app .\n\n# change the user back to the app user\nUSER app\n\n# install dependencies\nRUN npm install\n\n# copy the rest of the files to the working directory\nCOPY . .\n\n# expose port 5173 to tell Docker that the container listens on the specified network ports at runtime\nEXPOSE 5173\n\n# command to run the app\nCMD npm run dev\n```\n\n</details>\n\n<details>\n<summary><code>vite-docker/compose.yaml</code></summary>\n\n```yaml\n# define the services/containers to be run\nservices:\n  # define the application container/service\n  # we can use any name for the service. Here it is `web`\n  # we can create multiple services as well\n  web:\n    # specify the image to build the container from\n    # this can be any image available in docker hub or a custom one or the one we want to build\n    build:\n      # specify the path to the Dockerfile\n      context: .\n      # specify the file name (optional)\n      dockerfile: Dockerfile\n    \n    # specify the port mapping from host to the container\n    # this is similar to the -p flag in `docker run` command\n    # first port is the port on host machine and the second is the port inside the container\n    ports:\n      - 5173:5173\n\n    # specify the volumes to mount\n    # what this does is it mounts the current directory to the `/app` directory inside the container. \n    # due to this, any changes made to the files in the current directory will be reflected inside the container. It is similar to the -v flag in `docker run` command. \n    # even if a container is stopped or deleted, volumes are not deleted and can be used by other containers as well.\n    volumes:\n      # over here, we are mounting the current directory to the `/app` directory inside the container (which is the working directory of the container)\n      # syntax is `<path to the directory on host>:<path to the directory inside the container>`\n      # we're doing this because we want to reflect the changes made to the files in the current directory inside the container\n      - .:/app\n      # we also mount the node_modules directory inside the container at /app/node_modules. This is done to avoid installing the node_modules inside the container. \n      # node_modules will be installed on the host machine and mounted inside the container\n      - /app/node_modules\n```\n\n</details>\n\n<details>\n<summary><code>mern-docker/frontend/Dockerfile</code></summary>\n\n```dockerfile\nFROM node:20-alpine3.18\n\n# RUN addgroup app && adduser -S -G app app\n\n# USER app\n\nWORKDIR /app\n\nCOPY package*.json ./\n\n# USER root\n\n# RUN chown -R app:app .\n\n# USER app\n\nRUN npm install\n\nCOPY . .\n\nEXPOSE 5173\n\nCMD npm run dev\n```\n\n</details>\n\n<details>\n<summary><code>mern-docker/backend/Dockerfile</code></summary>\n\n```dockerfile\nFROM node:20-alpine3.18\n\nRUN addgroup app && adduser -S -G app app\n\nUSER app\n\nWORKDIR /app\n\nCOPY package*.json ./\n\n# change ownership of the /app directory to the app user\nUSER root\n\n# change ownership of the /app directory to the app user\n# chown -R <user>:<group> <directory>\n# chown command changes the user and/or group ownership of for given file.\nRUN chown -R app:app .\n\n# change the user back to the app user\nUSER app\n\nRUN npm install\n\nCOPY . . \n\nEXPOSE 8000 \n\nCMD npm start\n```\n\n</details>\n\n<details>\n<summary><code>mern-docker/compose.yaml</code></summary>\n\n```yaml\n# specify the version of docker-compose\nversion: \"3.8\"\n\n# define the services/containers to be run\nservices:\n  # define the frontend service\n  # we can use any name for the service. A standard naming convention is to use \"web\" for the frontend\n  web:\n    # we use depends_on to specify that service depends on another service\n    # in this case, we specify that the web depends on the api service\n    # this means that the api service will be started before the web service\n    depends_on: \n      - api\n    # specify the build context for the web service\n    # this is the directory where the Dockerfile for the web service is located\n    build: ./frontend\n    # specify the ports to expose for the web service\n    # the first number is the port on the host machine\n    # the second number is the port inside the container\n    ports:\n      - 5173:5173\n    # specify the environment variables for the web service\n    # these environment variables will be available inside the container\n    environment:\n      VITE_API_URL: http://localhost:8000\n\n    # this is for docker compose watch mode\n    # anything mentioned under develop will be watched for changes by docker compose watch and it will perform the action mentioned\n    develop:\n      # we specify the files to watch for changes\n      watch:\n        # it'll watch for changes in package.json and package-lock.json and rebuild the container if there are any changes\n        - path: ./frontend/package.json\n          action: rebuild\n        - path: ./frontend/package-lock.json\n          action: rebuild\n        # it'll watch for changes in the frontend directory and sync the changes with the container real time\n        - path: ./frontend\n          target: /app\n          action: sync\n\n  # define the api service/container\n  api: \n    # api service depends on the db service so the db service will be started before the api service\n    depends_on: \n      - db\n\n    # specify the build context for the api service\n    build: ./backend\n    \n    # specify the ports to expose for the api service\n    # the first number is the port on the host machine\n    # the second number is the port inside the container\n    ports: \n      - 8000:8000\n\n    # specify environment variables for the api service\n    # for demo purposes, we're using a local mongodb instance\n    environment: \n      DB_URL: mongodb://db/anime\n    \n    # establish docker compose watch mode for the api service\n    develop:\n      # specify the files to watch for changes\n      watch:\n        # it'll watch for changes in package.json and package-lock.json and rebuild the container and image if there are any changes\n        - path: ./backend/package.json\n          action: rebuild\n        - path: ./backend/package-lock.json\n          action: rebuild\n        \n        # it'll watch for changes in the backend directory and sync the changes with the container real time\n        - path: ./backend\n          target: /app\n          action: sync\n\n  # define the db service\n  db:\n    # specify the image to use for the db service from docker hub. If we have a custom image, we can specify that in this format\n    # In the above two services, we're using the build context to build the image for the service from the Dockerfile so we specify the image as \"build: ./frontend\" or \"build: ./backend\".\n    # but for the db service, we're using the image from docker hub so we specify the image as \"image: mongo:latest\"\n    # you can find the image name and tag for mongodb from docker hub here: https://hub.docker.com/_/mongo\n    image: mongo:latest\n\n    # specify the ports to expose for the db service\n    # generally, we do this in api service using mongodb atlas. But for demo purposes, we're using a local mongodb instance\n    # usually, mongodb runs on port 27017. So we're exposing the port 27017 on the host machine and mapping it to the port 27017 inside the container\n    ports:\n      - 27017:27017\n\n    # specify the volumes to mount for the db service\n    # we're mounting the volume named \"anime\" inside the container at /data/db directory\n    # this is done so that the data inside the mongodb container is persisted even if the container is stopped\n    volumes:\n      - anime:/data/db\n\n# define the volumes to be used by the services\nvolumes:\n  anime:\n```\n\n</details>\n\n<details>\n<summary><code>next-docker/Dockerfile</code></summary>\n\n```dockerfile\n# inherit from a existing image to add the functionality\nFROM node:20-alpine3.18\n\n# RUN addgroup app && adduser -S -G app app\n# USER app\n\n# Set the working directory and assign ownership to the non-root user\nWORKDIR /app\n\n# Copy the package.json and package-lock.json files into the image.\nCOPY package*.json ./\n\n# change ownership of the /app directory to the app user\n# USER root\n\n# change ownership of the /app directory to the app user\n# chown -R <user>:<group> <directory>\n# chown command changes the user and/or group ownership of for given file.\n# RUN chown -R app:app .\n\n# change the user back to the app user\n# USER app\n\n# Install the dependencies.\nRUN npm install\n\n# Copy the rest of the source files into the image.\nCOPY . .\n\n# Expose the port that the application listens on.\nEXPOSE 3000\n\n# Run the application.\nCMD npm run dev\n```\n\n</details>\n\n<details>\n<summary><code>next-docker/compose.yaml</code></summary>\n\n```yaml\nversion: '3.8'\n\nservices:\n  frontend:\n    # uncomment the following line if you want to run a local instance of MongoDB\n    # depends_on:\n    #   - db\n    build:\n      context: .\n      dockerfile: Dockerfile\n    \n    # do port mapping so that we can access the app from the browser\n    ports:\n      - 3000:3000\n    \n    # use docker compose to watch for changes and rebuild the container\n    develop:\n      watch:\n        - path: ./package.json\n          action: rebuild\n        - path: ./next.config.js\n          action: rebuild\n        - path: ./package-lock.json\n          action: rebuild\n        - path: .\n          target: /app\n          action: sync\n    \n    # define the environment variables\n    environment:\n      # we're using MongoDB atlas so we need to pass in the connection string\n      DB_URL: mongodb+srv://sujata:rnZzJjIDr3bIDymV@cluster0.hnn88vs.mongodb.net/ \n\n  # we're using MongoDB atlas so we don't need to run a local instance of MongoDB\n  # but if you want to run a local instance, you can do it this way\n  # db:\n  #   image: mongo\n  #   ports:\n  #     - 27017:27017\n  #   environment:\n  #     - MONGO_INITDB_ROOT_USERNAME=sujata\n  #     - MONGO_INITDB_ROOT_PASSWORD=rnZzJjIDr3bIDymV\n  #   volumes:\n  #     - tasked:/data/db\n    \nvolumes:\n  tasked:\n```\n\n</details>\n\n## <a name=\"more\">üöÄ More</a>\n\n**Advance your skills with Next.js 14 Pro Course**\n\nEnjoyed creating this project? Dive deeper into our PRO courses for a richer learning adventure. They're packed with detailed explanations, cool features, and exercises to boost your skills. Give it a go!\n\n<a href=\"https://jsmastery.pro/next14\" target=\"_blank\">\n<img src=\"https://github.com/sujatagunale/EasyRead/assets/151519281/557837ce-f612-4530-ab24-189e75133c71\" alt=\"Project Banner\">\n</a>\n\n<br />\n<br />\n\n**Accelerate your professional journey with the Expert Training program**\n\nAnd if you're hungry for more than just a course and want to understand how we learn and tackle tech challenges, hop into our personalized masterclass. We cover best practices, different web skills, and offer mentorship to boost your confidence. Let's learn and grow together!\n\n<a href=\"https://www.jsmastery.pro/masterclass\" target=\"_blank\">\n<img src=\"https://github.com/sujatagunale/EasyRead/assets/151519281/fed352ad-f27b-400d-9b8f-c7fe628acb84\" alt=\"Project Banner\">\n</a>\n\n#\n"}
