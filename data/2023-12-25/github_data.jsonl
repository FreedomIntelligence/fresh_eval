{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/johnma2006/mamba-minimal", "text_blocks": "## mamba-minimal\n\nSimple, minimal implementation of Mamba in one file of PyTorch.\n\nFeaturing:\n* Equivalent numerical output as official implementation for both forward and backward pass\n* Simplified, readable, annotated code\n\nDoes NOT include:\n* Speed. The official implementation is heavily optimized, and these optimizations are core contributions of the Mamba paper. I kept most implementations simple for readability.\n* Proper parameter initialization (though this could be added without sacrificing readability)\n\n## Demo\n\nSee [demo.ipynb](demo.ipynb) for examples of prompt completions.\n\n```python\nfrom model import Mamba\nfrom transformers import AutoTokenizer\n\nmodel = Mamba.from_pretrained('state-spaces/mamba-370m')\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n\ngenerate(model, tokenizer, 'Mamba is the')\n```\n> Mamba is the world's longest venomous snake with an estimated length of over 150 m. With such a large size and a venomous bite, Mamba kills by stabbing the victim (which is more painful and less effective than a single stab of the bite)\n\n150 meters... ü´¢ scary!\n\n## References\n\nThe Mamba architecture was introduced in [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) by [Albert Gu](https://twitter.com/_albertgu?lang=en) and [Tri Dao](https://twitter.com/tri_dao?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor).\n\nThe official implementation is here: https://github.com/state-spaces/mamba/tree/main\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/mnotgod96/AppAgent", "text_blocks": "# AppAgent\n\n<div align=\"center\">\n\n<a href='https://arxiv.org/abs/2312.13771'><img src='https://img.shields.io/badge/arXiv-2312.13771-b31b1b.svg'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n <a href='https://appagent-official.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n <a href='https://github.com/buaacyw/GaussianEditor/blob/master/LICENSE.txt'><img src='https://img.shields.io/badge/License-MIT-blue'></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n <a href=\"https://twitter.com/dr_chizhang\"><img src=\"https://img.shields.io/twitter/follow/dr_chizhang?style=social\" alt=\"Twitter Follow\"></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n <br><br>\n <!-- [![Model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue)](https://huggingface.co/listen2you002/ChartLlama-13b) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/listen2you002/ChartLlama-Dataset) -->\n\n[**Chi Zhang***](https://icoz69.github.io/), [**Zhao Yang***](https://github.com/yz93), [**Jiaxuan Liu***](https://www.linkedin.com/in/jiaxuan-liu-9051b7105/), [Yucheng Han](http://tingxueronghua.github.io), [Xin Chen](https://chenxin.tech/), [Zebiao Huang](),\n<br>\n[Bin Fu](https://openreview.net/profile?id=~BIN_FU2), [Gang Yu (Corresponding Author)](https://www.skicyyu.org/)\n<br>\n(* equal contributions)\n</div>\n\n![](./assets/teaser.png)\n\n## üîÜ Introduction\n\nWe introduce a novel LLM-based multimodal agent framework designed to operate smartphone applications. \n\nOur framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps.\n\nCentral to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications.\n\n## üìù Changelog\n- __[2023.12.21]__: üî•üî• Open-source the git repository, including the detailed configuration steps to implement our AppAgent!\n\n## ‚ú® Demo\n\nThe demo video shows the process of using AppAgent to follow a user on X (Twitter) in the deployment phase.\n\nhttps://github.com/mnotgod96/AppAgent/assets/40715314/db99d650-dec1-4531-b4b2-e085bfcadfb7\n\nAn interesting experiment showing AppAgent's ability to pass CAPTCHA.\n\nhttps://github.com/mnotgod96/AppAgent/assets/27103154/5cc7ba50-dbab-42a0-a411-a9a862482548\n\n## üöÄ Quick Start\n\nThis section will guide you on how to quickly use gpt-4-vision-preview as an agent to complete specific tasks for you on\nyour Android app.\n\n### ‚öôÔ∏è Step 1. Prerequisites \n\n1. Get an Android device and enable the USB debugging that can be found in Developer Options in Settings.\n\n2. On your PC, download and install [Android Debug Bridge](https://developer.android.com/tools/adb) (adb) which is a \ncommand-line tool that lets you communicate with your Android device from the PC.\n\n3. Connect your device to your PC using a USB cable.\n\n4. Clone this repo and install the dependencies. All scripts in this project are written in Python 3 so make sure you \nhave installed it.\n\n```bash\ncd AppAgent\npip install -r requirements.txt\n```\n\n### ü§ñ Step 2. Configure the Agent\n\nAppAgent needs to be powered by a multi-modal model which can receive both text and visual inputs. During our experiment\n, we used `gpt-4-vision-preview` as the model to make decisions on how to take actions to complete a task on the smartphone.\n\nTo configure your requests to GPT-4V, you should modify `config.yaml` in the root directory.\nThere are two key parameters that must be configured to try AppAgent:\n1. OpenAI API key: you must purchase an eligible API key from OpenAI so that you can have access to GPT-4V.\n2. Request interval: this is the time interval in seconds between consecutive GPT-4V requests to control the frequency \nof your requests to GPT-4V. Adjust this value according to the status of your account.\n\nOther parameters in `config.yaml` are well commented. Modify them as you need.\n\n> Be aware that GPT-4V is not free. Each request/response pair involved in this project costs around $0.03. Use it wisely.\n\nIf you want to test AppAgent using your own models, you should modify the `ask_gpt_4v` function in `scripts/model.py` \naccordingly.\n\n### üîç Step 3. Exploration Phase\n\nOur paper proposed a novel solution that involves two phases, exploration and deployment, to turn GPT-4V into a capable \nagent that can help users operate their Android phones when a task is given. The exploration phase starts with a task \ngiven by you, and you can choose to let the agent either explore the app on its own or learn from your demonstration. \nIn both cases, the agent generates documentations for elements interacted during the exploration/demonstration and \nsaves them for use in the deployment phase.\n\n#### Option 1: Autonomous Exploration\n\nThis solution features a fully autonomous exploration which allows the agent to explore the use of the app by attempting\nthe given task without any intervention from humans.\n\nTo start, run `learn.py` in the root directory. Follow prompted instructions to select `autonomous exploration` as the \noperating mode and provide the app name and task description. Then, your agent will do the job for you. Under this \nmode, AppAgent will reflect on its previous action making sure its action adheres to the given task and generate \ndocumentations for the elements explored.\n\n```bash\npython learn.py\n```\n\n#### Option 2: Learning from Human Demonstrations\n\nThis solution requires users to demonstrate a similar task first. AppAgent will learn from the demo and generate \ndocumentations for UI elements seen during the demo.\n\nTo start human demonstration, you should run `learn.py` in the root directory. Follow prompted instructions to select \n`human demonstration` as the operating mode and provide the app name and task description. A screenshot of your phone \nwill be captured and all interactive elements shown on the screen will be labeled with numeric tags. You need to follow \nthe prompts to determine your next action and the target of the action. When you believe the demonstration is finished, \ntype `stop` to end the demo.\n\n```bash\npython learn.py\n```\n\n![](./assets/demo.png)\n\n### üì± Step 4. Deployment Phase\n\nAfter the exploration phase finishes, you can run `run.py` in the root directory. Follow prompted instructions to enter \nthe name of the app, select the appropriate documentation base you want the agent to use, and provide the task \ndescription. Then, your agent will do the job for you. The agent will automatically detect if there is documentation \nbase generated before for the app; if there is no documentation found, you can also choose to run the agent without any \ndocumentation (success rate not guaranteed).\n\n```bash\npython run.py\n```\n\n## üìñ TO-DO LIST\n- [ ] Open source the Benchmark.\n- [x] Open source the configuration.\n\n## üòâ Citation\n```bib\n@misc{yang2023appagent,\n      title={AppAgent: Multimodal Agents as Smartphone Users}, \n      author={Chi Zhang and Zhao Yang and Jiaxuan Liu and Yucheng Han and Xin Chen and Zebiao Huang and Bin Fu and Gang Yu},\n      year={2023},\n      eprint={2312.13771},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=mnotgod96/AppAgent&type=Date)](https://star-history.com/#mnotgod96/AppAgent&Date)\n\n\n## License\nThe [MIT license](./assets/license.txt).\nMIT License\n\nCopyright (c) 2023 Jiaxuan Liu\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n\nOPENAI_API_BASE: \"https://api.openai.com/v1\"\nOPENAI_API_KEY: \"sk-\"  # Set the value to sk-xxx if you host the openai interface for open llm model\nOPENAI_API_MODEL: \"gpt-4-vision-preview\"  # The only OpenAI model by now that accepts visual input\nMAX_TOKENS: 300  # The max token limit for the response completion\nTEMPERATURE: 0.0  # The temperature of the model: the lower the value, the more consistent the output of the model\nREQUEST_INTERVAL: 10  # Time in seconds between consecutive GPT-4V requests\n\nANDROID_SCREENSHOT_DIR: \"/sdcard/Pictures/Screenshots\"  # Set the directory on your Android device to store the intermediate screenshots. Make sure the directory EXISTS on your phone!\nANDROID_XML_DIR: \"/sdcard\"  # Set the directory on your Android device to store the intermediate XML files used for determining locations of UI elements on your screen. Make sure the directory EXISTS on your phone!\n\nDOC_REFINE: false  # Set this to true will make the agent refine existing documentation based on the latest demonstration; otherwise, the agent will not regenerate a new documentation for elements with the same resource ID.\nMAX_ROUNDS: 20  # Set the round limit for the agent to complete the task\nDARK_MODE: false  # Set this to true if your app is in dark mode to enhance the element labeling\nMIN_DIST: 30  # The minimum distance between elements to prevent overlapping during the labeling process\n\n\nimport argparse\nimport datetime\nimport os\nimport time\n\nfrom scripts.utils import print_with_color\n\narg_desc = \"AppAgent - exploration phase\"\nparser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=arg_desc)\nparser.add_argument(\"--app\")\nparser.add_argument(\"--root_dir\", default=\"./\")\nargs = vars(parser.parse_args())\n\napp = args[\"app\"]\nroot_dir = args[\"root_dir\"]\n\n\nprint_with_color(\"Welcome to the exploration phase of AppAgent!\\nThe exploration phase aims at generating \"\n                 \"documentations for UI elements through either autonomous exploration or human demonstration. \"\n                 \"Both options are task-oriented, which means you need to give a task description. During \"\n                 \"autonomous exploration, the agent will try to complete the task by interacting with possible \"\n                 \"elements on the UI within limited rounds. Documentations will be generated during the process of \"\n                 \"interacting with the correct elements to proceed with the task. Human demonstration relies on \"\n                 \"the user to show the agent how to complete the given task, and the agent will generate \"\n                 \"documentations for the elements interacted during the human demo. To start, please enter the \"\n                 \"main interface of the app on your phone.\", \"yellow\")\nprint_with_color(\"Choose from the following modes:\\n1. autonomous exploration\\n2. human demonstration\\n\"\n                 \"Type 1 or 2.\", \"blue\")\nuser_input = \"\"\nwhile user_input != \"1\" and user_input != \"2\":\n    user_input = input()\n\nif not app:\n    print_with_color(\"What is the name of the target app?\", \"blue\")\n    app = input()\n    app = app.replace(\" \", \"\")\n\nif user_input == \"1\":\n    os.system(f\"python scripts/self_explorer.py --app {app} --root_dir {root_dir}\")\nelse:\n    demo_timestamp = int(time.time())\n    demo_name = datetime.datetime.fromtimestamp(demo_timestamp).strftime(f\"demo_{app}_%Y-%m-%d_%H-%M-%S\")\n    os.system(f\"python scripts/step_recorder.py --app {app} --demo {demo_name} --root_dir {root_dir}\")\n    os.system(f\"python scripts/document_generation.py --app {app} --demo {demo_name} --root_dir {root_dir}\")\n\n\n\nargparse\ncolorama\nopencv-python\npyshine\npyyaml\nrequests\n\n\nimport argparse\nimport os\n\nfrom scripts.utils import print_with_color\n\narg_desc = \"AppAgent - deployment phase\"\nparser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=arg_desc)\nparser.add_argument(\"--app\")\nparser.add_argument(\"--root_dir\", default=\"./\")\nargs = vars(parser.parse_args())\n\napp = args[\"app\"]\nroot_dir = args[\"root_dir\"]\n\nprint_with_color(\"Welcome to the deployment phase of AppAgent!\\nBefore giving me the task, you should first tell me \"\n                 \"the name of the app you want me to operate and what documentation base you want me to use. I will \"\n                 \"try my best to complete the task without your intervention. First, please enter the main interface \"\n                 \"of the app on your phone and provide the following information.\", \"yellow\")\n\nif not app:\n    print_with_color(\"What is the name of the target app?\", \"blue\")\n    app = input()\n    app = app.replace(\" \", \"\")\n\nos.system(f\"python scripts/task_executor.py --app {app} --root_dir {root_dir}\")\n\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/beeper/imessage", "text_blocks": "# beeper-imessage\nA Matrix-iMessage puppeting bridge.\n\n## Documentation\nThe bridge works like any other mautrix-go bridge, so the instructions at\n<https://docs.mau.fi/bridges/go/setup.html> can be applied directly.\nYou can find precompiled binaries from the GitLab CI at\n<https://mau.dev/mautrix/imessagego>.\n\nAdditionally, the bridge requires a registration provider running on a [Mac] or\n[jailbroken iPhone], as well as a [relay server] to help the bridge and\nregistration provider connect to each other.\n\n[Mac]: https://github.com/beeper/mac-registration-provider\n[jailbroken iPhone]: https://github.com/beeper/phone-registration-provider\n[relay server]: https://github.com/beeper/registration-relay\n\nWhen connecting the bridge to your Beeper account with bbctl, you don't need to\nself-host the relay, you only need to run the provider.\n\n## Discussion\nMatrix room: [#imessage:maunium.net](https://matrix.to/#/#imessage:maunium.net)\nroot = true\n\n[*]\nindent_style = tab\nindent_size = 4\nend_of_line = lf\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.md]\ntrim_trailing_whitespace = false\nindent_style = space\nindent_size = 2\n\n[*.{yaml,yml}]\nindent_style = space\nindent_size = 2\n\n[example-config.yaml]\nindent_size = 4\n\n[{Dockerfile,*.py}]\nindent_style = space\nindent_size = 4\n\n\n\n*.gen.go linguist-generated=true\n\n\n\n.idea\n\n*.yaml\n!.pre-commit-config.yaml\n!example-config.yaml\n!example-registration.yaml\n\n*.json\n/attachments\n!/imessage/direct/ids/types/errors.json\n!/imessage/direct/util/utitype/mime-to-uti.json\n\n*.session\n*.db*\n*.log\n\n/beeper-imessage\n/start\n/.android-ipc-bin\n\n\n\ninclude:\n- project: 'mautrix/ci'\n  file: '/go.yml'\n\nvariables:\n  BINARY_NAME: beeper-imessage\n\n\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n        exclude_types: [markdown]\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/tekwizely/pre-commit-golang\n    rev: v1.0.0-rc.1\n    hooks:\n      - id: go-imports-repo\n        args:\n          - \"-local\"\n          - \"github.com/beeper/imessage\"\n          - \"-w\"\n\n\n\nFROM golang:1-alpine3.19 AS builder\n\nARG COMMIT_HASH\nENV COMMIT_HASH=${COMMIT_HASH}\n\nRUN apk add --no-cache bash git ca-certificates build-base su-exec olm-dev\n\nCOPY . /build\nWORKDIR /build\nRUN ./build.sh\n\nFROM alpine:3.19\n\nENV UID=1337 \\\n    GID=1337\n\nRUN apk add --no-cache ffmpeg su-exec ca-certificates olm bash jq yq curl\n\nCOPY --from=builder /build/beeper-imessage /usr/bin/beeper-imessage\nCOPY --from=builder /build/example-config.yaml /opt/beeper-imessage/example-config.yaml\nCOPY --from=builder /build/docker-run.sh /docker-run.sh\nVOLUME /data\n\nCMD [\"/docker-run.sh\"]\n\n\n\nFROM alpine:3.19\n\nENV UID=1337 \\\n    GID=1337\n\nRUN apk add --no-cache ffmpeg su-exec ca-certificates bash jq curl yq\n\nARG EXECUTABLE=./beeper-imessage\nCOPY $EXECUTABLE /usr/bin/beeper-imessage\nCOPY ./example-config.yaml /opt/beeper-imessage/example-config.yaml\nCOPY ./docker-run.sh /docker-run.sh\nVOLUME /data\n\nCMD [\"/docker-run.sh\"]\n\n\n\n#!/usr/bin/env bash\nset -e\nrepo_dir=$(dirname $(realpath $0))\npushd $repo_dir > /dev/null\nLDFLAGS=\"-X main.Tag=$(git describe --exact-match --tags 2>/dev/null) -X main.Commit=$(git rev-parse HEAD) -X 'main.BuildTime=$(date '+%b %_d %Y, %H:%M:%S')'\"\ngo build -ldflags \"$LDFLAGS\" -o .android-ipc-bin ./imessage/ipc/\npopd > /dev/null\nrlwrap -H \"$HOME/.android-ipc_history\" $repo_dir/.android-ipc-bin 2> >(zeroparse >&2) | (trap '' INT; jq -c)\n\n\n\n#!/bin/bash\nset -xe\nbuild_tags=\"\"\n\ncommit=${COMMIT_HASH:-$(git rev-parse HEAD)}\n\nif [[ $(arch) == \"arm64\" && -z \"$LIBRARY_PATH\" && -d /opt/homebrew ]]; then\n\techo \"Using /opt/homebrew for LIBRARY_PATH and CPATH\"\n\texport LIBRARY_PATH=/opt/homebrew/lib\n\texport CPATH=/opt/homebrew/include\n\tHEIF_PATH=\"$LIBRARY_PATH\"\nelse\n\tHEIF_PATH=\"/usr/local/lib\"\nfi\n\nif [[ -f \"$HEIF_PATH/libheif.1.dylib\" ]]; then\n\techo \"libheif found in $HEIF_PATH, compiling with heif support\"\n\tbuild_tags=\"libheif\"\nelse\n\techo \"libheif not found in $HEIF_PATH, compiling without heif support\"\nfi\n\ngo build -o beeper-imessage -tags \"$build_tags\" -ldflags \"-X main.Commit=$commit -X 'main.BuildTime=`date '+%b %_d %Y, %H:%M:%S'`'\"\n\n\n\n// beeper-imessage - A Matrix-iMessage puppeting bridge.\n// Copyright (C) 2022 Tulir Asokan\n//\n// This program is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Affero General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// This program is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n// GNU Affero General Public License for more details.\n//\n// You should have received a copy of the GNU Affero General Public License\n// along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\npackage main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"maunium.net/go/mautrix/appservice\"\n\t\"maunium.net/go/mautrix/bridge\"\n\t\"maunium.net/go/mautrix/id\"\n)\n\nvar _ bridge.DoublePuppet = (*User)(nil)\n\nfunc (user *User) SwitchCustomMXID(accessToken string, mxid id.UserID) error {\n\tif mxid != user.MXID {\n\t\treturn errors.New(\"mismatching mxid\")\n\t}\n\tuser.DoublePuppetIntent = nil\n\tuser.AccessToken = accessToken\n\terr := user.StartCustomMXID(false)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (user *User) ClearCustomMXID() {\n\tuser.DoublePuppetIntent = nil\n\tuser.AccessToken = \"\"\n\terr := user.Update(context.TODO())\n\tif err != nil {\n\t\tuser.zlog.Warn().Err(err).Msg(\"Failed to clear access token from database\")\n\t}\n}\n\nfunc (user *User) StartCustomMXID(reloginOnFail bool) error {\n\tnewIntent, newAccessToken, err := user.bridge.DoublePuppet.Setup(user.MXID, user.AccessToken, reloginOnFail)\n\tif err != nil {\n\t\tuser.ClearCustomMXID()\n\t\treturn err\n\t}\n\tif user.AccessToken != newAccessToken {\n\t\tuser.AccessToken = newAccessToken\n\t\terr = user.Update(context.TODO())\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to save access token: %w\", err)\n\t\t}\n\t}\n\tuser.DoublePuppetIntent = newIntent\n\treturn nil\n}\n\nfunc (user *User) CustomIntent() *appservice.IntentAPI {\n\treturn user.DoublePuppetIntent\n}\n\nfunc (user *User) tryAutomaticDoublePuppeting() {\n\tif !user.bridge.Config.CanAutoDoublePuppet(user.MXID) || user.DoublePuppetIntent != nil {\n\t\treturn\n\t}\n\terr := user.StartCustomMXID(true)\n\tif err != nil {\n\t\tuser.zlog.Warn().Err(err).Msg(\"Failed to login with shared secret for double puppeting\")\n\t} else {\n\t\tuser.zlog.Info().Msg(\"Successfully automatically enabled double puppet\")\n\t}\n}\n\n\n\n#!/bin/sh\n\nif [[ -z \"$GID\" ]]; then\n\tGID=\"$UID\"\nfi\n\n# Define functions.\nfunction fixperms {\n\tchown -R $UID:$GID /data\n\n\t# /opt/beeper-imessage is read-only, so disable file logging if it's pointing there.\n\tif [[ \"$(yq e '.logging.writers[1].filename' /data/config.yaml)\" == \"./logs/beeper-imessage.log\" ]]; then\n\t\tyq -I4 e -i 'del(.logging.writers[1])' /data/config.yaml\n\tfi\n}\n\nif [[ ! -f /data/config.yaml ]]; then\n\tcp /opt/beeper-imessage/example-config.yaml /data/config.yaml\n\techo \"Didn't find a config file.\"\n\techo \"Copied default config file to /data/config.yaml\"\n\techo \"Modify that config file to your liking.\"\n\techo \"Start the container again after that to generate the registration file.\"\n\texit\nfi\n\nif [[ ! -f /data/registration.yaml ]]; then\n\t/usr/bin/beeper-imessage -g -c /data/config.yaml -r /data/registration.yaml || exit $?\n\techo \"Didn't find a registration file.\"\n\techo \"Generated one for you.\"\n\techo \"See https://docs.mau.fi/bridges/general/registering-appservices.html on how to use it.\"\n\texit\nfi\n\ncd /data\nfixperms\nexec su-exec $UID:$GID /usr/bin/beeper-imessage\n\n\n\nmodule github.com/beeper/imessage\n\ngo 1.21\n\nrequire (\n\tgithub.com/emersion/go-vcard v0.0.0-20230815062825-8fda7d206ec9\n\tgithub.com/gabriel-vasile/mimetype v1.4.3\n\tgithub.com/google/uuid v1.5.0\n\tgithub.com/gorilla/mux v1.8.0\n\tgithub.com/mattn/go-sqlite3 v1.14.19\n\tgithub.com/nyaruka/phonenumbers v1.3.0\n\tgithub.com/rs/zerolog v1.31.0\n\tgithub.com/stretchr/testify v1.8.4\n\tgithub.com/strukturag/libheif v1.17.6\n\tgithub.com/tidwall/gjson v1.17.0\n\tgo.mau.fi/util v0.2.2-0.20231120145840-55dca048d0d9\n\tgo.mau.fi/zeroconfig v0.1.2\n\tgo4.org v0.0.0-20230225012048-214862532bf5\n\tgolang.org/x/crypto v0.17.0\n\tgolang.org/x/exp v0.0.0-20231219180239-dc181d75b848\n\tgolang.org/x/image v0.14.0\n\tgolang.org/x/net v0.19.0\n\tgolang.org/x/sys v0.15.0\n\tgolang.org/x/time v0.5.0\n\tgoogle.golang.org/protobuf v1.31.0\n\thowett.net/plist v1.0.1\n\tmaunium.net/go/maulogger/v2 v2.4.1\n\tmaunium.net/go/mautrix v0.16.3-0.20231117160133-4784d6d09fe2\n)\n\nrequire (\n\tgithub.com/coreos/go-systemd/v22 v22.5.0 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/gorilla/websocket v1.5.0 // indirect\n\tgithub.com/kr/pretty v0.3.1 // indirect\n\tgithub.com/lib/pq v1.10.9 // indirect\n\tgithub.com/mattn/go-colorable v0.1.13 // indirect\n\tgithub.com/mattn/go-isatty v0.0.19 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/tidwall/match v1.1.1 // indirect\n\tgithub.com/tidwall/pretty v1.2.0 // indirect\n\tgithub.com/tidwall/sjson v1.2.5 // indirect\n\tgithub.com/yuin/goldmark v1.6.0 // indirect\n\tgolang.org/x/text v0.14.0 // indirect\n\tgopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c // indirect\n\tgopkg.in/natefinch/lumberjack.v2 v2.2.1 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n\tmaunium.net/go/mauflag v1.0.0 // indirect\n)\n\n// we only need heif from go4.org, so make sure other bloat can't be included\nexclude (\n\tcloud.google.com/go v0.53.0\n\tcloud.google.com/go/storage v1.5.0\n\tgolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d\n\tgoogle.golang.org/api v0.17.0\n)\n\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/JShollaj/awesome-llm-interpretability", "text_blocks": "\n# Awesome LLM Interpretability ![](https://github.com/your-username/awesome-llm-interpretability/workflows/Awesome%20Bot/badge.svg)\n\nA curated list of amazingly awesome tools, papers, articles, and communities focused on Large Language Model (LLM) Interpretability.\n\n\n## Table of Contents\n- [Awesome LLM Interpretability](#awesome-llm-interpretability)\n    - [LLM Interpretability Tools](#llm-interpretability-tools)\n    - [LLM Interpretability Papers](#llm-interpretability-papers)\n    - [LLM Interpretability Articles](#llm-interpretability-articles)\n    - [LLM Interpretability Groups](#llm-interpretability-groups)\n\n### LLM Interpretability Tools\n*Tools and libraries for LLM interpretability and analysis.*\n\n1. [Comgra](https://github.com/FlorianDietz/comgra) - Comgra helps you analyze and debug neural networks in pytorch.\n2. [Pythia](https://github.com/EleutherAI/pythia) - Interpretability analysis to understand how knowledge develops and evolves during training in autoregressive transformers.\n3. [Phoenix](https://github.com/Arize-ai/phoenix) - AI Observability & Evaluation - Evaluate, troubleshoot, and fine tune your LLM, CV, and NLP models in a notebook.\n4. [Automated Interpretability](https://github.com/openai/automated-interpretability) - Code for automatically generating, simulating, and scoring explanations of neuron behavior.\n5. [Fmr.ai](https://github.com/BizarreCake/fmr.ai) - AI interpretability and explainability platform.\n6. [Attention Analysis](https://github.com/clarkkev/attention-analysis) - Analyzing attention maps from BERT transformer.\n7. [SpellGPT](https://github.com/mwatkins1970/SpellGPT) - Explores GPT-3‚Äôs ability to spell own token strings.\n8. [SuperICL](https://github.com/JetRunner/SuperICL) - Super In-Context Learning code which allows black-box LLMs to work with locally fine-tuned smaller models.\n9. [Git Re-Basin](https://github.com/samuela/git-re-basin) - Code release for \"Git Re-Basin: Merging Models modulo Permutation Symmetries.‚Äù\n10. [Functionary](https://github.com/MeetKai/functionary) - Chat language model that can interpret and execute functions/plugins.\n11. [Sparse Autoencoder](https://github.com/ai-safety-foundation/sparse_autoencoder) - Sparse Autoencoder for Mechanistic Interpretability.\n12. [Rome](https://github.com/kmeng01/rome) - Locating and editing factual associations in GPT.\n13. [Inseq](https://github.com/inseq-team/inseq) - Interpretability for sequence generation models.\n14. [Neuron Viewer](https://openaipublic.blob.core.windows.net/neuron-explainer/neuron-viewer/index.html) - Tool for viewing neuron activations and explanations.\n15. [LLM Visualization](https://bbycroft.net/llm) - Visualizing LLMs in low level.\n\n---\n\n### LLM Interpretability Papers\n*Academic and industry papers on LLM interpretability.*\n\n\n1. [Finding Neurons in a Haystack: Case Studies with Sparse Probing](https://arxiv.org/abs/2305.01610) - Explores the representation of high-level human-interpretable features within neuron activations of large language models (LLMs).\n2. [Copy Suppression: Comprehensively Understanding an Attention Head](https://arxiv.org/abs/2310.04625) - Investigates a specific attention head in GPT-2 Small, revealing its primary role in copy suppression.\n3. [Linear Representations of Sentiment in Large Language Models](https://arxiv.org/abs/2310.15154) - Shows how sentiment is represented in Large Language Models (LLMs), finding that sentiment is linearly represented in these models.\n4. [Emergent world representations: Exploring a sequence model trained on a synthetic task](https://arxiv.org/abs/2210.13382) - Explores emergent internal representations in a GPT variant trained to predict legal moves in the board game Othello.\n5. [Towards Automated Circuit Discovery for Mechanistic Interpretability](https://arxiv.org/abs/2304.14997) - Introduces the Automatic Circuit Discovery (ACDC) algorithm for identifying important units in neural networks.\n6. [A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations](https://arxiv.org/abs/2302.03025) - Examines small neural networks to understand how they learn group compositions, using representation theory.\n7. [Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias](https://arxiv.org/abs/2004.12265) - Causal mediation analysis as a method for interpreting neural models in natural language processing.\n8. [The Quantization Model of Neural Scaling](https://arxiv.org/abs/2303.13506) - Proposes the Quantization Model for explaining neural scaling laws in neural networks.\n9. [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) - Presents a method for extracting accurate answers to yes-no questions from language models' internal activations without supervision.\n10. [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model](https://arxiv.org/abs/2305.00586) - Analyzes mathematical capabilities of GPT-2 Small, focusing on its ability to perform the 'greater-than' operation.\n11. [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html) - Using a sparse autoencoder to decompose the activations of a one-layer transformer into interpretable, monosemantic features.\n12. [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) - Explores how language models like GPT-4 can be used to explain the functioning of neurons within similar models.\n13. [Emergent Linear Representations in World Models of Self-Supervised Sequence Models](https://arxiv.org/abs/2309.00941) - Linear representations in a world model of Othello-playing sequence models.\n14. [\"Toward a Mechanistic Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model\"](https://openreview.net/forum?id=VJEcAnFPqC) - Explores stepwise inference in autoregressive language models using a synthetic task based on navigating directed acyclic graphs.\n15. [\"Successor Heads: Recurring, Interpretable Attention Heads In The Wild\"](https://openreview.net/forum?id=kvcbV8KQsi) - Introduces 'successor heads,' attention heads that increment tokens with a natural ordering, such as numbers and days, in LLM‚Äôs.\n16. [\"Large Language Models Are Not Robust Multiple Choice Selectors\"](https://openreview.net/forum?id=shr9PXz7T0) - Analyzes the bias and robustness of LLMs in multiple-choice questions, revealing their vulnerability to option position changes due to inherent \"selection bias‚Äù.\n17. [\"Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory\"](https://openreview.net/forum?id=4bSQ3lsfEV) - Presents a novel approach to understanding neural networks by examining feature complexity through category theory.\n18. [\"Let's Verify Step by Step\"](https://openreview.net/forum?id=v8L0pN6EOi) - Focuses on improving the reliability of LLMs in multi-step reasoning tasks using step-level human feedback.\n19. [\"Interpretability Illusions in the Generalization of Simplified Models\"](https://openreview.net/forum?id=v675Iyu0ta) - Examines the limitations of simplified representations (like SVD) used to interpret deep learning systems, especially in out-of-distribution scenarios.\n20. [\"The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models\"](https://openreview.net/forum?id=SQGUDc9tC8) - Presents a novel approach for identifying and mitigating social biases in language models, introducing the concept of 'Social Bias Neurons'.\n21. [\"Interpreting the Inner Mechanisms of Large Language Models in Mathematical Addition\"](https://openreview.net/forum?id=VpCqrMMGVm) - Investigates how LLMs perform the task of mathematical addition.\n22. [\"Measuring Feature Sparsity in Language Models\"](https://openreview.net/forum?id=SznHfMwmjG) - Develops metrics to evaluate the success of sparse coding techniques in language model activations.\n23. [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) - Investigates how models represent more features than dimensions, especially when features are sparse.\n24. [Spine: Sparse interpretable neural embeddings](https://arxiv.org/abs/1711.08792) - Presents SPINE, a method transforming dense word embeddings into sparse, interpretable ones using denoising autoencoders.\n25. [Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors](https://arxiv.org/abs/2103.15949) - Introduces a novel method for visualizing transformer networks using dictionary learning.\n26. [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2310.11207) - Introduces Pythia, a toolset designed for analyzing the training and scaling behaviors of LLMs.\n27. [On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron](https://zuva.ai/science/interpretability-and-feature-representations/) - Critically examines the effectiveness of the \"Sentiment Neuron‚Äù.\n28. [Engineering monosemanticity in toy models](https://arxiv.org/abs/2211.09169) - Explores engineering monosemanticity in neural networks, where individual neurons correspond to distinct features.\n29. [Polysemanticity and capacity in neural networks](https://arxiv.org/abs/2210.01892) - Investigates polysemanticity in neural networks, where individual neurons represent multiple features.\n30. [An Overview of Early Vision in InceptionV1](https://distill.pub/2020/circuits/early-vision/) - A comprehensive exploration of the initial five layers of the InceptionV1 neural network, focusing on early vision.\n31. [Visualizing and measuring the geometry of BERT](https://arxiv.org/abs/1906.02715) - Delves into BERT's internal representation of linguistic information, focusing on both syntactic and semantic aspects.\n32. [Neurons in Large Language Models: Dead, N-gram, Positional](https://arxiv.org/abs/2309.04827) - An analysis of neurons in large language models, focusing on the OPT family.\n33. [Can Large Language Models Explain Themselves?](https://arxiv.org/abs/2310.11207) - Evaluates the effectiveness of self-explanations generated by LLMs in sentiment analysis tasks.\n34. [Interpretability in the Wild: GPT-2 small (arXiv)](https://arxiv.org/abs/2211.00593) - Provides a mechanistic explanation of how GPT-2 small performs indirect object identification (IOI) in natural language processing.\n35. [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/abs/2310.11207) - Explores the use of sparse autoencoders to extract more interpretable and less polysemantic features from LLMs.\n36. [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2310.11207) - Investigates the use of sparse autoencoders for enhancing the interpretability of features in LLMs.\n37. [Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars](https://arxiv.org/abs/2312.01429) - Demonstrates that focusing only on specific parts like attention heads or weight matrices in Transformers can lead to misleading interpretability claims.\n38. [The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets](https://arxiv.org/abs/2310.06824) - This paper investigates the representation of truth in Large Language Models (LLMs) using true/false datasets.\n39. [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809) - This study presents Boundless Distributed Alignment Search (Boundless DAS), an advanced method for interpreting LLMs like Alpaca.\n40. [Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/abs/2310.01405) - Introduces Representation Engineering (RepE), a novel approach for enhancing AI transparency, focusing on high-level representations rather than neurons or circuits.\n\n---\n\n### LLM Interpretability Articles\n*Insightful articles and blog posts on LLM interpretability.*\n\n1. [A New Approach to Computation Reimagines Artificial Intelligenceg](https://www.quantamagazine.org/a-new-approach-to-computation-reimagines-artificial-intelligence-20230413/?mc_cid=ad9a93c472&mc_eid=506130a407) - Discusses hyperdimensional computing, a novel method involving hyperdimensional vectors (hypervectors) for more efficient, transparent, and robust artificial intelligence.\n2. [Interpreting GPT: the logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - Explores how the logit lens, reveals a gradual convergence of GPT's probabilistic predictions across its layers, from initial nonsensical or shallow guesses to more refined predictions.\n3. [A Mechanistic Interpretability Analysis of Grokking](https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking) - Explores the phenomenon of 'grokking' in deep learning, where models suddenly shift from memorization to generalization during training.\n4. [200 Concrete Open Problems in Mechanistic Interpretability](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj) - Series of posts discussing open research problems in the field of Mechanistic Interpretability (MI), which focuses on reverse-engineering neural networks.\n5. [Evaluating LLMs is a minefield](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/) - Challenges in assessing the performance and biases of large language models (LLMs) like GPT.\n6. [Attribution Patching: Activation Patching At Industrial Scale](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching) - Method that uses gradients for a linear approximation of activation patching in neural networks.\n7. [Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]](https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) - Introduces causal scrubbing, a method for evaluating the quality of mechanistic interpretations in neural networks.\n8. [A circuit for Python docstrings in a 4-layer attention-only transformer](https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only) - Proposes the Quantization Model for explaining neural scaling laws in neural networks.\n9. [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) - Examines a specific neural circuit within a 4-layer transformer model responsible for generating Python docstrings.\n\n---\n\n### LLM Interpretability Groups\n*Communities and groups dedicated to LLM interpretability.*\n\n1. [Alignment Lab AI](https://discord.com/invite/ad27GQgc7K) - Group of researchers focusing on AI alignment.\n8. [Nous Research](https://discord.com/invite/AVB9jkHZ) - Research group discussing various topics on interpretability.\n9. [EleutherAI](https://discord.com/invite/4pEBpVTN) - Non-profit AI research lab that focuses on interpretability and alignment of large models. \n\n---\n\n## Contributing and Collaborating\nPlease see [CONTRIBUTING](https://github.com/your-username/awesome-llm-interpretability/blob/master/CONTRIBUTING.md) and [CODE-OF-CONDUCT](https://github.com/your-username/awesome-llm-interpretability/blob/master/CODE-OF-CONDUCT.md) for details.\n\n# Awesome LLM Interpretability - Code of Conduct\n\nAs contributors and maintainers of the Awesome LLM Interpretability project, we pledge to create a welcoming and respectful environment for everyone who wants to participate. We value the diversity of our community and strive to provide a safe and inclusive space for all.\n\n## Our Standards\n\nTo ensure a positive and productive experience for everyone, we have established the following guidelines:\n\n### 1. Respect Each Other\n\nTreat all contributors, users, and maintainers with respect and kindness. Be open to different opinions and experiences. Disagreements are normal, but always be civil and constructive in your communication.\n\n### 2. Inclusive Language\n\nUse inclusive language that is respectful of different backgrounds and identities. Avoid offensive or exclusionary language, and respect people's preferred pronouns and identities.\n\n### 3. No Harassment or Discrimination\n\nHarassment or discrimination of any kind will not be tolerated. This includes, but is not limited to, harassment based on race, ethnicity, gender, sexual orientation, disability, age, religion, or any other protected characteristic.\n\n### 4. Be Mindful of Your Actions\n\nBe mindful of your words and actions. Do not engage in personal attacks, trolling, or any behavior that disrupts a positive and collaborative atmosphere within the community.\n\n### 5. Focus on the Project\n\nKeep discussions and interactions related to the Awesome LLM Interpretability project. Off-topic or spammy content should be avoided.\n\n## Reporting Violations\n\nIf you believe someone has violated the Code of Conduct, please report it to the project maintainers at [insert contact email or method]. All reports will be kept confidential, and we will take appropriate action to address the situation.\n\n## Enforcement\n\nMaintainers of the Awesome LLM Interpretability project are responsible for enforcing this Code of Conduct. In cases of reported violations, they will review and investigate the situation and take appropriate actions, which may include warnings, temporary or permanent bans from the project, or other measures deemed necessary.\n\n## Acknowledgment\n\nThis Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/version/2/0/code_of_conduct.html) and is inspired by the principles of openness, inclusivity, and respect for all contributors.\n\n## Questions and Clarifications\n\nIf you have any questions or need clarification regarding this Code of Conduct, please reach out to the project maintainers at [insert contact email or method].\n\nWe are committed to fostering a positive and inclusive community, and your cooperation in adhering to these guidelines is greatly appreciated.\n\nThank you for being a part of the Awesome LLM Interpretability community!\n\n\n\n\n# Awesome LLM Interpretability - Collaborating Guidelines\n\nWe welcome contributions from the community to make the Awesome LLM Interpretability project even more valuable. To ensure smooth collaboration and maintain the quality of the project, we have established the following guidelines for contributors:\n\n## How to Contribute\n\n1. **Fork the Repository**: Start by forking the Awesome LLM Interpretability repository to your own GitHub account.\n\n2. **Create a Branch**: Create a new branch in your forked repository for your contributions. It's a good practice to name the branch based on the feature or fix you're working on.\n\n3. **Make Changes**: Make your desired changes, improvements, or additions to the project. Please follow the project's style and formatting guidelines.\n\n4. **Testing**: If your contribution involves code changes, ensure that the code passes all existing tests. If relevant, add new tests to cover your changes.\n\n5. **Commit Changes**: Commit your changes with clear, descriptive commit messages. Reference any related issues or pull requests using issue numbers (e.g., \"Fixes #123\").\n\n6. **Create a Pull Request (PR)**: Once your changes are ready, create a pull request from your forked repository to the main Awesome LLM Interpretability repository. Provide a detailed description of your changes in the PR.\n\n7. **Review and Feedback**: Your PR will be reviewed by project maintainers and other contributors. Be responsive to feedback and make necessary updates to address any issues or concerns raised during the review.\n\n8. **Merging**: Once your PR is approved and passes all checks, it will be merged into the main branch by a project maintainer.\n\n## Code of Conduct\n\nContributors are expected to adhere to the [Code of Conduct](CODE-OF-CONDUCT.md) at all times when participating in the project. Respectful and inclusive behavior is essential for a positive collaborative environment.\n\n## Licensing\n\nBy contributing to the Awesome LLM Interpretability project, you agree that your contributions will be licensed under the same license as the project itself. Be sure to review the project's license (usually found in the repository's root directory) before contributing.\n\n## Attribution\n\nWe value and appreciate all contributions to the project. Contributors may be recognized in the project's documentation or credits section as a token of appreciation for their efforts.\n\n## Questions and Assistance\n\nIf you have any questions, need assistance, or want to discuss potential contributions before getting started, feel free to reach out to the project maintainers or the community in the relevant communication channels.\n\nThank you for your interest in collaborating on the Awesome LLM Interpretability project!\n\n\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/3DTopia/OpenLRM", "text_blocks": "# OpenLRM: Open-Source Large Reconstruction Models\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-yellow.svg)](LICENSE)\n[![Weight License](https://img.shields.io/badge/Weight%20License-CC%20By%20NC%204.0-red)](LICENSE_WEIGHT)\n[![LRM](https://img.shields.io/badge/LRM-Arxiv%20Link-green)](https://arxiv.org/abs/2311.04400)\n\n[![HF Models](https://img.shields.io/badge/Models-Huggingface%20Models-bron)](https://huggingface.co/zxhezexin/OpenLRM)\n[![HF Demo](https://img.shields.io/badge/Demo-Huggingface%20Demo-blue)](https://huggingface.co/spaces/zxhezexin/OpenLRM)\n\n<img src=\"assets/rendered_video/teaser.gif\" width=\"75%\" height=\"auto\"/>\n\n<div style=\"text-align: left\">\n    <img src=\"assets/mesh_snapshot/crop.owl.ply00.png\" width=\"12%\" height=\"auto\"/>\n    <img src=\"assets/mesh_snapshot/crop.owl.ply01.png\" width=\"12%\" height=\"auto\"/>\n    <img src=\"assets/mesh_snapshot/crop.building.ply00.png\" width=\"12%\" height=\"auto\"/>\n    <img src=\"assets/mesh_snapshot/crop.building.ply01.png\" width=\"12%\" height=\"auto\"/>\n    <img src=\"assets/mesh_snapshot/crop.rose.ply00.png\" width=\"12%\" height=\"auto\"/>\n    <img src=\"assets/mesh_snapshot/crop.rose.ply01.png\" width=\"12%\" height=\"auto\"/>\n</div>\n\n## News\n\n- [2023.12.21] [Hugging Face Demo](https://huggingface.co/spaces/zxhezexin/OpenLRM) is online. Have a try!\n- [2023.12.20] Release weights of the base and large models trained on Objaverse.\n- [2023.12.20] We release this project OpenLRM, which is an open-source implementation of the paper [LRM](https://arxiv.org/abs/2311.04400).\n\n## Setup\n\n### Installation\n```\ngit clone https://github.com/3DTopia/OpenLRM.git\ncd OpenLRM\n```\n\n### Environment\n```\npip install -r requirements.txt\n```\n\n## Quick Start\n\n### Pretrained Models\n\n- Model weights are released on [Hugging Face](https://huggingface.co/zxhezexin/OpenLRM).\n- Weights will be downloaded automatically when you run the inference script for the first time.\n- Please be aware of the [license](LICENSE_WEIGHT) before using the weights.\n\n| Model | Training Data | Layers | Feat. Dim | Trip. Dim. | Render Res. | Link |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| LRM-Base-Obj | Objaverse | 12 | 1024 | 40 | 192 | [HF](https://huggingface.co/zxhezexin/OpenLRM) |\n| LRM-Large-Obj | Objaverse | 16 | 1024 | 80 | 384 | [HF](https://huggingface.co/zxhezexin/OpenLRM) |\n| LRM-Base | Objaverse + MVImgNet | 12 | 1024 | 40 | 192 | To be released |\n| LRM-Large | Objaverse + MVImgNet | 16 | 1024 | 80 | 384 | To be released |\n\nModel cards with additional details can be found in [model_card.md](model_card.md).\n\n### Prepare Images\n- We put some sample inputs under `assets/sample_input`, and you can quickly try them.\n- Prepare RGBA images or RGB images with white background (with some background removal tools, e.g., [Rembg](https://github.com/danielgatis/rembg), [Clipdrop](https://clipdrop.co)).\n\n### Inference\n- Run the inference script to get 3D assets.\n- You may specify which form of output to generate by setting the flags `--export_video` and `--export_mesh`.\n\n    ```\n    # Example usages\n    # Render a video\n    python -m lrm.inferrer --model_name lrm-base-obj-v1 --source_image ./assets/sample_input/owl.png --export_video\n    \n    # Export mesh\n    python -m lrm.inferrer --model_name lrm-base-obj-v1 --source_image ./assets/sample_input/owl.png --export_mesh\n    ```\n\n## Training\nTo be released soon.\n\n## Acknowledgement\n\n- We thank the authors of the [original paper](https://arxiv.org/abs/2311.04400) for their great work! Special thanks to Kai Zhang and Yicong Hong for assistance during the reproduction.\n- This project is supported by Shanghai AI Lab by providing the computing resources.\n- This project is advised by Ziwei Liu and Jiaya Jia.\n\n## Citation\n\nIf you find this work useful for your research, please consider citing:\n```\n@article{hong2023lrm,\n  title={Lrm: Large reconstruction model for single image to 3d},\n  author={Hong, Yicong and Zhang, Kai and Gu, Jiuxiang and Bi, Sai and Zhou, Yang and Liu, Difan and Liu, Feng and Sunkavalli, Kalyan and Bui, Trung and Tan, Hao},\n  journal={arXiv preprint arXiv:2311.04400},\n  year={2023}\n}\n```\n\n```\n@misc{openlrm,\n  title = {OpenLRM: Open-Source Large Reconstruction Models},\n  author = {Zexin He and Tengfei Wang},\n  year = {2023},\n  howpublished = {\\url{https://github.com/3DTopia/OpenLRM}},\n}\n```\n\n## License\n\n- OpenLRM as a whole is licensed under the [Apache License, Version 2.0](LICENSE), while certain components are covered by [NVIDIA's proprietary license](LICENSE_NVIDIA). Users are responsible for complying with the respective licensing terms of each component.\n- Model weights are licensed under the [Creative Commons Attribution-NonCommercial 4.0 International License](LICENSE_WEIGHT). They are provided for research purposes only, and CANNOT be used commercially.\n# Ignore Python cache files\n**/__pycache__\n\n# Ignore compiled Python files\n*.pyc\n\n# Ignore editor-specific files\n.vscode/\n.idea/\n\n# Ignore operating system files\n.DS_Store\nThumbs.db\n\n# Ignore log files\n*.log\n\n# Ignore temporary and cache files\n*.tmp\n*.cache\n\n# Ignore build artifacts\n/build/\n/dist/\n\n# Ignore virtual environment files\n/venv/\n/.venv/\n\n# Ignore package manager files\n/node_modules/\n/yarn.lock\n/package-lock.json\n\n# Ignore database files\n*.db\n*.sqlite\n\n# Ignore secret files\n*.secret\n\n# Ignore compiled binaries\n*.exe\n*.dll\n*.so\n*.dylib\n\n# Ignore backup files\n*.bak\n*.swp\n*.swo\n*.~*\n\n\n\nCopyright (c) 2021-2022, NVIDIA Corporation & affiliates. All rights\nreserved.\n\n\nNVIDIA Source Code License for EG3D\n\n\n=======================================================================\n\n1. Definitions\n\n\"Licensor\" means any person or entity that distributes its Work.\n\n\"Software\" means the original work of authorship made available under\nthis License.\n\n\"Work\" means the Software and any additions to or derivative works of\nthe Software that are made available under this License.\n\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\n\"distribution\" have the meaning as provided under U.S. copyright law;\nprovided, however, that for the purposes of this License, derivative\nworks shall not include works that remain separable from, or merely\nlink (or bind by name) to the interfaces of, the Work.\n\nWorks, including the Software, are \"made available\" under this License\nby including in or with the Work either (a) a copyright notice\nreferencing the applicability of this License to the Work, or (b) a\ncopy of this License.\n\n2. License Grants\n\n    2.1 Copyright Grant. Subject to the terms and conditions of this\n    License, each Licensor grants to you a perpetual, worldwide,\n    non-exclusive, royalty-free, copyright license to reproduce,\n    prepare derivative works of, publicly display, publicly perform,\n    sublicense and distribute its Work and any resulting derivative\n    works in any form.\n\n3. Limitations\n\n    3.1 Redistribution. You may reproduce or distribute the Work only\n    if (a) you do so under this License, (b) you include a complete\n    copy of this License with your distribution, and (c) you retain\n    without modification any copyright, patent, trademark, or\n    attribution notices that are present in the Work.\n\n    3.2 Derivative Works. You may specify that additional or different\n    terms apply to the use, reproduction, and distribution of your\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\n    provide that the use limitation in Section 3.3 applies to your\n    derivative works, and (b) you identify the specific derivative\n    works that are subject to Your Terms. Notwithstanding Your Terms,\n    this License (including the redistribution requirements in Section\n    3.1) will continue to apply to the Work itself.\n\n    3.3 Use Limitation. The Work and any derivative works thereof only\n    may be used or intended for use non-commercially. The Work or\n    derivative works thereof may be used or intended for use by NVIDIA\n    or it‚Äôs affiliates commercially or non-commercially. As used\n    herein, \"non-commercially\" means for research or evaluation\n    purposes only and not for any direct or indirect monetary gain.\n\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\n    against any Licensor (including any claim, cross-claim or\n    counterclaim in a lawsuit) to enforce any patents that you allege\n    are infringed by any Work, then your rights under this License from\n    such Licensor (including the grants in Sections 2.1) will terminate\n    immediately.\n\n    3.5 Trademarks. This License does not grant any rights to use any\n    Licensor‚Äôs or its affiliates‚Äô names, logos, or trademarks, except\n    as necessary to reproduce the notices described in this License.\n\n    3.6 Termination. If you violate any term of this License, then your\n    rights under this License (including the grants in Sections 2.1)\n    will terminate immediately.\n\n4. Disclaimer of Warranty.\n\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\nTHIS LICENSE.\n\n5. Limitation of Liability.\n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGES.\n\n=======================================================================\n\n\n# Model Card for OpenLRM\n\n## Overview\n\nThis model card is for the [OpenLRM](https://github.com/3DTopia/OpenLRM) project, which is an open-source implementation of the paper [LRM](https://arxiv.org/abs/2311.04400).\n\n## Model Details\n\n| Model | Training Data | Layers | Feat. Dim | Trip. Dim. | Render Res. | Link |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| LRM-Base-Obj | Objaverse | 12 | 1024 | 40 | 192 | [HF](https://huggingface.co/zxhezexin/OpenLRM) |\n| LRM-Large-Obj | Objaverse | 16 | 1024 | 80 | 384 | [HF](https://huggingface.co/zxhezexin/OpenLRM) |\n| LRM-Base | Objaverse + MVImgNet | 12 | 1024 | 40 | 192 | To be released |\n| LRM-Large | Objaverse + MVImgNet | 16 | 1024 | 80 | 384 | To be released |\n\n## Differences from the Original Paper\n\n- We do not use the deferred back-propagation technique in the original paper.\n- The triplane decoder contains 4 layers in our implementation.\n\n## License\n\n- The model weights are released under the [Creative Commons Attribution-NonCommercial 4.0 International License](LICENSE_WEIGHT).\n- They are provided for research purposes only, and CANNOT be used commercially.\n\n## Disclaimer\n\nThis model is an open-source implementation and is NOT the official release of the original research paper. While it aims to reproduce the original results as faithfully as possible, there may be variations due to model implementation, training data, and other factors.\n\n### Ethical Considerations\n\n- This model should be used responsibly and ethically, and should not be used for malicious purposes.\n- Users should be aware of potential biases in the training data.\n- The model should not be used under the circumstances that could lead to harm or unfair treatment of individuals or groups.\n\n### Usage Considerations\n\n- The model is provided \"as is\" without warranty of any kind.\n- Users are responsible for ensuring that their use complies with all relevant laws and regulations.\n- The developers and contributors of this model are not liable for any damages or losses arising from the use of this model.\n\n---\n\n*This model card is subject to updates and modifications. Users are advised to check for the latest version regularly.*\n\n\n\ntorch>=2.1.0\ntransformers\nomegaconf\npillow\nimageio[ffmpeg]\nPyMCubes\ntrimesh\n\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/elifgazioglu/doyouwannagooutwithme", "text_blocks": "‚ú®[doyouwannagooutwithme.com](http://doyouwannagooutwithme.com) \n\nA website to invite your lover for a date ü•∞\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <link rel=\"stylesheet\" href=\"./styles.css\">\n        \n    </head> \n    <body>\n        <div class=\"container\">\n            <div >\n                <h1 class = \"header_text\">Do you wanna go out with me?</h1>\n            </div>\n            <div class=\"gif_container\">\n                <img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExcDdtZ2JiZDR0a3lvMWF4OG8yc3p6Ymdvd3g2d245amdveDhyYmx6eCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9cw/cLS1cfxvGOPVpf9g3y/giphy.gif\" alt=\"Cute animated illustration\">\n            </div>\n            <div class = \"buttons\">\n                <button class=\"btn\" id = \"yesButton\" onclick=\"nextPage()\">Yes</button>\n                <button class=\"btn\" id=\"noButton\" onmouseover=\"moveButton()\" onclick=\"moveButton()\">No</button>\n                <script>\n                    function nextPage() {\n                        window.location.href = \"yes.html\";\n                    }\n                    \n                    function moveButton() {\n                        var x = Math.random() * (window.innerWidth - document.getElementById('noButton').offsetWidth) - 85;\n                        var y = Math.random() * (window.innerHeight - document.getElementById('noButton').offsetHeight) - 48;\n                        document.getElementById('noButton').style.left = `${x}px`;\n                        document.getElementById('noButton').style.top = `${y}px`;\n                    }\n                </script> \n            </div>\n        </div>\n       \n    </body> \n</html>\n\n\nbody {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    height: 90vh;\n    background-color: #F8C8DC;\n}\n\n#noButton {\n    position: absolute;\n    margin-left: 150px;\n    transition: 0.5s;\n}\n\n#yesButton {\n    position: absolute;\n    margin-right: 150px;\n}\n\n.header_text {\n    font-family: 'Nunito';\n    font-size: 40px;\n    font-weight: bold;\n    color: white;\n    text-align: center;\n    margin-top: 20px;\n    margin-bottom: 0px;\n}\n\n.buttons {\n    display: flex;\n    flex-direction: row;\n    justify-content: center;\n    align-items: center;\n    margin-top: 20px;\n    margin-left: 20px;\n}\n\n.btn {\n    background-color: #FFB6C1;\n    color: white;\n    padding: 15px 32px;\n    text-align: center;\n    display: inline-block;\n    font-size: 16px;\n    margin: 4px 2px;\n    cursor: pointer;\n    border: none;\n    border-radius: 12px;\n    transition: background-color 0.3s ease;\n}\n\n.gif_container {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n}\n\n@media only screen and (max-width: 320px) and (max-height: 568px) {\n    body {\n        height: 100vh;\n    }\n\n    .header_text {\n        font-size: 20px;\n    }\n\n    img {\n        height: 60vh;\n    }\n\n    .btn {\n        padding: 10px 18px;\n        font-size: 12px;\n    }\n}\n\n@media only screen and (max-width: 414px) and (max-height: 736px) {\n    body {\n        height: 90vh;\n    }\n\n    .header_text {\n        font-size: 28px;\n    }\n\n    img {\n        height: 60vh;\n    }\n\n    .btn {\n        padding: 15px 25px;\n        font-size: 14px;\n    }\n}\n\n\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <link rel=\"stylesheet\" href=\"./yes_style.css\">\n        \n    </head> \n    <body>\n        <div class=\"container\">\n            <div >\n                <h1 class = \"header_text\">Yeeeyyyy!!</h1>\n            </div>\n            <div class=\"gif_container\">\n                <img src=\"https://media0.giphy.com/media/T86i6yDyOYz7J6dPhf/giphy.gif\" alt=\"Cute animated illustration\">\n            </div>\n        </div>\n       \n    </body> \n</html>\n\n\nbody {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    height: 100vh;\n    background-color: #F8C8DC;\n}\n\n.header_text {\n    font-family: 'Nunito';\n    font-size: 50px;\n    font-weight: bold;\n    color: white;\n    text-align: center;\n    margin-top: 20px;\n    margin-bottom: 0px;\n}\n\n.gif_container {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n}\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/open-mmlab/PIA", "text_blocks": "# PIAÔºöPersonalized Image Animator\n\n[**PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models**](https://arxiv.org/abs/2312.13964)\n\n[Yiming Zhang‚Ä†](https://github.com/ymzhang0319), [Zhening Xing‚Ä†](https://github.com/LeoXing1996/), [Yanhong Zeng](https://zengyh1900.github.io/), [Youqing Fang](https://github.com/FangYouqing), [Kai Chen*](https://chenkai.site/)\n\n(*Corresponding Author, ‚Ä†Equal Contribution)\n\n\n[![arXiv](https://img.shields.io/badge/arXiv-2312.13964-b31b1b.svg)](https://arxiv.org/abs/2312.13964)\n[![Project Page](https://img.shields.io/badge/PIA-Website-green)](https://pi-animator.github.io)\n[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/zhangyiming/PiaPia)\n\nPIA is a personalized image animation method which can generate videos with **high motion controllability** and **strong text and image alignment**.\n\n\n<img src=\"__assets__/image_animation/teaser/teaser.gif\">\n\n## What's New\n[2023/12/22] Release the model and demo of PIA. Try it to make your personalized movie!\n\n- Online Demo on [OpenXLab](https://openxlab.org.cn/apps/detail/zhangyiming/PiaPia)\n- Checkpoint on [Google Drive](https://drive.google.com/file/d/1RL3Fp0Q6pMD8PbGPULYUnvjqyRQXGHwN/view?usp=drive_link) or [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/zhangyiming/PIA)\n\n## Setup\n### Prepare Environment\n```\nconda env create -f environment.yaml\nconda activate pia\n```\n\n### Download checkpoints\n<li>Download the Stable Diffusion v1-5</li>\n\n```\ngit lfs install\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5 models/StableDiffusion/\n```\n\n<li>Download Personalized Models</li>\n\n```\nbash download_bashscripts/1-RealisticVision.sh\nbash download_bashscripts/2-RcnzCartoon.sh\nbash download_bashscripts/3-MajicMix.sh\n```\n\n<li>Download PIA</li>\n\n```\nbash download_bashscripts/0-PIA.sh\n```\n\n\nYou can also download *pia.ckpt* through this link on [Google Drive](https://drive.google.com/file/d/1RL3Fp0Q6pMD8PbGPULYUnvjqyRQXGHwN/view?usp=drive_link)\n\nPut checkpoints as follows:\n```\n‚îî‚îÄ‚îÄ models\n    ‚îú‚îÄ‚îÄ DreamBooth_LoRA\n    ‚îÇ   ‚îú‚îÄ‚îÄ ...\n    ‚îú‚îÄ‚îÄ PIA\n    ‚îÇ   ‚îú‚îÄ‚îÄ pia.ckpt\n    ‚îî‚îÄ‚îÄ StableDiffusion\n        ‚îú‚îÄ‚îÄ vae\n        ‚îú‚îÄ‚îÄ unet\n        ‚îî‚îÄ‚îÄ ...\n```\n\n## Usage\n### Image Animation\nImage to Video result can be obtained by:\n```\npython inference.py --config=example/config/lighthouse.yaml\npython inference.py --config=example/config/harry.yaml\npython inference.py --config=example/config/majic_girl.yaml\n```\nRun the command above, you will get:\n<table class=\"center\">\n    <tr>\n      <td><p style=\"text-align: center\">Input Image</p></td>\n      <td><p style=\"text-align: center\">lightning, lighthouse</p></td>\n      <td><p style=\"text-align: center\">sun rising, lighthouse</p></td>\n      <td><p style=\"text-align: center\">fireworks, lighthouse</p></td>\n    </tr>\n    <tr>\n      <td><img src=\"__assets__/image_animation/real/lighthouse.jpg\"></td>\n      <td><img src=\"__assets__/image_animation/real/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/real/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/real/3.gif\"></td>\n    </tr>\n    <tr>\n      <td><p style=\"text-align: center\">Input Image</p></td>\n      <td><p style=\"text-align: center\">1boy smiling</p></td>\n      <td><p style=\"text-align: center\">1boy playing the magic fire</p></td>\n      <td><p style=\"text-align: center\">1boy is waving hands</p></td>\n    </tr>\n    <tr>\n      <td><img src=\"__assets__/image_animation/rcnz/harry.png\"></td>\n      <td><img src=\"__assets__/image_animation/rcnz/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/rcnz/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/rcnz/3.gif\"></td>\n    </tr>\n    <tr>\n      <td><p style=\"text-align: center\">Input Image</p></td>\n      <td><p style=\"text-align: center\">1girl is smiling</p></td>\n      <td><p style=\"text-align: center\">1girl is crying</p></td>\n      <td><p style=\"text-align: center\">1girl, snowing </p></td>\n    </tr>\n    <tr>\n      <td><img src=\"__assets__/image_animation/majic/majic_girl.jpg\"></td>\n      <td><img src=\"__assets__/image_animation/majic/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/majic/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/majic/3.gif\"></td>\n    </tr>\n\n</table>\n\n<!-- More results:\n\n<table class=\"center\">\n    <tr>\n      <td><p style=\"text-align: center\">Input Image</p></td>\n    </tr>\n    <tr>\n    </tr>\n</table> -->\n\n### Motion Magnitude\nYou can control the motion magnitude through the parameter **magnitude**:\n```sh\npython inference.py --config=example/config/xxx.yaml --magnitude=0 # Small Motion\npython inference.py --config=example/config/xxx.yaml --magnitude=1 # Moderate Motion\npython inference.py --config=example/config/xxx.yaml --magnitude=2 # Large Motion\n```\nExamples:\n\n```sh\npython inference.py --config=example/config/labrador.yaml\npython inference.py --config=example/config/bear.yaml\npython inference.py --config=example/config/genshin.yaml\n```\n\n<table class=\"center\">\n    <tr>\n      <td><p style=\"text-align: center\">Input Image<br>& Prompt</p></td>\n      <td><p style=\"text-align: center\">Small Motion</p></td>\n      <td><p style=\"text-align: center\">Moderate Motion</p></td>\n      <td><p style=\"text-align: center\">Large Motion</p></td>\n    </tr>\n    <tr>\n    <td><img src=\"__assets__/image_animation/magnitude/labrador.png\" style=\"width: 220px\">a golden labrador is running</td>\n     <td><img src=\"__assets__/image_animation/magnitude/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/magnitude/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/magnitude/3.gif\"></td>\n    </tr>\n    <tr>\n    <td><img src=\"__assets__/image_animation/magnitude/bear/bear.jpg\" style=\"width: 220px\">1bear is walking, ...</td>\n     <td><img src=\"__assets__/image_animation/magnitude/bear/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/magnitude/bear/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/magnitude/bear/3.gif\"></td>\n    </tr>\n    <tr>\n    <td><img src=\"__assets__/image_animation/magnitude/genshin/genshin.jpg\" style=\"width: 220px\">cherry blossom, ...</td>\n     <td><img src=\"__assets__/image_animation/magnitude/genshin/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/magnitude/genshin/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/magnitude/genshin/3.gif\"></td>\n    </tr>\n</table>\n\n### Style Transfer\nTo achieve style transfer, you can run the command(*Please don't forget set the base model in xxx.yaml*):\n\nExamples:\n\n```sh\npython inference.py --config example/config/concert.yaml --style_transfer\npython inference.py --config example/config/ania.yaml --style_transfer\n```\n<table class=\"center\">\n    <tr>\n      <td><p style=\"text-align: center\">Input Image<br> & Base Model</p></td>\n      <td><p style=\"text-align: center\">1man is smiling</p></td>\n      <td><p style=\"text-align: center\">1man is crying</p></td>\n      <td><p style=\"text-align: center\">1man is singing</p></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\"><img src=\"__assets__/image_animation/style_transfer/concert.png\" style=\"width:220px\">Realistic Vision</td>\n      <td><img src=\"__assets__/image_animation/style_transfer/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/style_transfer/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/style_transfer/3.gif\"></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\"><img src=\"__assets__/image_animation/style_transfer/concert.png\" style=\"width:220px\">RCNZ Cartoon 3d</td>\n      <td><img src=\"__assets__/image_animation/style_transfer/4.gif\"></td>\n      <td><img src=\"__assets__/image_animation/style_transfer/5.gif\"></td>\n      <td><img src=\"__assets__/image_animation/style_transfer/6.gif\"></td>\n    </tr>\n    <tr>\n      <td><p style=\"text-align: center\"></p></td>\n      <td><p style=\"text-align: center\">1girl smiling</p></td>\n      <td><p style=\"text-align: center\">1girl open mouth</p></td>\n      <td><p style=\"text-align: center\">1girl is crying, pout</p></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\"><img src=\"__assets__/image_animation/style_transfer/anya/anya.jpg\" style=\"width:220px\">RCNZ Cartoon 3d</td>\n      <td><img src=\"__assets__/image_animation/style_transfer/anya/1.gif\"></td>\n      <td><img src=\"__assets__/image_animation/style_transfer/anya/2.gif\"></td>\n      <td><img src=\"__assets__/image_animation/style_transfer/anya/3.gif\"></td>\n    </tr>\n</table>\n\n### Loop Video\n\nYou can generate loop by using the parameter --loop\n\n```sh\npython inference.py --config=example/config/xxx.yaml --loop\n```\n\nExamples:\n```sh\npython inference.py --config=example/config/lighthouse.yaml --loop\npython inference.py --config=example/config/labrador.yaml --loop\n```\n\n<table>\n  <tr>\n    <td><p style=\"text-align: center\">Input Image</p></td>\n    <td><p style=\"text-align: center\">lightning, lighthouse</p></td>\n    <td><p style=\"text-align: center\">sun rising, lighthouse</p></td>\n    <td><p style=\"text-align: center\">fireworks, lighthouse</p></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center\"><img src=\"__assets__/image_animation/loop/lighthouse/lighthouse.jpg\" style=\"width:auto\"></td>\n    <td><img src=\"__assets__/image_animation/loop/lighthouse/1.gif\"></td>\n    <td><img src=\"__assets__/image_animation/loop/lighthouse/2.gif\"></td>\n    <td><img src=\"__assets__/image_animation/loop/lighthouse/3.gif\"></td>\n  </tr>\n  <tr>\n    <td><p style=\"text-align: center\">Input Image</p></td>\n    <td><p style=\"text-align: center\">labrador jumping</p></td>\n    <td><p style=\"text-align: center\">labrador walking</p></td>\n    <td><p style=\"text-align: center\">labrador running</p></td>\n  </tr>\n  <tr>\n    <td style=\"text-align: center\"><img src=\"__assets__/image_animation/loop/labrador/labrador.png\" style=\"width:auto\"></td>\n    <td><img src=\"__assets__/image_animation/loop/labrador/1.gif\"></td>\n    <td><img src=\"__assets__/image_animation/loop/labrador/2.gif\"></td>\n    <td><img src=\"__assets__/image_animation/loop/labrador/3.gif\"></td>\n  </tr>\n</table>\n\n## AnimateBench\nWe have open-sourced AnimateBench on [HuggingFace](https://huggingface.co/datasets/ymzhang319/AnimateBench) which includes images, prompts and configs to evaluate PIA and other image animation methods.\n\n## Contact Us\n**Yiming Zhang**: zhangyiming@pjlab.org.cn\n\n**Zhening Xing**: xingzhening@pjlab.org.cn\n\n**Yanhong Zeng**: zengyanhong@pjlab.org.cn\n\n## Acknowledgements\nThe code is built upon [AnimateDiff](https://github.com/guoyww/AnimateDiff), [Tune-a-Video](https://github.com/showlab/Tune-A-Video) and [PySceneDetect](https://github.com/Breakthrough/PySceneDetect)*.pkl\n*.pt\n*.mov\n*.pth\n*.json\n*.mov\n*.npz\n*.npy\n*.boj\n*.onnx\n*.tar\n*.bin\ncache*\nbatch*\n*.jpg\n*.png\n*.mp4\n*.gif\n*.ckpt\n*.safetensors\n*.zip\n*.csv\n\n**/__pycache__/\nsamples/\nwandb/\noutputs/\n\n!pia.png\n.DS_Store\n\n\n\nname: pia\nchannels:\n  - pytorch\n  - nvidia\ndependencies:\n  - python=3.10\n  - pytorch=1.13.1\n  - torchvision=0.14.1\n  - torchaudio=0.13.1\n  - pytorch-cuda=11.7\n  - pip\n  - pip:\n    - diffusers==0.24.0\n    - transformers==4.25.1\n    - xformers==0.0.16\n    - imageio==2.27.0\n    - decord==0.6.0\n    - gdown\n    - einops\n    - omegaconf\n    - safetensors\n    - gradio\n    - wandb\n\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/hua1995116/indiehackers-steps", "text_blocks": "# Áã¨Á´ãÂºÄÂèëËÄÖÂá∫Êµ∑‰πãË∑Ø\n\n> Êú¨ÊïôÁ®ãÂ∞ÜËÆ∞ÂΩïÊàëÁöÑÁã¨Á´ãÂºÄÂèëËÄÖÂá∫Êµ∑‰πãË∑ØÔºåÂåÖÂê´‰ªéÊúÄÂü∫Êú¨ÁöÑÂ¶Ç‰ΩïÊ≥®ÂÜåÁæéÂõΩÂÖ¨Âè∏Âà∞Èì∂Ë°åÔºå‰ΩøÁî®stripeÊî∂Ê¨æÔºåÂá∫Êµ∑ÁöÑÂ∑•‰ΩúÊµÅÈÄâÊã©‰ª•ÂèäÂ¶Ç‰ΩïËê•ÈîÄ„ÄÇÊúâÁñëÈóÆÂèØ‰ª•ÂÖ≥Ê≥®ÊàëÁöÑ [twitter](https://twitter.com/qiufenghyf) ,ÁßÅ‰ø°‰∫§ÊµÅ\n\n## 1. 193 ÂàÄÊ≥®ÂÜåÁæéÂõΩÂÖ¨Âè∏ + EIN + stripe + Ê∞¥ÊòüÈì∂Ë°å\n\n<img width=\"919\" alt=\"image\" src=\"https://github.com/hua1995116/indiehackers-steps/assets/12070073/911d5ed7-acca-4607-8d6e-0b021f9ffa52\">\n\n\n### Step1: Áî≥ËØ∑ÁæéÂõΩÂÖ¨Âè∏\n\nËøôÈáåÊàëÈÄâÊã©ÁöÑÊòØ  wyoming Â∑ûÊ≥®ÂÜå, Âõ†‰∏∫ wyoming ÂÖçÂÖ¨Âè∏Á®éÂíå‰∏™‰∫∫Á®éÔºåÂπ∂‰∏î‰ªª‰ΩïÂõΩÁ±ç‰πüÂèØ‰ª•Ê≥®ÂÜå\n\n1.ÊâìÂºÄ https://www.wyomingagents.com/ \n\n<img width=\"705\" alt=\"image\" src=\"https://github.com/hua1995116/indiehackers-steps/assets/12070073/5242b5c6-a90d-4e8e-8926-919046358598\">\n\nÊåâÁÖßË¶ÅÊ±ÇÂ°´ÂÜôÂü∫Êú¨ËµÑÊñô\n\n<img width=\"759\" alt=\"image\" src=\"https://github.com/hua1995116/indiehackers-steps/assets/12070073/5c1ff0b3-6ecc-4a32-809a-31d7c94f285e\">\n\n<img width=\"696\" alt=\"image\" src=\"https://github.com/hua1995116/indiehackers-steps/assets/12070073/f6f55555-cdb8-4047-9461-64069eec55f7\">\n\nÂÖ¨Âè∏Á±ªÂûãÔºå‰∏™‰∫∫ÊàñËÄÖÂ∞èÂûãÁªÑÁªáÂèØ‰ª•ÈÄâÊã© LLC, Â¶ÇÊûúÊúâÂÖ¨Âè∏ÊúâËûçËµÑÈúÄÊ±ÇÂèØ‰ª•ÈÄâÊã© CORPÔºå‰ΩÜÊòØÊú¨ÊïôÁ®ãÈù¢ÂêëÁöÑËøòÊòØÂ§ßÂ§öÊï∞Áã¨Á´ãÂºÄÂèëËÄÖÔºåÂõ†Ê≠§ÈÄâÊã© LLC, ÈáåÈù¢ÁöÑÂú∞ÂùÄ‰ø°ÊÅØÂ∞±Â°´ÂÜô‰Ω†ÂÆûÈôÖÁöÑÂú∞ÂùÄÔºå‰∏çÈúÄË¶ÅÂÅöÂï•‰øÆÈ•∞\n\n<img width=\"714\" alt=\"image\" src=\"https://github.com/hua1995116/indiehackers-steps/assets/12070073/bcf62ebb-55bb-4f5e-8a57-68ac0d711559\">\n\nÂÖ≥‰∫éÂ¢ûÂÄº‰ªòË¥πÈÉ®ÂàÜÔºåÊåâÁÖßÈªòËÆ§Â∞±Â•ΩÔºåÂ¢ûÂÄºÈÉ®ÂàÜÂú®Ê≥®ÂÜåÂÆåÊØïÂêéÔºåÈÉΩÊòØÂèØ‰ª•È¢ùÂ§ñË¥≠‰π∞ÁöÑÔºåÈúÄË¶ÅÊãÖÂøÉÈÄâÈîô‰∫ÜÔºåÂØºËá¥Ê≥®ÂÜåÂ§±Ë¥•ÔºåÈªòËÆ§ÁöÑÈÄâÈ°πÂ∞±ÂèØ‰ª•ÂÆåÊàêÂÖ¨Âè∏Ê≥®ÂÜå„ÄÇ\n\n<img width=\"690\" alt=\"image\" src=\"https://github.com/hua1995116/indiehackers-steps/assets/12070073/449f187f-cd29-4aa5-83cd-44e23682ac2e\">\n\nÂ§ßÁ∫¶Á≠âÂæÖ1Âë®ÔºåÂ∞±ÂèØ‰ª•ÂÆåÊàêÂÖ¨Âè∏Ê≥®ÂÜåÔºå‰Ω†‰ºöÂæóÂà∞ ÂÖ¨Âè∏Á´†Á®ã‰ª•Âèä‰∏Ä‰∫õÂÖ¨Âè∏ÁöÑ‰ø°ÊÅØÊùêÊñô„ÄÇ\n\nËä±Ë¥π 150 ÂàÄÔºåËÄóÊó∂ 1Âë®\n\n### Step2: EIN \n\nÊúâ‰∫ÜÂÖ¨Âè∏Á´†Á®ãÂêéÔºåÊàë‰ª¨Â∞±ÂèØ‰ª•ÂéªÁî≥ËØ∑ EIN ‰∫Ü„ÄÇ ‰Ω†ÂèØ‰ª•Ëá™Â∑±ÂéªÁî≥ËØ∑Âà∞ÂÆòÁΩëÔºå‰πüÊòØÊúâ‰∏≠ÊñáÁöÑ https://www.irs.gov/zh-hant/businesses/small-businesses-self-employed/how-to-apply-for-an-ein\n\nÂΩìÁÑ∂‰πüÂèØ‰ª•Êâæ‰ª£ÂäûÔºåÊàëËøôÈáåÂéª fiverr ‰∏äÊâæ‰∫∫Â∏ÆÊàë‰ª£Âäû‰∫ÜÔºå‰Ω†ÂèëÁªô‰ªñÂÖ¨Âè∏Á´†Á®ãÔºå‰ªñÂ∞±Áõ¥Êé•ÂèØ‰ª•Â∏Æ‰Ω†Áî≥ËØ∑„ÄÇ\n\nÂπ≥Âè∞‰∏äËØ¥‰∏ÄÂçäÊòØ10ÁÇπÊâçËÉΩ‰∫§‰ªòÔºå‰ΩÜÊòØÊàëÁ≠â‰∫ÜÂ§ßÊ¶Ç3Â§©Â∞±ÊãøÂà∞‰∫Ü EIN + IRS ÁöÑÂõûÊâß‰ø°Ôºå‰ΩÜÊòØ‰ª£ÂäûÁöÑ‰∫∫ËØ¥ÂêåÊ≠• EIN ÈúÄË¶ÅÊó∂Èó¥ÔºåËÆ©ÊàëÈöî1Âë®ÂÜçÂéªÊ≥®ÂÜå Stripe\n\nËä±Ë¥π 35 ÂàÄ + 8.17ÂàÄ(fiverrÂπ≥Âè∞ÊâãÁª≠Ë¥π) + ËÄóÊó∂ 3Â§©\n\n### Step3: Ê∞¥ÊòüÈì∂Ë°å\n\nhttps://mercury.com\n\nÂÖ∂ÂÆûÊúâ‰∫ÜÁæéÂõΩÂÖ¨Âè∏‰∏ª‰Ωì + EIN, Áî≥ËØ∑ÁúüÁöÑË∂ÖÁ∫ßÁÆÄÂçïÔºåÁõ¥Êé•ÊåâÁÖß‰Ω†ÁöÑÁúüÂÆûÊÉÖÂÜµÂ°´ÂÜôÂ•Ω‰∫ÜÔºåÂ§ßÁ∫¶ÁªèËøá3‰∏™Â∑•‰ΩúÊó•ÁöÑÂÆ°Ê†∏Ôºå‰ºöË¶ÅÊ±Ç‰Ω†Êèê‰æõ‰∏Ä‰∏™Âú∞ÂùÄËØÅÊòéÔºåËØÅÊòéÊùêÊñôÈúÄË¶ÅÊòØ Ê∞¥ÁîµË¥πË¥¶Âçï + Êúâ‰Ω†ÁöÑÂêçÂ≠ó + ‰Ω†ÁöÑÂú∞ÂùÄÔºåÊàñËÄÖÊòØÈì∂Ë°åË¥¶Âçï + ‰Ω†ÁöÑÂêçÂ≠ó + ‰Ω†ÁöÑÂú∞ÂùÄ„ÄÇ\n\nÊâÄ‰ª•Âú®Áî≥ËØ∑Ë¥¶Âè∑ÁöÑÊó∂ÂÄôÔºåÂú∞ÂùÄÈúÄË¶ÅÂ°´ÂÜôÁöÑË∞®ÊÖé‰∏Ä‰∫õÔºåÊàëÂΩìÊó∂‰∏∫‰∫ÜÊñπ‰æøÔºåÂ°´ÂÜô‰∫ÜÂÆ∂Â∫≠Âú∞ÂùÄ„ÄÇÁÑ∂ÂêéËØÅÊòéÊùêÊñô‰ΩøÁî®‰∫Ü Ê∞¥ÁîµË¥¶Âçï + Êà∑Âè£Êú¨(Âõ†‰∏∫Ê∞¥ÁîµÊà∑ÂêçÊòØÊàëÁà∏ÁöÑÂêçÂ≠ó) „ÄÇÂ¶ÇÊûú‰Ω†Êèê‰æõÁöÑÊ∞¥ÁîµË¥¶ÂçïÊ≤°Êúâ‰Ω†ÁöÑÂêçÂ≠óÔºåÂ∫îËØ•Âä†‰∏äÁßüÊàøÂêàÂêåÊàñËÄÖÊàø‰∫ßËØÅÊòé‰πüÊòØÂèØ‰ª•ÁöÑ„ÄÇ\n\nËä±Ë¥π 0 ÂàÄ + ËÄóÊó∂ 3Â§©ÂàùÂÆ°Ê†∏ + 2Â§©Âú∞ÂùÄÂÆ°Ê†∏\n\n### Step4: Stripe\n\nÊúâ‰∫Ü‰ª•‰∏äÊùêÊñôÔºåÊàë‰ª¨Áõ¥Êé•Áî≥ËØ∑ [Stripe](https://stripe.com/) Â∞±ÂèØ‰ª•ÔºåÂõ†‰∏∫ Stripe ÈöæÁÇπÂ∞±ÊòØ EIN \n\nËä±Ë¥π 0 ÂàÄ + 1Â§©\n\nÊúÄÂêéÔºåÂ∞±ÂèØ‰ª•ÂºÄÂßã‰Ω†ÁöÑÂÆèÂõæÂ§ß‰∏ö‰∫Ü!!!\n\n## 2. ÁæéÂõΩÂÖ¨Âè∏Á¶èÂà©\n\nÂæÖÁª≠...\n\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/matt8707/ha-fusion", "text_blocks": "# ha-fusion\n\nA modern, easy-to-use and performant custom [Home Assistant](https://www.home-assistant.io/) dashboard\n\n<https://www.youtube.com/watch?v=D8mWruSuPOM>\n\n[![preview](/static/preview.png)](https://www.youtube.com/watch?v=D8mWruSuPOM)\n\nIf you find this project useful, be sure to üåü this repository! If you love it, please consider donating! ‚ù§Ô∏è <https://www.paypal.com/paypalme/matt8707>\n\n---\n\n## üì£ Pre-beta\n\nThe current state of this project is **pre-beta**. This means that there's basic functionality missing, incomplete features and unresolved issues. General feedback, bug reports and feature requests are welcome!\n\n---\n\n## Installation\n\nTodo\n\n- Addon - <https://github.com/matt8707/addon-ha-fusion>\n- Docker - <https://github.com/matt8707/ha-fusion/blob/master/docker-compose.yml>\n\n---\n\n## Query strings\n\n### View\n\nTo set a particular view when the page loads, add the \"view\" parameter. For example, if you have a \"Bedroom\" view, append the query string `?view=Bedroom` to the URL.\n\n### Menu\n\nTo disable the menu button, append the query string `?menu=false` to the URL. This is useful when you want to avoid unwanted changes to your dashboard, such as on wall-mounted tablets.\n\n---\n\n## Keyboard Shortcuts\n\n| Key                 | Description |\n| ------------------- | ----------- |\n| **f**               | filter      |\n| **esc**             | exit        |\n| **cmd + s**         | save        |\n| **cmd + z**         | undo        |\n| **cmd + shift + z** | redo        |\n\n---\n\n## Debug\n\nTo debug any errors, check the \"Log\" tab if you're using the addon, or use `docker logs ha-fusion` for Docker setups. To inspect frontend issues, open the browser's console.\n\n---\n\n## Develop\n\nTo begin contributing to the project, you'll first need to install node. It's also recommended to install pnpm. If you're unfamiliar with Svelte, consider doing the tutorial at <https://learn.svelte.dev>\n\n```bash\n# prerequisites (macos)\nbrew install node pnpm\n\n# install\ngit clone https://github.com/matt8707/ha-fusion.git\ncd ha-fusion\npnpm install\n\n# server\nnpm run dev -- --open\n\n# dependencies\npnpm outdated\npnpm upgrade\n\n# lint\nnpm run check\nnpm run lint\nnpm run format\n```\n**/.DS_Store\n.dockerignore\n.git\n.gitignore\nDockerfile\nREADME.md\n.svelte-kit\nbuild\nnode_modules\n\n\n\nroot = true\n\n[*]\nend_of_line = lf\ninsert_final_newline = true\nindent_style = tab\nindent_size = 2\ncharset = utf-8\ntrim_trailing_whitespace = true\n\n\n\n# This file is only for development, environment should be\n# set in docker, and when using addon `hassUrl` is irrelevant.\n\nHASS_URL=http://192.168.1.241:8123\n\n\n\n.DS_Store\nnode_modules\n/build\n/.svelte-kit\n/package\n.env\n.env.*\n!.env.example\n\n# Ignore files for PNPM, NPM and YARN\npnpm-lock.yaml\npackage-lock.json\nyarn.lock\n\n\n\nmodule.exports = {\n\troot: true,\n\textends: [\n\t\t'eslint:recommended',\n\t\t'plugin:@typescript-eslint/recommended',\n\t\t'plugin:svelte/recommended',\n\t\t'prettier'\n\t],\n\tparser: '@typescript-eslint/parser',\n\tplugins: ['@typescript-eslint'],\n\tparserOptions: {\n\t\tsourceType: 'module',\n\t\tecmaVersion: 2020,\n\t\textraFileExtensions: ['.svelte']\n\t},\n\tenv: {\n\t\tbrowser: true,\n\t\tes2017: true,\n\t\tnode: true\n\t},\n\toverrides: [\n\t\t{\n\t\t\tfiles: ['*.svelte'],\n\t\t\tparser: 'svelte-eslint-parser',\n\t\t\tparserOptions: {\n\t\t\t\tparser: '@typescript-eslint/parser'\n\t\t\t}\n\t\t}\n\t],\n\t/*\n\t * Temporarily disable certain rules to mitigate\n\t * unnecessary distractions during development.\n\t */\n\trules: {\n\t\t'@typescript-eslint/no-explicit-any': 'off',\n\t\t'@typescript-eslint/ban-ts-comment': 'off',\n\t\t'svelte/no-at-html-tags': 'off'\n\t},\n\tglobals: {\n\t\tDndEvent: 'readonly'\n\t}\n};\n\n\n\n.DS_Store\nnode_modules\n/build\n/.svelte-kit\n/package\n.env.*\n!.env.example\nvite.config.js.timestamp-*\nvite.config.ts.timestamp-*\n.vscode\n\n\n\nengine-strict=true\nresolution-mode=highest\n\n\n\n.DS_Store\nnode_modules\n/build\n/.svelte-kit\n/package\n.env\n.env.*\n!.env.example\n\n# Ignore files for PNPM, NPM and YARN\npnpm-lock.yaml\npackage-lock.json\nyarn.lock\n\n\n\n{\n\t\"useTabs\": true,\n\t\"singleQuote\": true,\n\t\"trailingComma\": \"none\",\n\t\"printWidth\": 100,\n\t\"plugins\": [\"prettier-plugin-svelte\"],\n\t\"overrides\": [\n\t\t{\n\t\t\t\"files\": \"*.svelte\",\n\t\t\t\"options\": {\n\t\t\t\t\"parser\": \"svelte\"\n\t\t\t}\n\t\t}\n\t]\n}\n\n\n\nFROM node:20-alpine AS builder\nWORKDIR /app\nCOPY package*.json .\nRUN npm install\nCOPY . .\nRUN npm run build --no-cache\nRUN npm prune --omit=dev\n\nFROM node:20-alpine\nWORKDIR /app\nCOPY --from=builder /app/build ./build\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/server.js .\nCOPY package.json .\n\nRUN ln -s /app/build/client/themes ./themes\n\nENV PORT 5050\nENV NODE_ENV=production\nEXPOSE 5050\n\nCMD [\"node\", \"server.js\"]\n\n\n\nMIT License\n\nCopyright (c) 2023 Mattias Persson\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n\nversion: '3'\nservices:\n  ha-fusion:\n    container_name: ha-fusion\n    build: .\n    volumes:\n      - ~/Developer/ha-fusion/data:/app/data\n    network_mode: bridge\n    ports:\n      - 5050:5050\n    environment:\n      TZ: Europe/Stockholm\n      HASS_URL: http://192.168.1.241:8123\n    restart: always\n# cd ~/Developer/ha-fusion && docker-compose up -d --build ha-fusion\n\n\n\n{\n\t\"name\": \"hass-svelte4\",\n\t\"version\": \"0.0.1\",\n\t\"private\": true,\n\t\"scripts\": {\n\t\t\"dev\": \"vite dev\",\n\t\t\"build\": \"vite build\",\n\t\t\"preview\": \"vite preview\",\n\t\t\"check\": \"svelte-kit sync && svelte-check --tsconfig ./tsconfig.json\",\n\t\t\"check:watch\": \"svelte-kit sync && svelte-check --tsconfig ./tsconfig.json --watch\",\n\t\t\"lint\": \"prettier --plugin prettier-plugin-svelte . --check . && eslint .\",\n\t\t\"format\": \"prettier --plugin prettier-plugin-svelte . --write .\"\n\t},\n\t\"devDependencies\": {\n\t\t\"@iconify/svelte\": \"^3.1.6\",\n\t\t\"@sveltejs/adapter-node\": \"^2.0.2\",\n\t\t\"@sveltejs/kit\": \"^2.0.6\",\n\t\t\"@sveltejs/vite-plugin-svelte\": \"^3.0.1\",\n\t\t\"@types/d3-array\": \"^3.2.1\",\n\t\t\"@types/d3-scale\": \"^4.0.8\",\n\t\t\"@types/d3-shape\": \"^3.1.6\",\n\t\t\"@types/express\": \"^4.17.21\",\n\t\t\"@types/js-yaml\": \"^4.0.9\",\n\t\t\"@types/promise-fs\": \"^2.1.5\",\n\t\t\"@typescript-eslint/eslint-plugin\": \"^6.15.0\",\n\t\t\"@typescript-eslint/parser\": \"^6.15.0\",\n\t\t\"eslint\": \"^8.56.0\",\n\t\t\"eslint-config-prettier\": \"^9.1.0\",\n\t\t\"eslint-plugin-svelte\": \"^2.35.1\",\n\t\t\"prettier\": \"^3.1.1\",\n\t\t\"prettier-plugin-svelte\": \"^3.1.2\",\n\t\t\"svelte\": \"^4.2.8\",\n\t\t\"svelte-check\": \"^3.6.2\",\n\t\t\"svelte-fast-dimension\": \"^1.1.0\",\n\t\t\"tslib\": \"^2.6.2\",\n\t\t\"typescript\": \"^5.3.3\",\n\t\t\"vite\": \"^5.0.10\"\n\t},\n\t\"type\": \"module\",\n\t\"dependencies\": {\n\t\t\"@codemirror/autocomplete\": \"^6.11.1\",\n\t\t\"@codemirror/commands\": \"^6.3.2\",\n\t\t\"@codemirror/language\": \"^6.9.3\",\n\t\t\"@codemirror/legacy-modes\": \"^6.3.3\",\n\t\t\"@codemirror/lint\": \"^6.4.2\",\n\t\t\"@codemirror/state\": \"^6.3.3\",\n\t\t\"@codemirror/theme-one-dark\": \"^6.1.2\",\n\t\t\"@codemirror/view\": \"^6.22.3\",\n\t\t\"@fontsource-variable/inter\": \"^5.0.16\",\n\t\t\"@jaames/iro\": \"^5.5.2\",\n\t\t\"codemirror\": \"^6.0.1\",\n\t\t\"d3-array\": \"^3.2.4\",\n\t\t\"d3-scale\": \"^4.0.2\",\n\t\t\"d3-shape\": \"^3.2.0\",\n\t\t\"dotenv\": \"^16.3.1\",\n\t\t\"express\": \"^4.18.2\",\n\t\t\"home-assistant-js-websocket\": \"^9.1.0\",\n\t\t\"http-proxy-middleware\": \"^2.0.6\",\n\t\t\"js-yaml\": \"^4.1.0\",\n\t\t\"maplibre-gl\": \"^3.6.2\",\n\t\t\"marked\": \"^11.1.0\",\n\t\t\"svelecte\": \"^3.17.2\",\n\t\t\"svelte-confetti\": \"^1.3.1\",\n\t\t\"svelte-dnd-action\": \"^0.9.38\",\n\t\t\"svelte-modals\": \"^1.3.0\",\n\t\t\"svelte-ripple\": \"^0.1.1\",\n\t\t\"svelte-tiny-virtual-list\": \"^2.0.5\"\n\t}\n}\n\n\n\nimport { handler } from './build/handler.js';\nimport express from 'express';\nimport { createProxyMiddleware } from 'http-proxy-middleware';\nimport dotenv from 'dotenv';\n\n// dotenv is only used for local build\n// it's not used when building container\ndotenv.config();\n\nconst app = express();\nconst PORT = 5050;\n\n// production proxy\nif (process.env.HASS_URL) {\n\tapp.use(\n\t\t['/local/', '/api/*_proxy*'],\n\t\tcreateProxyMiddleware({\n\t\t\ttarget: process.env.HASS_URL,\n\t\t\tchangeOrigin: true\n\t\t})\n\t);\n}\n\n// let sveltekit handle everything else\napp.use(handler);\n\napp.listen(PORT, () => {\n\tconsole.debug(`listening on port ${PORT}`);\n});\n\n\n\nimport adapter from '@sveltejs/adapter-node';\nimport { vitePreprocess } from '@sveltejs/vite-plugin-svelte';\nimport { fastDimension } from 'svelte-fast-dimension';\n\n/** @type {import('@sveltejs/kit').Config} */\nconst config = {\n\tpreprocess: [vitePreprocess(), fastDimension()],\n\tkit: {\n\t\tadapter: adapter(),\n\t\t// handle ingress\n\t\t// sveltekit 2, test if 'relative: true' can be removed...\n\t\t// https://kit.svelte.dev/docs/migrating-to-sveltekit-2#paths-are-now-relative-by-default\n\t\tpaths: {\n\t\t\trelative: true\n\t\t}\n\t},\n\tvitePlugin: {\n\t\t// dev inspector\n\t\tinspector: {\n\t\t\ttoggleKeyCombo: 'control-shift',\n\t\t\tshowToggleButton: 'never'\n\t\t}\n\t}\n};\n\nexport default config;\n\n\n\n{\n\t\"include\": [\"src/**/*\", \"global.d.ts\"],\n\t\"extends\": \"./.svelte-kit/tsconfig.json\",\n\t\"compilerOptions\": {\n\t\t\"allowJs\": true,\n\t\t\"checkJs\": true,\n\t\t\"esModuleInterop\": true,\n\t\t\"forceConsistentCasingInFileNames\": true,\n\t\t\"resolveJsonModule\": true,\n\t\t\"skipLibCheck\": true,\n\t\t\"sourceMap\": true,\n\t\t\"strict\": true\n\t}\n}\n\n\n\nimport { sveltekit } from '@sveltejs/kit/vite';\nimport { defineConfig } from 'vite';\nimport dotenv from 'dotenv';\n\ndotenv.config();\n\nexport default defineConfig({\n\tplugins: [sveltekit()],\n\tbuild: {\n\t\t// increase chunk size because of maplibre-gl\n\t\tchunkSizeWarningLimit: 800\n\t},\n\tssr: {\n\t\t// \"cannot use import statement outside a module\" because of svelte-ripple\n\t\tnoExternal: ['svelte-ripple']\n\t},\n\toptimizeDeps: {\n\t\tinclude: [\n\t\t\t// include all because of dynamic imports, this prevents: ‚ú® optimized dependencies changed. reloading\n\t\t\t// (pnpm ls -P | grep -Ev 'codemirror|@fontsource' | awk '/dependencies:/{flag=1; next} flag{print \"\\047\" $1 \"\\047,\"}'; echo \"'svelecte/item'\")\n\t\t\t'@jaames/iro',\n\t\t\t'd3-array',\n\t\t\t'd3-scale',\n\t\t\t'd3-shape',\n\t\t\t'dotenv',\n\t\t\t'express',\n\t\t\t'home-assistant-js-websocket',\n\t\t\t'http-proxy-middleware',\n\t\t\t'js-yaml',\n\t\t\t'maplibre-gl',\n\t\t\t'marked',\n\t\t\t'svelecte',\n\t\t\t'svelte-confetti',\n\t\t\t'svelte-dnd-action',\n\t\t\t'svelte-modals',\n\t\t\t'svelte-ripple',\n\t\t\t'svelte-tiny-virtual-list',\n\t\t\t'svelecte/item'\n\t\t],\n\t\texclude: [\n\t\t\t// exclude codemirror to avoid state duplication\n\t\t\t// pnpm ls -P | grep codemirror | awk '{print \"\\047\" $1 \"\\047,\"}'\n\t\t\t'@codemirror/autocomplete',\n\t\t\t'@codemirror/commands',\n\t\t\t'@codemirror/language',\n\t\t\t'@codemirror/legacy-modes',\n\t\t\t'@codemirror/lint',\n\t\t\t'@codemirror/state',\n\t\t\t'@codemirror/theme-one-dark',\n\t\t\t'@codemirror/view',\n\t\t\t'codemirror'\n\t\t]\n\t},\n\t// development proxy endpoints\n\tserver: {\n\t\tproxy: {\n\t\t\t'/local/': {\n\t\t\t\ttarget: process.env.HASS_URL,\n\t\t\t\tchangeOrigin: true\n\t\t\t},\n\t\t\t'/api/image_proxy/': {\n\t\t\t\ttarget: process.env.HASS_URL,\n\t\t\t\tchangeOrigin: true\n\t\t\t},\n\t\t\t'/api/media_player_proxy/': {\n\t\t\t\ttarget: process.env.HASS_URL,\n\t\t\t\tchangeOrigin: true\n\t\t\t},\n\t\t\t'/api/camera_proxy/': {\n\t\t\t\ttarget: process.env.HASS_URL,\n\t\t\t\tchangeOrigin: true\n\t\t\t},\n\t\t\t'/api/camera_proxy_stream/': {\n\t\t\t\ttarget: process.env.HASS_URL,\n\t\t\t\tchangeOrigin: true\n\t\t\t}\n\t\t}\n\t}\n});\n\n\n\n"}
{"date": "2023-12-25-04-17", "error": false, "url": "https://github.com/liujuntao123/chines-history-video", "text_blocks": "# ‰∏≠ÂõΩÂéÜÊúù‰ª£ÂéÜÂè≤ËßÜÈ¢ëËÆ≤Ëß£\n\nÊ±áËÅö‰∫ÜBÁ´ô‰∏äÁªºÂêàÊï∞ÊçÆÔºàÁÇπËµûÔºåÊí≠ÊîæÔºåÊäïÂ∏ÅÔºåÊî∂ËóèÔºâÊúÄÈ´òÁöÑ‰∏Ä‰∏™/Âá†‰∏™ËßÜÈ¢ë„ÄÇ\n\nÁ∫ØÊâãÂä®Êï¥ÁêÜÔºåÂ¶ÇÊûú‰Ω†‰πüÊòØÂéÜÂè≤Áà±Â•ΩËÄÖÔºåËßâÂæóÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåÂèØ‰ª•star‰∏Ä‰∏™„ÄÇ\n\nÂ¶ÇÊûúÊúâ‰ªª‰ΩïÂª∫ËÆÆÔºåÊ¨¢ËøéÊèêissue„ÄÇ\n\nÂú®Á∫øÂú∞ÂùÄ:https://historyline.online\n\n## ÁΩëÁ´ôÈ¢ÑËßà\n\n![PixPin_2023-12-22_14-55-54 (3)](https://github.com/liujuntao123/chines-history-video/assets/22583601/eb865274-606b-49ca-9bd2-62fa35e97adb)\n\nmodule.exports = {\r\n  root: true,\r\n  env: {\r\n    node: true,\r\n  },\r\n  extends: [\r\n    \"plugin:vue/vue3-essential\",\r\n    \"eslint:recommended\",\r\n    \"@vue/prettier\",\r\n  ],\r\n  parserOptions: {\r\n    ecmaVersion: 2020,\r\n  },\r\n  rules: {\r\n  },\r\n};\r\n\n\n\n# Logs\nlogs\n*.log\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\npnpm-debug.log*\nlerna-debug.log*\n\nnode_modules\n.DS_Store\ndist\ndist-ssr\ncoverage\n*.local\n\n/cypress/videos/\n/cypress/screenshots/\n\n# Editor directories and files\n.vscode/*\n!.vscode/extensions.json\n.idea\n*.suo\n*.ntvs*\n*.njsproj\n*.sln\n*.sw?\n\n\n\nmodule.exports = {\r\n  presets: ['@vue/cli-plugin-babel/preset'],\r\n};\r\n\n\n\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <link rel=\"icon\" href=\"/favicon.ico\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>‰∏≠ÂõΩÂéÜÊúù‰ª£ËßÜÈ¢ëËÆ≤Ëß£</title>\n  </head>\n  <body>\n    <div id=\"app\"></div>\n    <script type=\"module\" src=\"/src/main.js\"></script>\n  </body>\n</html>\n\n\n\n{\n  \"name\": \"vuejs-with-vite\",\n  \"version\": \"0.0.0\",\n  \"scripts\": {\n    \"dev\": \"vite\",\n    \"build\": \"vite build\",\n    \"preview\": \"vite preview --port 4173\"\n  },\n  \"dependencies\": {\n    \"@ant-design/icons-vue\": \"^7.0.1\",\n    \"ant-design-vue\": \"^4.0.7\",\n    \"vue\": \"^3.3.4\"\n  },\n  \"devDependencies\": {\n    \"@vitejs/plugin-vue\": \"^4.2.3\",\n    \"@vue/cli-plugin-eslint\": \"^5.0.8\",\n    \"@vue/eslint-config-prettier\": \"^8.0.0\",\n    \"eslint\": \"^8.55.0\",\n    \"eslint-plugin-prettier\": \"^5.0.1\",\n    \"eslint-plugin-vue\": \"^9.19.2\",\n    \"prettier\": \"^3.1.1\",\n    \"unplugin-vue-components\": \"^0.26.0\",\n    \"vite\": \"^4.5.1\"\n  }\n}\n\n\n\nmodule.exports = {\r\n  singleQuote: true,\r\n  printWidth: 4000,\r\n  endOfLine: 'auto',\r\n};\r\n\n\n\n{\r\n\t\"compilerOptions\": {\r\n\t\t\"target\": \"esnext\",\r\n\t\t\"module\": \"esnext\",\r\n\t\t\"strict\": true,\r\n\t\t\"jsx\": \"preserve\",\r\n\t\t\"importHelpers\": true,\r\n\t\t\"moduleResolution\": \"node\",\r\n\t\t\"experimentalDecorators\": true,\r\n\t\t\"skipLibCheck\": true,\r\n\t\t\"esModuleInterop\": true,\r\n\t\t\"allowSyntheticDefaultImports\": true,\r\n\t\t\"sourceMap\": true,\r\n\t\t\"baseUrl\": \".\",\r\n\t\t\"allowJs\": true,\r\n\t\t\"types\": [\r\n\t\t\t\"webpack-env\",\r\n\t\t\t\"node\"\r\n\t\t],\r\n\t\t\"paths\": {\r\n\t\t\t\"@/*\": [\r\n\t\t\t\t\"src/*\"\r\n\t\t\t]\r\n\t\t},\r\n\t\t\"lib\": [\r\n\t\t\t\"esnext\",\r\n\t\t\t\"dom\",\r\n\t\t\t\"dom.iterable\",\r\n\t\t\t\"scripthost\"\r\n\t\t],\r\n\t\t\"noImplicitAny\": false\r\n\t},\r\n\t\"include\": [\r\n\t\t\"src/**/*.ts\",\r\n\t\t\"src/**/*.tsx\",\r\n\t\t\"src/**/*.vue\",\r\n\t\t\"tests/**/*.ts\",\r\n\t\t\"tests/**/*.tsx\",\r\n\t\t\"src/image.d.ts\"\r\n\t],\r\n\t\"exclude\": [\r\n\t\t\"node_modules\"\r\n\t]\r\n}\r\n\n\n\nimport { fileURLToPath, URL } from 'node:url';\nimport { defineConfig } from 'vite';\nimport vue from '@vitejs/plugin-vue';\nimport Components from 'unplugin-vue-components/vite';\nimport { AntDesignVueResolver } from 'unplugin-vue-components/resolvers';\n\n// https://vitejs.dev/config/\nexport default defineConfig({\n  server: {\n    host: true,\n  },\n  plugins: [\n    vue(),\n    Components({\n      resolvers: [\n        AntDesignVueResolver({\n          importStyle: false, // css in js\n        }),\n      ],\n    }),\n  ],\n  resolve: {\n    alias: {\n      '@': fileURLToPath(new URL('./src', import.meta.url)),\n    },\n  },\n});\n\n\n\n"}
