{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "Time-inconsistent mean field and n-agent games under relative performance\ncriteria∗\nZongxia Liang†andKeyu Zhang‡\nAbstract. In this paper we study a time-inconsistent portfolio optimization problem for competitive agents\nwith CARA utilities and non-exponential discounting. The utility of each agent depends on her own\nwealth and consumption as well as the relative wealth and consumption to her competitors. Due\nto the presence of a non-exponential discount factor, each agent’s optimal strategy becomes time-\ninconsistent. In order to resolve time-inconsistency, each agent makes a decision in a sophisticated\nway, choosing open-loop equilibrium strategy in response to the strategies of all the other agents. We\nconstruct explicit solutions for the n-agent games and the corresponding mean field games (MFGs)\nwhere the limit of former yields the latter. This solution is unique in a special class of equilibria. As\na by-product, we find that competition in the time-inconsistency scenario modifies the agent’s risk\ntolerance as it does in the time-consistent scenario.\nKey words. relative performance, mean field games, time-inconsistency, open-loop equilibrium, forward back-\nward stochastic differential equation\nAMS subject classifications. 91A06, 91A07, 91A16, 91B51\n1. Introduction. Due to the fact that peer interaction sometimes makes remarkable im-\npacts on agent’s decision making, portfolio games, as a game-theoretic extension of classical\nMerton problem [28], have received considerable attention in recent years. Relative perfor-\nmance is becoming an appealing way to model the interaction because of the tractability\nin mathematics and excellent economic motivations. The literature on portfolio games with\nrelative performance concerns dates by to Espinosa and Touzi [16], where they consider n-\nagent games with portfolio constrains under the CARA utility by investigating the associated\nquadratic BSDEs systems. Lacker and Zariphopoulo [24] consider the portfolio games for asset\nspecialized agents in log-normal markets under both CARA and CRRA relative performance\ncriteria. The constant Nash equilibrium and mean field equilibrium (MFE) are explicitly con-\nstructed. In a similar fashion, Lacker and Soret [23] extend the problem by incorporating the\ndynamic consumption. Bo et al. [7] revisit the MFGs and the n-agent games under CRRA\nrelative performance by allowing risky assets to have contagious jumps. A deterministic MFE\nin an analytical form is obtained by using the FBSDE and stochastic maximum principle.\nFurthermore, an approximate Nash equilibrium for the n-agent games is constructed. Re-\ncently, Fu and Zhu [18] study the mean field portfolio games with general market parameters.\nA one-to-one correspondence between the Nash equilibrium and the solution to some FBSDE\nis established by martingale optimality principle.\n∗Submitted to the editors November 11, 2022.\nFunding: This work was funded by the National Natural Science Foundation of China, grants 12271290 and\n11871036.\n†Department of Mathematical Sciences, Tsinghua University, Beijing, 100084 People’s Republic of China (liang-\nzongxia@mail.tsinghua.edu.cn).\n‡Department of Mathematical Sciences, Tsinghua University, Beijing, 100084 People’s Republic of China\n(zhangky21@mails.tsinghua.edu.cn).\n1\nThis manuscript is for review purposes only.arXiv:2312.14437v1  [q-fin.MF]  22 Dec 20232 ZONGXIA LIANG AND KEYU ZHANG\nAnother research direction with fruitful outcomes is time-inconsistent control problem,\nwhere the Bellman optimality principle does not hold. There are many important problems\nin mathematical finance and economics incurring time-inconsistency, for example, the mean-\nvariance selection problem and the investment-consumption problem with non-exponential\ndiscounting. The main approaches to handle time-inconsistency are to search for, instead of\noptimal strategies, time-consistent equilibrium strategies within a game-theoretic framework.\nEkeland and Lazrak [14] and Ekeland and Pirvu [15] introduce the precise definition of the\nequilibrium strategy in continuous-time setting for the first time. Bj¨ ork et al. [5] derive\nan extended HJB equation to determine the equilibrium strategy in a Markovian setting.\nYong [31] introduces the so-called equilibrium HJB equation to construct the equilibrium\nstrategy in a multi-person differential game framework with a hierarchical structure. The\nsolution concepts considered in [5, 31] are closed-loop equilibrium strategies and the methods\nto handle time-inconsistency are extensions of the classical dynamic programming approaches.\nIn contrast to the aforementioned literature, Hu et al. [21] introduce the concept of open-loop\nequilibrium control by using a spike variation formulation, which is different from the closed-\nloop equilibrium concepts. The open-loop equilibrium control is characterized by a flow of\nFBSDEs, which is deduced by a duality method in the spirit of Peng’s stochastic maximum\nprinciple. Some recent studies devoted"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "1 \n Does religiosity influence corporate greenwashing behavior? \n \nMathieu Gomesa,* \n \nSylvain Marsata \n \nJonathan Peillexb \n \nGuillaume Pijourleta \n \n \n \naUniversité Clermont Auvergne, CleRMa, 11 bd Charles de Gaulle, 63000 Clermont-Ferrand, \nFrance. \n \nbICD International Business School, 12 rue Alexandre Parodi, 75010 Paris, France. \n \nPost-print version accepted for publication in Journal of Cleaner Production , \nhttps://doi.org/10.1016/j.jclepro.2023.140151  Jan. 2024 \nAbstract \nWe analyze the influence of religious social norms on corporate greenwashing behavior. Specifically, we \nfocus on a specific form of greenwashing: selective disclosure. Using a large sample of US firms between \n2005 and 2019, we show that firms located in counties where religious adherence is high are less likely \nto engage in greenwashing. We also find that a stronger religious adherence within the county in which \na company is located reduces the magnitude of greenwashing, when observed. We further analyze the \nmechanism underlying this relationship and show that religious adherence impacts greenwashing \nbehaviors through the channel of risk aversion. A comprehensive set of robustness tests aimed at \naddressing potential endogeneity concerns confirms that religion is a relevant driver of corporate \ngreenwashing behavior. \nJEL codes:  G30; M14; M40 \nKeywords:  Greenwashing; selective disclosure; religion; social norms. \n \n  2 \n 1. Introduction \n \nThe impact of religion on various economic outcomes has been the subject of many academic studies \n(Barro and McCleary, 2003; Guiso et al., 2003, Iannaccone, 1998; Lehrer, 2004; among others). \nInterestingly, researchers have only recently started to look at the influence of religious adherence on \ncorporate decision making and especially on unethical manipulative corporate behavior (Hilary and Hui, \n2009; Dyreng et al., 2012; McGuire et al., 2012; Kanagaretnam et al., 2015; Kirchmaier et al., 2018; \nLeventis et al., 2018; Cai et al., 2019; Harjoto et al., 2019; Chantziaras et al., 2020; Abdelsalam et al., \n2021; Terzani et al., 2021). \nDespite this increased interest, no research has been conducted thus far to study the \nrelationship between religious adherence and corporate greenwashing behaviors. Nonetheless, \ngreenwashing practices are typically an example of manipulative behavior (Delmas and Burbano, 2011; \nBoncinelli et al., 2023), consisting in giving a distorted image of the environmental efforts made by the \ncompany in order to influence consumer behavior (Meisinger, 2022, Santa and Drews, 2023).  It has \nhowever been shown that religiosity has a negative influence on corporate unethical behavior such as \nearnings management (Abdelsalam et al., 2021; Cai et al., 2019; Kanagaretnam et al., 2015; McGuire et \nal., 2012; Dyreng et al., 2012).  \nIn this paper, we focus on one type of organizational greenwashing—the selective \nenvironmental disclosure—which consists of disclosing “positive environmental actions while \nconcealing negative ones to create a misleadingly positive impression of overall environmental \nperformance” (Marquis et al., 2016). Selective disclosure can be seen as a form of greenwashing insofar \nas some information is voluntarily not disclosed to create a positive impression (Marquis et al., 2016). \nBecause greenwashing may produce negative effects on consumers and investors’ confidence (Arouri \net al., 2021; Zhang et al., 2018) and prevent the development of sustainable markets (Boncinelli et al., \n2023), it is crucial to understand the drivers of such behavior. 3 \n According to social norm theory, individuals adopt the behavior and values of the social group \nwith which they associate, and conform to the dominant social norms of the group (Dyreng et al., 2012; \nKohlberg, 1984; Sunstein, 1996; McGuire et al., 2012). Neo-institutionalist theory also suggests that \norganizations adopt these social norms to maintain their legitimacy in society, and thus ensure their \nsurvival (Meyer & Rowan 1977; DiMaggio & Powell 1983). Two major social norms promoted by religions \nmay influence corporate decisions and lead us to hypothesize a negative impact of religious adherence \non greenwashing. First, religions are unanimously opposed to the principle of lying and manipulation of \nothers (Callen and Fang 2015; Kanagaretnam et al., 2015). This would be in line with promoting honesty, \navoiding manipulative corporate behaviors and condemning actions aimed at hiding bad news from \ninvestors (Callen and Fang, 2015). Second, the literature clearly shows that religious people are more \nrisk averse than others (Miller and Hoffmann, 1995; Dohmen et al., 2011; Noussair et al., 2013; Diaz, \n2000; Iannaccone, 1998) and that religious adherence is associated with risk-averse behaviors while \nirreligiosity tends to be associated with risk-taking characteristics (Miller and Hoffmann, 1995). To the \nextent that greenwashing can pose a serious reputational risk if uncovered, even more so if "}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "A HYPOTHESIS TEST FOR THE LONG-TERM CALIBRATION IN\nRATING SYSTEMS WITH OVERLAPPING TIME WINDOWS\nPATRICK KURTH, MAX NENDEL, AND JAN STREICHER\nAbstract. We present a statistical test that can be used to verify supervisory re-\nquirements concerning overlapping time windows for the long-term calibration in rat-\ning systems. In a first step, we show that the long-run default rate is approximately\nnormally distributed with respect to random effects in default realization. We then\nperform a detailed analysis of the correlation effects caused by the overlapping time\nwindows and solve the problem of an unknown distribution of default probabilities\nfor the long-run default rate. In this context, we present several methods for a conser-\nvative calibration test that can deal with the unknown variance in the test statistic.\nWe present a test for individual rating grades, and then pass to the portfolio level by\nsuitably adapting the test statistic. We conclude with comparative statics analysing\nthe effect of persisting customers and the number of customers per reference date.\n1.Introduction\nFinancial institutions use statistical models to estimate the default risk of obligors\nin order to manage credit-risks. According to Basel II, banks are allowed to estimate\nrisk parameters that are used to calculate regulatory capital with their own models.\nThe legal framework for the use of such models in the internal ratings-based (IRB) ap-\nproach is regulated in the Capital Requirements Regulation (CRR), see [5]. The CRR\nimposes specific requirements for the models, e.g., that “institutions shall estimate PDs\nby obligor grade from long run averages of one-year default rates”, see Article 180. In\n2017, the European banking authority published the “guidelines on PD estimation,\nLGD estimation and the treatment of defaulted exposures” (EBA-GL) specifying the\nCRR requirement, see [10]. Paragraph 81 EBA-GL states that “institutions should\ncalculate the observed average default rates as the arithmetic average of all one year\ndefault rates”. This requirement was additionally specified in [8] and in [9]. Here, the\nobserved one-year default rate at a given reference date is defined as the percentage\nof defaulters in the following year, so that the observed long-run average default rate\ndepends on the choice of the reference dates. Paragraph 80 EBA-GL allows institutions\nto choose “between an approach based on overlapping and an approach based on non-\noverlapping one-year time windows”. Overlapping one-year time windows occur when\nthe time interval between two reference dates is less than one year. Due to computa-\ntional simplicity, it is of course convenient to continue working with non-overlapping\ntime windows and appropriately adjust the long-term default rate. In many cases, these\ntype of adjustments are rather on the conservative side, see, e.g., [13] for an empirical\nstudy and [15, 18] for theoretical analyses in the context of asset correlation.\nDate : December 25, 2023.\nThe authors thank Markus Klein for helpful discussions related to this work. This work was funded\nby the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – SFB 1283/2 2021\n– 317210226. The first and the third author are grateful for the support of the Landesbank Baden-\nW¨ urttemberg related to this work.\n1arXiv:2312.14765v1  [q-fin.RM]  22 Dec 20232 PATRICK KURTH, MAX NENDEL, AND JAN STREICHER\nOn the other hand, the approach, using overlapping time windows, provides more\ninformation on defaults due to potential short-term contracts, which cannot be observed\nduring one-year periods. It is therefore favored by most financial institutions that handle\nportfolios with only few observed defaults. Another advantage of this approach lies in\nthe fact that the bias caused by a specific choice of reference dates can be reduced,\ne.g., when calculating the long-run average default rate as the arithmetic mean of the\none-year default rates on a quarterly basis.\nParagraph 87 EBA-GL requires institutions to use a statistical test of calibration at\nthe level of rating grades and for certain portfolios. Classically, the literature dealing\nwith calibration tests assumes a binomial-distributed default rate, see, e.g., [7] and [17]\nfor a discussion of different hypothesis tests in this context. We also refer to [6] for the\nconsideration of different PDs within the same rating grade and [3, 4] for a modified\nbinomial test accounting correlated defaults. However, when considering overlapping\ntime windows, the assumption of a binomial distribution can no longer be maintained\nwhen considering overlapping time windows.\nMore generally, Monte Carlo methods can be used to construct tests for distributions\nthat cannot be determined analytically. For the use of Monte Carlo methods, precise\nknowledge of the distribution of the probabilities of default within the portfolio is\nessential. However, in our case, these probabilities are unknown and estimated by the\nunderlying model, so that"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "1 \n  \n \nIn the Line of Fire: A Systematic Review and Meta -Analysis of Job Burnout \nAmong Nurses  \n \n \n \n \n \n \n \nZahra Ghasemi Kooktapeh \nManagement Faculty, Kharazmi University, Tehran, Iran.  \n Zahra.ghasemi0304@gmail.com  \n \nHakimeh Dustmohammadloo  \nDepartment of Management , Unikl University  Kuala Lumpur, Malaysia.  \n \nHooman Mehrdoost  \nDepartment of Mechanical Engineering, Faculty of Engineering, Islamic Azad University, Mashhad, Iran.  \n \nFarivar Fatehi  \nSheldon B. Lubar College of Business, University of Wisconsin Milwaukee , Milwaukee,  WI, USA  \nFfatehi@uwm.edu   2 \n Abstract  \nUsing a systematic review and meta -analysis, this study investigates the impact of the COVID -19 \npandemic on job burnout among nurses. We review healthcare articles following the PRISMA \n2020 guidelines and identify the main aspects and factors of burnout am ong nurses during the \npandemic. Using the Maslach Burnout questionnaire, we searched PubMed, ScienceDirect, and \nGoogle Scholar, three open -access databases, for relevant sources measuring emotional burnout, \npersonal failure, and nurse depersonalization . Two reviewers extract and screen data from the \nsources and evaluate the risk of bias. The analysis reveals that 2.75% of nurses experienced job \nburnout during the pandemic, with a 95% confidence interval and rates varying from 1.87% to \n7.75%. These findings emphasize the need for interventions to address the pandemic's effect on \njob burnout among nurses and enhance their well -being and healthcare quality. We recommend \nconsidering individual, organizational, and contextual factors influencing healthcare worker s' \nburnout . Future research should focus on identifying effective interventions to lower burnout in \nnurses and other healthcare professionals during pandemics and high -stress situations.  \nKeywords : Healthcare , Nurse Burnout,  Human  Resources,  COVID -19 pandemic , Systematic Review .  \n \nIntroduction  \nBurnout syndrome poses a significant global challenge in human resource management (Rusca et \nal., 2019). It arises from prolonged exposure to work -related stress and is characterized by a decline \nin performance and functionality due to overwhelming psychological demands within the \nworkplace (Panagioti et al., 2018).  Bai et al. (2023) mention in their study that several factors \nunderpin workplace pressures, with notable stressors including the challenges of \"balancing work \nand family,\" \"weak internal communications,\" \"working hours/workload,\" and \"weak leadership.\"   \nNotably, healthcare service providers, including doctors, nurses, and medical staff, are particularly \nsusceptible to burnout (Friganoviü et al., 2019; Jahanshahi et al., 2020; Chen et al., 2021; Zhang \net al., 2020 ). Nurses, in particular, play a critical role on the front lines in combating illnesses (Zare \net al., 2021), and they experience the highest incidence of burnout (Woo et al., 2020; Khaksar et \nal., 2010a ), estimated at a staggering 54% probability (Rezaei et al., 2018a). The burnout syndrome \nencompasses a constellation of physical, mental, and emotional disorders that manifest as \npessimism and a decline in the performance of healthcare professionals (Gómez Urquiza et al., \n2017). Importantly, this deterioration in performance has far -reaching consequences, adversely \naffecting the quality of patient care and contributing to increased job turnover (Shajiei et al., 2020; 3 \n Hailay et al., 2020)  due to less organizational commitment . According to Bai and Vahedian (2023), \nthe relationship between organizational commitment and turnover is inverse. Employees who are \nmore committed are less likely to leave the organization than those who are not. Their finding \nshows how important it is to increase organizational commitment among nurses to reduce turnover \nintentions.  \nThe current global issue of nurse shortages is of significant concern , primarily driven by high \nattrition rates (Liu et al., 2018). This problem has been further exacerbated in the context of the \nCOVID -19 pandemic (KILIÇ et al., 2021), with a distressing 31.9% mortality rate among nurses \nas of September 16, 20 (Galanis et a l., 2020). Consequently, this situation presents a grave health \npolicy crisis  (Azari et al., 2021).  Hence, healthcare system managers must prioritize  the issue of \nburnout syndrome for two  fundamental reasons. First , uphold the patients' rights charter and ensure  \nhigh-quality services while safeguarding patients' rights (Gilavandi et al., 2019). Secondly, it is \nessential to support the diverse skills of healthcare professionals, particularly nurses, given the \nmany  responsibilities they shoulder within medical facilities (Gilavandi et al., 2019).  \nThe advent of the COVID -19 pandemic has impacted nurses' attention, understanding, and \ndecision -making abilities and has  had chronic and ongoing repercussions on their physical, mental, \nand social well -being (Kang et al., 2020). Consequently, this has the pot"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "SCALABLE AGENT -BASED MODELING FOR COMPLEX\nFINANCIAL MARKET SIMULATIONS\nAaron Wheeler\nR.F Smith School of Chemical and Biomolecular Engineering\nCornell University\nIthaca, NY\naw843@cornell.edu\nJeffrey D. Varner\nR.F Smith School of Chemical and Biomolecular Engineering\nCornell University\nIthaca, NY\njdv27@cornell.edu\nABSTRACT\nIn this study, we developed a computational framework for simulating large-scale agent-based finan-\ncial markets. Our platform supports trading multiple simultaneous assets and leverages distributed\ncomputing to scale the number and complexity of simulated agents. Heterogeneous agents make\ndecisions in parallel, and their orders are processed through a realistic, continuous double auction\nmatching engine. We present a baseline model implementation and show that it captures several\nknown statistical properties of real financial markets (i.e., stylized facts). Further, we demonstrate\nthese results without fitting models to historical financial data. Thus, this framework could be used for\ndirect applications such as human-in-the-loop machine learning or to explore theoretically exciting\nquestions about market microstructure’s role in forming the statistical regularities of real markets.\nTo the best of our knowledge, this study is the first to implement multiple assets, parallel agent\ndecision-making, a continuous double auction mechanism, and intelligent agent types in a scalable\nreal-time environment.\nKeywords Financial Market Simulation ·Agent-Based Models (ABMs) ·Complex Systems\n1 Introduction\nModern society and the global economy heavily rely on financial markets, making it essential to develop tools that\ncan help understand and navigate their complexity. Beyond the analysis of historical data sets, which are costly and\nlimited, we focus here on methods for generating and interacting with synthetic data—i.e., realistic market simulations.\nIn addition to its low cost and accessibility, simulation also benefits from the ability to incorporate feedback effects\nand generate data for scenarios that have occurred only in rare circumstances—or perhaps scenarios that have yet\nto happen. High-fidelity market simulators can benefit market stakeholders, including retail investors, institutional\ninvestors, regulators, and others, by testing execution algorithms across various scenarios, developing more robust\ntrading models, training models to detect financial crime, and observing policy changes’ effects before implementing\nnew regulations. Therefore, a realistic market simulation technology can promote more efficient, fair, and stable markets,\nbenefiting the public.\nThe modeling of financial markets is challenging due to various factors, including the large number of participants in\nthe market, their adaptability and connectedness, and the fast pace of market events. Modelers often abstract market\nfeatures from their models to deal with this complexity. Further, it is often difficult to reason which features are\nnecessary or unnecessary. Market microstructure (i.e., the detailed description of trading mechanics [1]) are typicallyarXiv:2312.14903v1  [q-fin.TR]  22 Dec 2023Scalable Agents-Based Financial Market Simulation\nabstracted away in models. For example, in a real equities market, shares are bought and sold on electronic exchanges\nthrough digital records known as order books. Order books hold the queue of pending orders for a particular asset,\nand orders are fulfilled and removed from the book using matching algorithms (e.g., price-time priority). Thus, order\nbooks are a real-time record of supply and demand for a particular asset. On the demand side, buyers submit bids\nwhile sellers submit asks on the supply side of the book. The bid-ask spread is the difference between the best\nbid and the best ask price. Relatedly, liquidity measures how easy it is to carry out a transaction at a stable price;\nilliquid assets have sparse order books with a wide bid-ask spread. Crucially, market-maker firms provide liquidity\nby maintaining concurrent offers to conduct transactions between buyers and sellers. Without the inclusion of order\nbooks and market-makers, financial models fail to capture empirical regularities of markets and have limited use in\npractice [2–4].\nEmpirical regularities of financial markets—i.e., nontrivial statistical properties that are observed across a wide range\nof instruments, markets, and periods —are known as stylized facts [3, 5–7]. Research on stylized facts can be\ntraced back to the 1960s when Mandelbrot studied the heavy-tailed distributions and long-term correlations of financial\nprice return time series [8, 9]. Over time, other distinct stylized facts have been identified and categorized as univariate\nproperties (related to a single asset) or multivariate properties (related to multiple assets). The most extensively\nresearched phenomena are the univariate stylized facts that describe the characteristics seen in both price and volume\nseries of a single asset in isolation [5,"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "1 \n Learning from diversity: “ jati” fractionalization, social expectations and \nimproved sanitation practices in India  \nSania Ashraf1, Cristina Bicchieri2, Upasak Das3, Tanu Gupta4, Alex Shpenev5 \nAbstract  \nPrevalence of open defecation is associated with adverse health effects , detrimental  not only for \nthe individual but also the community. Therefore, neighborhood characteristics can influence \ncollective progressive behavior such as  improved sanitation practices. This paper uses primary \ndata collected from rural and urban areas of Bihar to study the relationship between ‘jati’ (sub -\ncastes) level fractionalization within the community and toilet ownership and its usage for \ndefecation.  The findings indicate a diversity dividend wherein jati fr actionalization is found to \nimprove toilet ownership and usage significantly. While exploring the channels, we find social \nexpectations to play an important role, where individuals from diverse communities tend to believe \nthat there is a higher prevalence of toilet usage within the community. To assess the reasons for \nthe existence of these social expectations, we use data from an egocentric network survey on a \nsub-sample of the households. The findings reveal that in fractionalized communities, the \nneighbo rs with whom our respondents interacted are more likely to be from different jatis. They \nare also more likely to use toilets and approve  of its usage due to health reasons. Discussions \nabout toilets are more common among neighbors from fractionalized communities, which \nunderscore the discernible role of social learning. The inferences drawn from the paper have \nsignificant implications for com munity level behavioral change interventions that aim at reducing \nopen defecation.  \nKeywords:  sanitation , open defecation, toilet use,  health,  caste diversity,  collective behavior, \nsocial learning, networks , norms, India  \nJEL Classification:  I15, O2, H42 , Z13   \n \n1 University of Pennsylvania, saniashraf@gmail.com  \n2 University of Pennsylvania, cb36@upenn.edu   \n3 Global Development Institute, University of Manchester , upasak.das@manchester.ac.uk  \n4 Indian Statistical Institute, Delhi , Corresponding author, tanug@isid.ac.in  \n5 University of Pennsylvania, shpenev@sas.upenn.edu  2 \n 1. Introduction  \nPoor sanitation practices that include defecation in open spaces have remained a long -standing \nissue in India, especially in the rural areas. It has been documented that 48 per cent of the \npopulation in India  (close to 594 million)  practices open d efecation (OD) .6 This assumes \nimportance as existing studies indicate that OD can be linked adversely to health measures such \nas fecal -borne illness, diarrhoea, urinary tract infection symptoms and premature birth (Baker et \nal. 2017, Baker et al. 2018; Sclar et al., 2018 ). Apart from the health impact, the economic cost \nalso remains high (Hutton  et al. , 2008). Additionally, OD can pose significant health hazard to the \nnearby residents. If neighbors continue to OD, the whole community is at potentially higher health \nrisk compared to those residing in areas with lower prevalence of OD (Cameron et al. 2013 ). \nTherefore policies including the Swachh Bharat Abhiyan (SBA), which have been implemented \nto enhance toilet construction and increase toilet usage for defecation, depend on behavioral \nchange activities that leverage the community or neighborhood and their characteristics to achieve \nits objectives. In this paper, we examine one key feature of neighborhood - caste diversity among \nthe residents and examine its implications for toilet usage behavior in India. In the process, we \nassess the role of social expectations and beliefs and then investigate the significance of social \nlearning by exploring the relevant social network characteristics in influencing sanitation behavior.  \n In the context of India’s social structure, caste or “ jati” (in many Indian languages) is a \ncrucial ingredient of stratification (Deshpande, 2000). Literature has indicated its significance as \nthe basis of social network formation and connection within Indian society (Desai and Dubey , \n2011, Munshi, 2019). It is also documented that households from deprived jatis such as those from \nScheduled Castes (SC) and Other Backward Castes (OBC) groups are less likely to use toilets for \ndefecation (Kumar and Vollmer, 2013;  Banerjee et al. 2017). Nevertheless, there has been limited \nfocus in the literature on understanding how social and institutional factors, viewed through the \nlens of caste, impact sanitation b ehavior at the community level . India is among the most socially \ndiverse countries in the world with over 3000 different jatis (Munshi, 2019) and therefore assessing \nthe effects of caste diversity assumes importance.  \n \n6 Please refer https://www.unicef.org/india/campaigns/take -poo-loo (accessed on December 6, 2023)  3 \n  Existing literature has emphasized the concept of “diversity debit”, whic"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "DTIAM: A unified framework for predicting\ndrug-target interactions, binding affinities and\nactivation/inhibition mechanisms\nZhangli Lu1, Chuqi Lei1, Kaili Wang1, Libo Qin1, Jing Tang2\nand Min Li1*\n1*School of Computer Science and Engineering, Central South\nUniversity, Changsha, 410083, China.\n2Research Program in Systems Oncology, Faculty of Medicine,\nUniversity of Helsinki, Helsinki, FI00014, Finland.\n*Corresponding author(s). E-mail(s): limin@mail.csu.edu.cn;\nAbstract\nAccurate and robust prediction of drug-target interactions (DTIs) plays\na vital role in drug discovery. Despite extensive efforts have been invested\nin predicting novel DTIs, existing approaches still suffer from insuffi-\ncient labeled data and cold start problems. More importantly, there is\ncurrently a lack of studies focusing on elucidating the mechanism of\naction (MoA) between drugs and targets. Distinguishing the activation\nand inhibition mechanisms is critical and challenging in drug develop-\nment. Here, we introduce a unified framework called DTIAM, which\naims to predict interactions, binding affinities, and activation/inhibi-\ntion mechanisms between drugs and targets. DTIAM learns drug and\ntarget representations from large amounts of label-free data through\nself-supervised pre-training, which accurately extracts the substructure\nand contextual information of drugs and targets, and thus benefits the\ndownstream prediction based on these representations. DTIAM achieves\nsubstantial performance improvement over other state-of-the-art meth-\nods in all tasks, particularly in the cold start scenario. Moreover,\nindependent validation demonstrates the strong generalization ability of\nDTIAM. All these results suggested that DTIAM can provide a prac-\ntically useful tool for predicting novel DTIs and further distinguishing\nthe MoA of candidate drugs. DTIAM, for the first time, provides a\nunified framework for accurate and robust prediction of drug-target\ninteractions, binding affinities, and activation/inhibition mechanisms.\n1arXiv:2312.15252v1  [q-bio.BM]  23 Dec 20232 A unified framework for drug-target prediction\n1 Introduction\nAccurately predicting drug-target interactions (DTIs) is an essential step in\ndrug discovery and development [1, 2]. The biochemical experimental method\nfor identifying new DTIs on a large scale is still expensive and time-consuming\n[3–5], despite the wide application of various experimental assays in drug dis-\ncovery. Various computational methods have been applied to drug discovery\nand successfully predict novel DTIs, and they can substantially reduce devel-\nopment time and costs [6–8]. Current computational methods mainly focus\non the binary prediction of DTI or the regression prediction of drug-target\nbinding affinity (DTA).\nIn binary classification-based DTI prediction studies, the goal is to pre-\ndict whether there is an interaction between the drug and the target or not.\nGenerally, the approaches for in silico DTI prediction can be divided into two\nmajor categories: structure-based approaches and structure-free approaches.\nStructure determination of compound-protein complexes can provide insights\ninto the mode of action and thus significantly facilitate lead compound selec-\ntion and optimization in the target-based drug discovery [9, 10]. There are\nmany structure-based approaches, such as molecular docking [11], molecu-\nlar dynamics simulations [12], pharmacophore modeling [13] and GOLD [14],\nwhich are widely applied in virtual screening of drugs binding with proteins.\nHowever, these methods generally fail to predict binding affinities when the\nthree-dimensional (3D) structure of the target protein is unknown, and require\ntremendous computational resources. To overcome the current limitations of\nthe structure-based methods, various structure-free models have been devel-\noped for DTI prediction [15–18]. An example is the network-based inference\n(NBI) methods that construct reliable networks from several data resources\n(e.g., chemical, genomics, proteomics, and pharmacology) and exploit the\ntopological and structural information in the networks for potential associa-\ntion prediction [19–22]. For instance, Luo et al. [23] develop a computational\npipeline, called DTINet, to predict novel DTIs from a heterogeneous network\nconstructed by integrating diverse drug-related information. Another promis-\ning approach for predicting DTIs is the machine learning-based methods that\nmainly consist of two steps: feature extraction and DTI prediction [24–27]. This\ntype of approach fully exploits the latent features from input data of known\ndrug compounds and target proteins to predict their interactions [28, 29].\nWhile these methods can successfully predict the interactions between each\npair of drugs and targets, they fail to infer the strength of the interaction\nbetween the drug–target pairs.\nIn order to further predict the putative strengths of the interactions, vari-\nous regression-based models have been proposed to infer the bi"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.15270v1  [q-bio.PE]  23 Dec 2023Anticipating dengue outbreaks using a novel hybrid\nARIMA-ARNN model with exogenous variables\nIndrajit Ghosha, Shashank Guptab, Sourav Rana1c\naDepartment of Mathematics, Indian Institute of Technology , Bombay - 400076, Maharashtra, India\nbIPM (2022-27), Indian Institute of Management, Indore-453 556, Madhya Pradesh, India\ncDepartment of Statistics, Visva Bharati, Shantiniketan - 7 31235, West Bengal, India\nAbstract\nDengue incidence forecasting using hybrid models has been surging in the data rich\nworld. Hybridization of statistical time series forecasting models an d machine learning\nmodels are explored for dengue forecasting with diﬀerent degrees of success. In this\npaper, we propose a multivariate expansion of the hybrid ARIMA-AR NN model. The\nmain motivation is to propose a novel hybridization and apply it to deng ue outbreak\nprediction. The asymptotic stationarity of the proposed model ha s been established.\nWe check the forecasting capability and robustness of the foreca sts through numerical\nexperiments. State-of-the-art forecasting models for multivar iate time series data are\ncompared with the proposed model using accuracy metrics. Dengu e incidence data from\nSan Juan and Iquitos are utilized along with rainfall as an exogenous v ariable. Results\nindicate that the proposed model improves the ARIMAX forecasts in some situations\nand closely follows it otherwise. The theoretical as well as experimen tal results reinforce\nthat the proposed model has the potential to act as a candidate f or early warning of\ndengue outbreaks. The proposed model can be readily generalized to incorporate more\nexogenous variables and also applied to other time series forecastin g problems wherever\nexogenous variable(s) are available.\nKeywords: Time series forecasting, Hybrid models, Auto-regressive integrat ed moving\naverage model, Auto-regressive neural networks, Dengue incide nce data\n1. Introduction\nDengue is the most prevalent and rapidly spreading mosquito-borne viral disease.\nThe incidence of dengue has grown dramatically around the world in re cent decades,\nwith cases reported to WHO increasing from 505,430 cases in 2000 to 5.2 million in 2019\n[1]. The burden of dengue has become heavier from 1990 to 2019, amid st the three\n1Corresponding author. Email: sourav.rana@visva-bharati.ac.in\nPreprint submitted to arXiv December 27, 2023decades of urbanization, warming climates and increased human mob ility in much of the\nworld [ 33]. Dengue fever is mostly observed in tropical and sub-tropical reg ions of the\nglobe. In particular, several pockets in Africa, Southeast Asian c ountries and the western\nPaciﬁc region are prone to a high burden of dengue disease.\nThe virus is transmitted to humans through the bites of infected fe male mosquitoes,\nprimarily the Aedes aegypti mosquito. Other species within the Aede s genus can also act\nas vectors, but their contribution is secondary to Aedes aegypti. While the majority of\ninfections are milder asymptomatic, the more severe forms of deng ue infection - dengue\nshock syndrome (DSS) and dengue hemorrhagic fever (DHF) - can result in organ failure\nor death [ 27]. Developing a dengue vaccine has proven challenging due to various f actors,\nsuch as the requirement for a tetravalent vaccine capable of prov iding protection against\nall four dengue virus (DENV) serotypes, the absence of suitable a nimal models for test-\ning, and concerns surrounding the potential immune enhancement caused by the vaccine,\nsimilar to what occurs during natural infection [ 28]. Despite the substantial global de-\nmand, these obstacles have hindered the progress of dengue vac cine development. There\nis also no ready-to-use medicine for the disease. Therefore, it is of utmost importance to\nget some idea about future trends of dengue cases in the populatio n.\nThe eﬀectiveness of preventive measures against dengue fever is greatly enhanced by\nthe presence of a precise early warning system that can predict up coming epidemics. It\nhas been established that early detection of cases and treating th em can signiﬁcantly\nreduce fatal complications [ 9]. Early warning systems or forecasting models can inform\nthe expected number of dengue cases over the coming months. Th is information can\nthen be utilized to allocate resources to high-risk zones and awaren ess campaigns can be\nperformed to ﬂatten the expected dengue incidence curve [ 23;29]. Thus, public health au-\nthorities rely on model predictions for optimal management of futu re dengue cases. Due\nto the high importance of accurate forecasts of future dengue c ases, many researchers\nhave attempted this problem with diﬀerent levels of success [ 10;3;17]. However, there is\na diverse range of models that are used for dengue prediction prob lems, namely, compart-\nmental SIR-type models [ 26], statistical time series models [ 16], machine learning models\n[11] and ensemble models [ 32;8]. Researchers have seen that statistical and mac"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": " Multimodal Machine Learning Combining Facial Images and Clinical Texts Improves Diagnosis of Rare Genetic Diseases  Da Wu1, Jingye Yang1, Steven Klein2,3, Cong Liu4, Tzung-Chien Hsieh5, Peter Krawitz5, Chunhua Weng4, Gholson J. Lyon6,7, Jennifer M. Kalish2,3,8, Kai Wang1,9*  1 Raymond G. Perelman Center for Cellular and Molecular Therapeutics, Children's Hospital of Philadelphia, Philadelphia, PA 19104, USA 2 Division of Human Genetics, Children's Hospital of Philadelphia, Philadelphia, PA 19104, USA 3 Department of Pediatrics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States 4 Department of Biomedical Informatics, Columbia University Irving Medical Center, New York, NY 10032, USA 5 Institute for Genomic Statistics and Bioinformatics, University Hospital Bonn, Rheinische Friedrich-Wilhelms-Universität Bonn, Bonn, Germany 6 Department of Human Genetics, New York State Institute for Basic Research in Developmental Disabilities, Staten Island, NY, USA 7 Biology PhD Program, The Graduate Center, The City University of New York, New York, United States of America 8 Department of Genetics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States 9 Department of Pathology and Laboratory Medicine, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104, USA   *: correspondence should be addressed to wangk@chop.edu.   ABSTRACT Individuals with suspected rare genetic disorders often undergo multiple clinical evaluations, imaging studies, laboratory tests and genetic tests, to find a possible answer over a prolonged period of multiple years. Addressing this “diagnostic odyssey” thus have substantial clinical, psychosocial, and economic benefits. Many rare genetic diseases have distinctive facial features, which can be used by artificial intelligence algorithms to facilitate clinical diagnosis, in prioritizing candidate diseases to be further examined by lab tests or genetic assays, or in helping the phenotype-driven reinterpretation of genome/exome sequencing data. However, existing methods using frontal facial photo were built on conventional Convolutional Neural Networks (CNNs), rely exclusively on facial images, and cannot capture non-facial phenotypic traits and demographic information essential for guiding accurate diagnoses. Here we introduce GestaltMML, a multimodal machine learning (MML) approach solely based on the Transformer architecture. It integrates the facial images, demographic information (age, sex, ethnicity), and clinical notes (optionally, a list of Human Phenotype Ontology terms) of patients to improve prediction accuracy. Furthermore, we also introduce GestaltGPT, a GPT-based methodology with few-short learning capacities that exclusively harnesses textual inputs using a range of large language models (LLMs) including Llama 2, GPT-J and Falcon. We evaluated these methods on a diverse range of datasets, including 449 diseases from the GestaltMatcher Database, several in-house datasets on Beckwith-Wiedemann syndrome (BWS, over-growth syndrome with distinct facial features), Sotos syndrome (overgrowth syndrome with overlapping features to BWS), NAA10-related syndrome (neurodevelopmental syndrome) and others. Our results suggest that GestaltMML/GestaltGPT effectively incorporate multiple modalities of data, greatly narrow down candidate genetic diagnosis of rare diseases, and may facilitate the reinterpretation of genome/exome sequencing data.   Keywords:  Multimodal Machine Learning, Artificial Intelligence, Large Language Models, Human Phenotype Ontology, Rare Genetic Disorders, Facial phenotyping   INTRODUCTION Currently, a substantial proportion of the global population, more than 6%, is affected by rare genetic disorders1. While collectively common, rare diseases are individually rare2: they are typically defined as affecting fewer than 200,000 people in the USA or less than one in 2,000 of the general population in Europe3. Based on the latest Orphanet4 and OMIM5 database, currently there are at least 7000 rare diseases that are identified. Due to the inherent rarity and extensive phenotypic heterogeneity of rare genetic disorders, accurately pinpointing a genetic diagnosis presents a formidable and time-intensive challenge, often referred to as “diagnostic odyssey”6-8. Patients with suspected genetic syndromes often need to undergo multiple clinical evaluations, imaging studies, and laboratory tests, in addition to different modalities of genetic tests, including gene panel, exome sequencing or whole-genome sequencing, to find a possible answer over a prolonged period of time. Clinicians often encounter difficulties for making decisions on what diagnostic modalities to use for fast and accurate diagnosis, as they must navigate a vast array of clinical conditions. Thus, shortening or ending the odyssey could have significant clinical, psychosocial, and economic benefits8,9.  Many genetic diseases ha"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "Interdependent Total Factor Productivity in an\nInput-Output model\nThomas M. Bombarde1,2and Andrew L. Krause1*\n1*Department of Mathematical Sciences, Durham University, Upper\nMountjoy Campus, Stockton Road, Durham, DH1 3LE, United Kingdom.\n2St Hugh’s College, University of Oxford, St Margaret’s Rd, Oxford,\nOX2 6LE, United Kingdom.\n*Corresponding author(s). E-mail(s): andrew.krause@durham.ac.uk;\nContributing authors: thomas.bombarde@st-hughs.ox.ac.uk;\nAbstract\nIndustries learn productivity improvements from their suppliers. The observed\nempirical importance of these interactions, often omitted by input-output mod-\nels, mandates larger attention. This article embeds interdependent total factor\nproductivity (TFP) growth into a general non-parametric input-output model.\nTFP growth is assumed to be Cobb-Douglas in TFP-stocks of adjacent sectors,\nwhere elasticities are the input-output coefficients. Studying how the steady state\nof the system reacts to changes in research effort bears insight for policy and\nthe input-output literature. First, industries higher in the supply chain see a\ngreater multiplication of their productivity gains. Second, the presence of ‘lag-\ngard’ industries can bottleneck the the rest of the economy. By deriving these\ninsights formally, we review a canonical method for aggregating TFP – Hulten’s\nTheorem – and show the potential importance of backward linkages.\nKeywords: Input-output modelling, total factor productivity\n1 Introduction\nIn the long-run, economies grow by increasing total factor productivity (TFP). By\nchanging the nature of industries and the interrelations between them, economies tap\ninto new opportunities for growth. We motivate this paper by discussing the questions\nin economic development that input-output (IO) models help answer, the recent renew\n1arXiv:2312.15362v1  [econ.TH]  23 Dec 2023of interest in IO economics, and the observations of TFP-growth interacting with IO\nlinkages.\nEarly development economics argued for changes in the nature of industries as\na precondition for take-off (Monga and Lin, 2019). The challenge is to qualify this\nnature. A neat distinction between agriculture, manufacturing, and services as in early\nmodels (Kuznets, 1955) is less pertinent to modern economies, where high-skill ser-\nvice industries may play leading sector roles previously attributed to manufacturing,\nand informal manufacturing may serve as reserves of surplus-labour that were previ-\nously attributed to agriculture (Lamba and Subramanian, 2020; Mensah et al., 2022).\nAt the same time, premature deindustrialisation of developing economies calls for a\nfiner understanding of the interaction between industrial specialisations and economic\ngrowth (Rodrik, 2016; Hausmann and Rodrik, 2003; Lin, 2013). Beyond what indus-\ntries do, defining how important an industry is to the economy at a state in time\nshould consider its place in the supply chain.\nInput-Output (IO) economics provides quantitative measures on an industry’s\neconomic importance. It assumes that an economy can be divided into industries\ncharacterised by distinct output quantities. ‘Technical coefficients’ aij, the quantity\nof input from the jth industry required per unit of output of the ith industry, are\nstored in an adjacency matrix that describes a network of industries (Ten Raa, 2006).\nThe output-multiplier is a centrality metric on this network for the position of an\nindustry in the supply chain (McNerney et al., 2022). Hulten’s Theorem (Hulten,\n1978) abstracts from these dependencies to aggregate TFP growth rate as the sum\nof industry-level TFP growth rates weighted by the their ‘Domar weights’, the ratio\nof industry sales to final consumption. If ˆgdpis the growth rate of final consumption\n(henceforth ‘economic growth’), θithe Domar weight of an industry i= 1, ..., n , and\nγiits growth rate of TFP, then we formulate Hulten’s result as\n∂ˆgdp\n∂γi=θi. (1)\nIn this view, the Domar weights of industries are sufficient to understanding the impact\nof industry-level TFP-shocks on economic growth. In short, eq. (1) suggests that IO\ndependencies are irrelevant to development policy.\nRecent literature explores the role of IO dependencies in propagating economic\nshocks. Gabaix (2011) and Acemoglu et al. (2012) use Hulten’s Theorem to show how\na fat-tailed distribution of Domar weights exposes the economy to shocks cascading\nfrom central industries. However, as commented by Baqaee and Farhi (2019), Hulten’s\nTheorem is strikingly unintuitive, as it suggests that deleting the supermarket chain\nWalmart from the US economy would have the same effect as deleting all electric-\nity providers, their Domar weights being roughly equal. Most challenges to Hulten’s\nTheorem focus on second-order effects the theorem omits. Acemoglu et al. (2017)\nmodel a fat-tailed distribution of shocks, second order effects, and Domar weights\nthat trigger economic collapse. Baqaee and Farhi (2019) assess susceptibility to eco-\nnomic disaster through second-order"}
