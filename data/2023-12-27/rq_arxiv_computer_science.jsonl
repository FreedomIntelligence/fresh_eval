{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "SoundCount: Sound Counting from Raw Audio with\nDyadic Decomposition Neural Network\nYuhang He1, Zhuangzhuang Dai2, Long Chen3,4*, Niki Trigoni1, Andrew Markham1\n1Department of Computer Science, University of Oxford, UK. yuhang.he@cs.ox.ac.uk\n2Department of Applied AI and Robotics, Aston University. UK\n3Institute of Automation, Chinese Academy of Sciences, China.\n4WAYTOUS Ltd., China.\nAbstract\nIn this paper, we study an underexplored, yet important and\nchallenging problem: counting the number of distinct sounds\nin raw audio characterized by a high degree of polyphonic-\nity. We do so by systematically proposing a novel end-to-end\ntrainable neural network (which we call DyDecNet, consist-\ning of a dyadic decomposition front-end and backbone net-\nwork), and quantifying the difficulty level of counting de-\npending on sound polyphonicity. The dyadic decomposition\nfront-end progressively decomposes the raw waveform dyad-\nically along the frequency axis to obtain time-frequency rep-\nresentation in multi-stage, coarse-to-fine manner. Each in-\ntermediate waveform convolved by a parent filter is further\nprocessed by a pair of child filters that evenly split the par-\nent filter’s carried frequency response, with the higher-half\nchild filter encoding the detail and lower-half child filter\nencoding the approximation . We further introduce an en-\nergy gain normalization to normalize sound loudness vari-\nance and spectrum overlap, and apply it to each intermedi-\nate parent waveform before feeding it to the two child fil-\nters. To better quantify sound counting difficulty level, we\nfurther design three polyphony-aware metrics: polyphony ra-\ntio,max polyphony andmean polyphony . We test DyDecNet\non various datasets to show its superiority, and we further\nshow dyadic decomposition network can be used as a general\nfront-end to tackle other acoustic tasks. Code: github.com/\nyuhanghe01/SoundCount.\nIntroduction\nSuppose you went to the seaside and heard a cacophony of\nseagulls, squawking and squabbling. An interesting question\nthat naturally arises is whether you can tell the number of\nseagulls flocking around you from the sound you heard? Al-\nthough a trivial example, this sound “crowd counting” prob-\nlem has a number of important applications. For example,\npassive acoustic monitoring is widely used to record sounds\nin natural habitats, which provides measures of ecosystem\ndiversity and density (Aguiar et al. 2021; Dohi et al. 2021;\nChronister et al. 2021). Sound counting helps to quantify\nand map sound pollution by counting the number of individ-\nual polluting events (Bello, Mydlarz, and Salamon 2018). It\ncan also be used in music content analysis (J. Humphrey,\n*corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.Durand, and McFee 2018). Despite its importance, research\non sound counting has far lagged behind than its well-\nestablished crowd counting counterparts from either im-\nages (Zhang et al. 2016; Wang et al. 2019), video (Li, Zhang,\nand Chen 2018) or joint audio-visual (Hu et al. 2020).\nWe conjecture the lack of exploration stems from three\nmain factors. First, sound counting has long been treated\nas an over-solved problem by sound event detection (SED)\nmethods (Mesaros et al. 2021; Cakir et al. 2017; Adavanne,\nPertil ¨a, and Virtanen 2017; He, Trigoni, and Markham\n2021), in which SED goes further to identify each sound\nevent’s ( e.g.a bird call) start time, end time and semantic\nidentity. Sound counting number then becomes easily ac-\ncessible by simply adding up all detected events. Secondly,\ncurrent SED only tags whether a class of sound event is\npresent within a window, regardless of the number of con-\ncurrent sound sources of the same class like a series of baby\ncrying or multiple bird calls (Phan et al. 2022). Thirdly, la-\nbelling acoustic data is technically-harder and more time-\nconsuming than labelling images, due to the overlap of con-\ncurrent and diverse sources. The lack of well-labelled sound\ndata in crowded sound scenes naturally hampers research\nprogress. Existing SED sound datasets (Adavanne, Pertil ¨a,\nand Virtanen 2017; Heittola et al. 2010) capture simple\nacoustic scenarios with low polyphony and where the event\nvariance is small. The simplified acoustic scenario in turn\nmakes sound counting task by SED methods tackleable. But\nwhen the sound scene becomes more complex with highly\nconcurrent sound events, SED methods soon lose their ca-\npability in discriminating different sound events (Pankajak-\nshan, Bear, and Benetos 2019; Cakir et al. 2017). In the\nmeantime, some researchers think sound counting is equiv-\nalent to sound source separation task (Neumann et al. 2020;\nTurpault et al. 2021; Tzinis, Wang, and Smaragdis 2022;\nSubakan et al. 2022; Tzinis et al. 2020), in which the sound\nis counted as the source number by isolating individual\nsound from sound mixture and assigning it to correspond-\ning sound source. However, our "}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "Large-scale Long-tailed Disease Diagnosis on Radiology Images\nQiaoyu Zheng∗,1,2, Weike Zhao∗,1,2, Chaoyi Wu∗,1,2, Xiaoman Zhang1,2,\nYa Zhang1,2, Yanfeng Wang1,2,†and Weidi Xie1,2,†\n1Shanghai Jiao Tong University2Shanghai AI Laboratory\nhttps://qiaoyu-zheng.github.io/RP3D-Diag\nIn this study, we aim to investigate the problem of large-scale, large-vocabulary disease classification\nfor radiologic images, which can be formulated as a multi-modal, multi-anatomy, multi-label, long-tailed\nclassification. Our main contributions are three folds: ( i), on dataset construction, we build up an\nacademically accessible, large-scale diagnostic dataset that encompasses 5568 disorders linked with 930\nunique ICD-10-CM codes, containing 39,026 cases (192,675 scans). ( ii), on model design, we present a\nnovel architecture that enables to process arbitrary number of input scans, from various imaging modalities,\nwhich is trained with knowledge enhancement to leverage the rich domain knowledge; ( iii), on evaluation,\nwe initialize a new benchmark for multi-modal multi-anatomy long-tailed diagnosis. Our method shows\nsuperior results on it. Additionally, our final model serves as a pre-trained model, and can be finetuned to\nbenefit diagnosis on various external datasets.\n1 Introduction\nIn the ever-evolving landscape of clinical medicine, the advent of radiology techniques, such as X-ray, CT,\nMRI, and ultrasound, has truly revolutionized the medical field, offering a non-invasive yet deeply revealing\nperspective of the human body for disease diagnosis and management. These imaging techniques are now at\nthe cusp of a new era with the integration of artificial intelligence (AI).\nRecent literature highlights the significant potential of developing diagnostic models in the medical field, the\ndevelopments can be generally cast into two categories: one is a specialist, which has already shown success in\nidentifying and managing a wide array of diseases [ 15,77,85]. However, a notable limitation of these models\nis their specialization, as they often focus on a narrow range of disease categories, targeting limited anatomical\nregions, based on specific imaging modalities. This specialization restricts their ability to fully address the\ndiverse and complex cases encountered in real-world clinical settings.\nWhile on the other extreme, there is an emerging trend towards developing Generalist Medical Artificial\nIntelligence (GMAI) models [ 55,80,84]. Taking inspiration from the breakthroughs in natural language\nprocessing and computer vision, these models aim to amalgamate data from diverse sources, including various\nimaging methods, patient histories, and current medical research, to offer more comprehensive diagnostic and\ntreatment solutions. Nonetheless, the development of GMAI models faces formidable challenges, such as the\nneed for substantial computational power, meticulously curated multimodal datasets covering an extensive\nrange of medical conditions and patient demographics, and advanced models equipped to tackle unique medical\nintricacies, like extremely unbalanced data distribution and the need for domain-specific expertise.\nIn this paper, we consider the problem of large-scale, large-vocabulary disease classification for radiologic\nimages, marking a transition phase between specialist and generalist models. Specifically, compared to\nexisting specialists, we aim to initiate the research for developing a computational model that can handle\nmulti-modal, multi-anatomy, and multi-label disease diagnosis, in the face of extremely unbalanced\ndistribution, embracing a wider scope of diagnoses across various anatomical regions and imaging modalities.\nIn contrast to generalist models, our investigation offers a more feasible and targeted playground for exploring\nsophisticated algorithms in academic labs, offering opportunities for detailed error analysis, which is often\nimpractical in the development of large-scale generalist models due to prohibitive computational costs. Overall,\nwe make contributions from three aspects, namely, a large-scale open dataset and its construction pipeline,\npreliminary model architecture exploration, and an evaluation benchmark.\nOn dataset construction, we build up an academically accessible, large-scale diagnostic dataset derived from\nRadiopaedia [ 5]. Each sample is associated to the International Classification of Diseases, i.e., ICD-10-CM [ 1],\n∗Equal contributions.\n†Corresponding author. Email addresses: {three-world, zwk0629, wtzxxxwcy02, weidi}@sjtu.edu.cnarXiv:2312.16151v1  [cs.CV]  26 Dec 2023Figure 1 |Overview of RP3D-DiagDS. There are 39,026 cases (192,675 scans) across 7 human anatomy regions and 9\ndiverse modalities covering 930 ICD-10-CM codes .\nindicating the diagnostic category, for example, ‘S86’ refers to ‘Injury of muscle, fascia and tendon at lower\nleg level’. The dataset naturally displays an unbalanced distribution, varying from 1 to 964 cases in each\ndisease category. Additionally, each case "}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "The Clustered Orienteering Problem with Subgroups\nLuciano E. Almeida Douglas G. Macharet\nAbstract — This paper introduces an extension to the Orien-\nteering Problem (OP), called Clustered Orienteering Problem\nwith Subgroups (COPS). In this variant, nodes are arranged\ninto subgroups, and the subgroups are organized into clusters.\nA reward is associated with each subgroup and is gained only\nif all of its nodes are visited; however, at most one subgroup\ncan be visited per cluster. The objective is to maximize the\ntotal collected reward while attaining a travel budget. We show\nthat our new formulation has the ability to model and solve\ntwo previous well-known variants, the Clustered Orienteering\nProblem (COP) and the Set Orienteering Problem (SOP),\nin addition to other scenarios introduced here. An Integer\nLinear Programming (ILP) formulation and a Tabu Search-\nbased heuristic are proposed to solve the problem. Experimental\nresults indicate that the ILP method can yield optimal solutions\nat the cost of time, whereas the metaheuristic produces compa-\nrable solutions within a more reasonable computational cost.\nI. I NTRODUCTION\nVehicle routing problems are essential in different re-\nsearch fields, such as Operations Research and Robotics.\nThe Traveling Salesman Problem (TSP), in particular, is a\nwell-known combinatorial optimization problem that aims to\nfind the shortest route that visits a given set of cities exactly\nonce and returns to the starting city. Despite being an NP-\nhard problem, the TSP has numerous real-world applications,\nincluding logistics, transportation, and VLSI design.\nAn important generalization of the TSP is the Orienteering\nProblem (OP) [1], which can be defined as a combination\nof the TSP and a Knapsack Problem (KP). In the OP, the\ngoal is to determine a route that maximizes a total reward\nobtained when visiting the cities, with the constraint that it\nshould not exceed a given travel budget ( e.g., length or time).\nThe OP also has several variants, and we are interested\nin two relatively recent ones, the Clustered Orienteering\nProblem (COP) and the Set Orienteering Problem (SOP).\nThe COP [2] considers that the cities are divided into\nclusters. A reward is associated with each cluster, not the\ncities individually as in the OP, and is collected only if all\ncities belonging to the cluster are visited. The SOP [3] also\nconsiders that the cities are grouped into clusters; however,\nthe reward is collected only if at least one city is visited.\nIn this paper, we propose a novel generalization related to\nboth of these variants, called Clustered Orienteering Problem\nwith Subgroups (COPS). In the COPS, the target locations\nare also organized into clusters, however, inside a cluster,\nThis work was supported by CAPES/Brazil - Finance Code 001,\nCNPq/Brazil, and FAPEMIG/Brazil.\nThe authors are with the Computer Vision and Robotics Lab-\noratory (VeRLab), Department of Computer Science, Universidade\nFederal de Minas Gerais, Brazil. E-mails: {luciano.almeida,\ndoug }@dcc.ufmg.br .they are arranged into one or more subgroups, and a reward\nis assigned to each one of these subgroups. The objective is\nto maximize the total reward, where a reward is collected\nifat most one subgroup is selected and allof its internal\nlocations are visited. Fig. 1 illustrates the problem and shows\na possible solution (blue path).\nFig. 1. Example of seven clusters with distinct subgroups. The subgroups’\nassociated reward is represented by their colors. The cluster with a single\nvertex is the start/end point, and the blue line is a possible route.\nTherefore, in contrast to both COP and SOP, a cluster\ncan now provide distinct rewards accordingly to the visited\nsubgroup. This formulation is useful in various scenarios,\nwhere one can use the clusters to model tasks of a more\ngeneral mission, while the subgroups define specific ways\nof executing a task. For example, given a 3D reconstruction\nproblem, a cluster can group all points representing different\nviewing angles of an object and a subgroup a set of views\nwith certain properties. In this context, a cluster refers to\ndifferent possible ways of reconstructing the same object.\nIn summary, our main contributions are:\n•Concept of a novel variant that allows to model different\nsubgroups in clustered-related OP instances;\n•Unified formulation for solving both COP and SOP;\n•Proposal of an exact ILP method;\n•Design of an efficient Tabu Search-based heuristic.\nII. R ELATED WORK\nThe Orienteering Problem is a variant of the Traveling\nSalesman Problem that focuses on identifying the most\nrewarding subset of locations to visit with limited resources.\nThis optimization problem has garnered significant attention\ndue to its relevance in various fields, such as logistics,\ntransportation, and tourism.\nThe classic OP formulation has evolved to encompass a\nrange of generalizations, each tailored to specific characteris-\ntics and applications. For example, vehicles with motion con-\nstraints, as in the Du"}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "From Text to Multimodal: A Comprehensive\nSurvey of Adversarial Example Generation in\nQuestion Answering Systems\nGulsum Yigit1,2*†and Mehmet Fatih Amasyali2†\n1*Department of Computer Engineering, Kadir Has University, Istanbul,\nTurkey.\n2Department of Computer Engineering, Yildiz Technical University,\nIstanbul, Turkey.\n*Corresponding author(s). E-mail(s): gulsum.yigit@khas.edu.tr;\nContributing authors: amasyali@yildiz.edu.tr;\n†These authors contributed equally to this work.\nAbstract\nIntegrating adversarial machine learning with Question Answering (QA) systems\nhas emerged as a critical area for understanding the vulnerabilities and robust-\nness of these systems. This article aims to comprehensively review adversarial\nexample-generation techniques in the QA field, including textual and multimodal\ncontexts. We examine the techniques employed through systematic categoriza-\ntion, providing a comprehensive, structured review. Beginning with an overview\nof traditional QA models, we traverse the adversarial example generation by\nexploring rule-based perturbations and advanced generative models. We then\nextend our research to include multimodal QA systems, analyze them across var-\nious methods, and examine generative models, seq2seq architectures, and hybrid\nmethodologies. Our research grows to different defense strategies, adversarial\ndatasets, and evaluation metrics and illustrates the comprehensive literature on\nadversarial QA. Finally, the paper considers the future landscape of adversarial\nquestion generation, highlighting potential research directions that can advance\ntextual and multimodal QA systems in the context of adversarial challenges.\nKeywords: question answering, adversarial question generation, visual question\ngeneration, adversarial datasets, adversarial evaluation metrics\n1arXiv:2312.16156v1  [cs.CL]  26 Dec 20231 Introduction\nIn recent years, natural language processing (NLP) has experienced a remarkable\ntransformation through revolutions in deep learning architectures and the availability\nof vast amounts of textual data. These advancements have led to the development of\nsophisticated question-answering (QA) systems that aim to bridge the gap between\nhuman language and machine understanding. State-of-the-art QA models, such as\ntransformer-based architectures like BERT (Bidirectional Encoder Representations\nfrom Transformers) and its variants, have demonstrated remarkable performance\nimprovements. These models utilize pre-trained language models to capture complex\nlinguistic patterns and contextual dependencies, enabling them to generate accurate\nand relevant answers to user questions. Moreover, integrating multimodal data, where\ntextual, visual, or audio information joins, has also raised the capabilities of QA sys-\ntems. Such multimodal QA models, leveraging textual, visual, or audio signals, have\nshown promising results in analyzing and generating questions. However, a vital vul-\nnerability has arisen in the form of adversarial examples among their outstanding\nimprovements.\nInitially presented in computer vision, adversarial examples are meticulously for-\nmulated inputs created to fool machine learning models [1]. These inputs are modified\nin malicious forms, almost invisible to humans but are influential in yielding AI systems\nto develop inaccurate outcomes. Recent research has brought to light the utilization of\nadversarial examples in various scenarios. To illustrate, adversaries can simulate phys-\nical adversarial examples, thereby confusing autonomous vehicles by manipulating the\nappearance of a stop sign within a traffic sign recognition system [2, 3]. Additionally,\nmalicious actors show expertise in generating adversarial commands aimed at sab-\notaging automatic speech recognition systems and voice-controllable systems [4, 5],\nincluding crucial platforms such as Apple Siri, Amazon Alexa, etc.\nThe influence of adversarial examples quickly attracted the researchers’ attention,\ndefense mechanisms started to develop, and researchers investigated the underlying\ncauses of the vulnerabilities. With the increased interest in QA systems, the research\nhas shifted to innovative approaches, such as adversarial question generation models,\nto enhance the quality, diversity, and effectiveness of question generation across textual\nand multimodal contexts.\nAdversarial examples in QA systems are maliciously formulated inputs created to\nfool the model and generate inaccurate answers. These adversarially designed inputs\nare constructed by applying subtle transformations to the original question, usually\nin a form that is invisible to humans but drastically modifies the model’s generated\nanswer. These adversarial perturbations could vary from word substitutions to more\ncomplicated linguistic structures aimed at making inaccurate or biased texts, images,\nor videos, challenging the robustness of textual and multimodal QA systems.\nThe impact of adversarial examples on QA systems performance can be critical.\nThey p"}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "1 \n Association rule mining  with earthquake data collected \nfrom T ürkiye  region  \n \nBaha ALTURAN1, İlker  TÜRKER2* \n1 Cankiri Karatekin  University, Engineering Faculty, Department of Computer Engineering, Cankiri, Türkiye  \n2 Karabuk University, Engineering Faculty, Department of Computer Engineering, Karabuk,  Türkiye.  \niturker@karabuk.edu.tr  \n \nABSTRACT  \nEarthquakes are evaluated among the most destructive disasters for human beings , as also \nexperienced for Türkiye region. Data science has the property of discovering hidden patterns in \ncase a sufficient volume of data is supplied. Time dependency of events, specifically being defined \nby co- occurrence in a specific time window, may be  handled as an associate rule mining task such \nas a market -basket analysis application. In this regard, we assumed each day’s seismic activity as \na single basket of events, leading to discovering the association patterns between these events. \nConsequently, this study presents the most prominent association rules for the earthquakes \nrecorded in Türkiye region in the last 5 years, each year presented separately. Results indicate \nstatistical inference with events recorded from regions of various distances, which could be further \nverified with geologic evidence from the field. As a result, we believe that the current study may \nform a statistical basis for the future works w ith the aid of machine learning algorithm performed \nfor associate rule mining.  \n \nKeywords:  Time series analysis , associate rule mining , machine learning , earthquake dynamics, \ncollective behavior . \n \n1. Introduction  \nEarthquakes, as natural phenomena, pose significant challenges to communities, infrastructures, \nand disaster management systems worldwide  [1]. As our understanding of seismic activities \ncontinues to evolve, leveraging advanced technologies becomes imperative for effective \nearthquake prediction, monitoring, and mitigation [2]. One such technological frontier is the \napplication of data mining techniques, specifically the powerful tool of Association Rule Mining, \nto seismic data analysis  [3]. \nIn recent years, the surge in data generated by seismic sensors, satellite imagery, and geological \nsurveys has opened new avenues for researchers to extract valuable insights into earthquake \npatterns and behaviors. Associate Rule Mining, a data mining appr oach initially developed for 2 \n market basket analysis, has found intriguing applications in uncovering hidden patterns and \nrelationships within complex datasets, including those related to seismic activities  [4]. \nSeismic data collection is a fundamental process in geophysics, integral to the study of the Earth's \nsubsurface structure and composition. The primary method employed for seismic data acquisition \ninvolves the controlled generation of seismic waves at the E arth's surface and the subsequent \nrecording of their reflections and refractions by subsurface geological formations. This is typically \nachieved through the use of seismic sources, such as explosives or specialized vibrators, which \nemit acoustic waves that  penetrate the Earth [ 5, 6]. These waves interact with the subsurface layers, \nproducing reflections that are detected by an array of sensitive receivers known as geophones or \nhydrophones strategically placed across the survey area  [7]. The recorded data, in the form of \nseismic traces, are then meticulously processed to enhance signal quality, reduce noise, and \ntransform the information into a coherent image of the subsurface  [8]. Processing techniques \ninvolve various steps, including time -to-depth conversion, filtering, migration, and velocity \nanalysis. Advanced computational algorithms are applied to correct for complex geological \nfeatures, ensuring a more accurate representatio n of subsurface structures. The culmination of these \nefforts yields detailed seismic images that provide valuable insights into the Earth's interior, aiding \nin the exploration and characterization of oil and gas reservoirs, geological formations, and other  \nsubsurface features  [9, 10 ]. \nThe current study delves into the exploration of Associate Rule Mining as a promising \nmethodology for earthquake data analysis. By examining patterns of co- occurrence and \ndependencies among various seismic parameters, we aim to unveil hidden associations that can \ncontribut e to a deeper comprehension of earthquake dynamics. Such insights have the potential to \nenhance early warning systems, risk assessments, and overall preparedness for seismic events  [11]. \nIn this exploration, we will discuss the foundational principles of Associate Rule Mining and its \nadaptation to earthquake data. Through the data collected and served by the Kandilli Rasathanesi \n(Kandilli Observatory and Earthquake Research Institute -  KOERI) located in Turkey, we unveil \nmeaningful patterns, providing researchers and seismologists with valuable information to better \nunderstand the intricacies of"}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "Zero-Shot Cross-Lingual Reranking with Large Language\nModels for Low-Resource Languages\nMofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep, Jimmy Lin\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n{moadeyem, aooladipo, rpradeep, jimmylin}@uwaterloo.ca\nAbstract\nLarge language models (LLMs) have shown im-\npressive zero-shot capabilities in various doc-\nument reranking tasks. Despite their success-\nful implementations, there is still a gap in ex-\nisting literature on their effectiveness in low-\nresource languages. To address this gap, we\ninvestigate how LLMs function as rerankers in\ncross-lingual information retrieval (CLIR) sys-\ntems for African languages. Our implementa-\ntion covers English and four African languages\n(Hausa, Somali, Swahili, and Yoruba) and we\nexamine cross-lingual reranking with queries in\nEnglish and passages in the African languages.\nAdditionally, we analyze and compare the effec-\ntiveness of monolingual reranking using both\nquery and document translations. We also eval-\nuate the effectiveness of LLMs when leverag-\ning their own generated translations. To get a\ngrasp of the effectiveness of multiple LLMs,\nour study focuses on the proprietary mod-\nels RankGPT 4and RankGPT 3.5, along with\nthe open-source model, RankZephyr. While\nreranking remains most effective in English,\nour results reveal that cross-lingual reranking\nmay be competitive with reranking in African\nlanguages depending on the multilingual capa-\nbility of the LLM.\n1 Introduction\nSeveral works have demonstrated the effective-\nness of large language models (LLMs) across NLP\ntasks (Zhou et al., 2022; Zhu et al., 2023; Wang\net al., 2023). For text ranking, researchers have ex-\nplored the effectiveness of LLMs as retrievers (Ma\net al., 2023a), and as pointwise or listwise rerankers.\nReranking is cast as text generation so that the\nmodels either generate an ordered list (Sun et al.,\n2023; Pradeep et al., 2023a; Ma et al., 2023b) or\nthe ordered list is created by sorting the token prob-\nabilities generated (Ma et al., 2023b). The large\ncontext size of LLMs makes listwise approaches\nparticularly attractive because the model attendsto multiple documents and produces a relative or-\ndering. Ma et al. (2023b) outperforms zero-shot\npointwise approach on three TREC web search\ndatasets using a listwise approach. Further, their\nwork showed the potential that listwise reranking\nby LLMs generalizes across different languages.\nIn this study, we examine the effectiveness of\nproprietary and open-source models for listwise\nreranking in low-resource African languages. Our\ninvestigation is guided by the following research\nquestions:\n•How well do LLMs perform as listwise rerankers\nfor low-resource languages?\n•How effectively do LLMs perform listwise\nreranking in cross-lingual scenarios compared to\nmonolingual (English or low-resource language)\nscenarios?\n•When we leverage translation, is reranking more\neffective when translation is performed using the\nsame LLM used for zero-shot reranking?\nThis study aims to answer these questions\nthrough an extensive investigation of the effec-\ntiveness of RankGPT (Sun et al., 2023) and\nRankZephyr (Pradeep et al., 2023b) in cross-\nlingual and monolingual retrieval settings. We use\nCIRAL (Adeyemi et al., 2023), a cross-lingual\ninformation retrieval dataset covering four ( 4)\nAfrican languages, and construct monolingual re-\ntrieval scenarios through either document or query\ntranslation. The cross-lingual scenarios entail\nsearching with English queries and retrieving pas-\nsages in the African languages.\nOur results show that cross-lingual reranking\nusing these models is consistently more effective\nthan reranking in low-resource languages, under-\nscoring the fact that these LLMs are better tuned\nto English than low-resource languages. Across\nall languages, we achieve our best results whenarXiv:2312.16159v1  [cs.IR]  26 Dec 2023reranking entirely in English language using re-\ntrieval results obtained by document translation. In\nthis setting, we see up to 7 points improvement\nin nDCG@20 over cross-lingual reranking using\nRankGPT 4, and up to 9 points over reranking in\nAfrican languages. When reranking in African\nlanguages, we gain improvements for RankGPT 4\nwhen we perform query translation using GPT-4\nitself. However, for RankGPT 3.5, we see no signifi-\ncant difference in reranking effectiveness when we\ntranslate queries using GPT-3.5.\n2 Background and Related Work\nGiven a corpus C={D1, D2, ..., D n}and a query\nq, information retrieval (IR) systems aim to re-\nturn the kmost relevant documents. Modern IR\npipelines typically feature multi-stage architecture\nin which a first-stage retriever returns a list of can-\ndidate documents which a reranker reorders for\nimproved quality (Asadi and Lin, 2013; Nogueira\net al., 2019; Zhuang et al., 2023). While earlier\nwork relied on sparse models such as TF-IDF or\nBM25 (Robertson and Zaragoza, 2009) as first-\nstage retrievers, the improved dense representations\nof "}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "1\nAge of Information in Gossip Networks:\nA Friendly Introduction and Literature Survey\nPriyanka Kaswan Purbesh Mitra Arunabh Srivastava Sennur Ulukus\nAbstract — Gossiping is a communication mechanism, used\nforfast information dissemination in a network, where each\nnode of the network randomly shares its information with the\nneighboring nodes. To characterize the notion of fastness in\nthe context of gossip networks, age of information (AoI) is\nused as a timeliness metric. In this article, we summarize the\nrecent works related to timely gossiping in a network. We start\nwith the introduction of randomized gossip algorithms as an\nepidemic algorithm for database maintenance, and how the\ngossiping literature was later developed in the context of rumor\nspreading, message passing and distributed mean estimation.\nThen, we motivate the need for timely gossiping in applications\nsuch as source tracking and decentralized learning. We evaluate\ntimeliness scaling of gossiping in various network topologies, such\nas, fully connected, ring, grid, generalized ring, hierarchical, and\nsparse asymmetric networks. We discuss age-aware gossiping and\nthe higher order moments of the age process. We also consider\ndifferent variations of gossiping in networks, such as, file slicing\nand network coding, reliable and unreliable sources, information\nmutation, different adversarial actions in gossiping, and energy\nharvesting sensors. Finally, we conclude this article with a few\nopen problems and future directions in timely gossiping.\nI. I NTRODUCTION\nIn this review article, we discuss goal-oriented applications\nof gossip networks for time-sensitive information. An example\nof such applications is an autonomous driving system, where\ntimely communication with nearby connected devices, such\nas other cars in the vicinity, sensors, infrastructure and even\nsmartphones, is crucial to accurately perform driving actions\nand avoid accidents. A parallel example is a smart factory\nenvironment with human, robot, drone, camera collaborative\nsystem to safely and effectively perform manufacturing; see\nFig. 1. Another application is remote surgery, where a doctor\nperforms surgery on a patient using a remote surgical system,\neven though they are not physically present in the same\nlocation. The absence of real-time surgical data has to be taken\ninto consideration to minimize the chance of inaccuracies in\nthe surgical procedure. In such systems, there is a source,\nwhich has some time-varying information that is of interest\nto a user or a group of users. A user would like to track the\ntime-varying information at the source as closely as possible\nin real-time to achieve its goal. However, there are network\nlimitations, such as processing delays in the buffer queue, that\nprevent the user from tracking the source arbitrarily closely,\neven with the high data rates afforded in the state-of-the-art\ncommunication systems.\nThe authors are with the Department of Electrical and Computer Engineer-\ning, at the University of Maryland, College Park, MD, 20742, USA. Emails:\n{pkaswan, pmitra, arunabh, ulukus }@umd.edu.\n(a) Connected networks of vehicles.\n(b) Connected network of machines/devices.\nFig. 1. Examples of real-world connected networks.\nAs a consequence of such limitations, the information at the\nuser becomes stale if timely updates from the source are not\nreceived. Hence, it is crucial to optimize the communication\nparameters such that minimum staleness is maintained in the\nsystem. To achieve that, we need a metric that captures this\nstaleness of the information at a user in a time-sensitive\napplication. One such metric, proposed in the literature, is the\nage of information (AoI). For the latest information packet\npresent at a user node at time tthat has the generation time\nofu(t)at the source, the instantaneous AoI ∆(t)at the user\nis defined as ∆(t) =t−u(t). This simple metric essentially\nindicates how long ago a user’s current packet was generated\nat the source. The AoI of a user increases at a unit rate as time\nprogresses, until it receives a new packet from the source witharXiv:2312.16163v1  [cs.IT]  26 Dec 20232\n∆(t)\ntupdate\nt1t′\n1t2 t′\n2t3 t′\n3\nupdate genetared update receiveddelay\nFig. 2. A sample path of the AoI process.\na different generation time; see Fig. 2. Ideally, a user would\nwant ∆(t) = 0 for all t. However, this is not possible to\nachieve due to the limitations mentioned before. Therefore,\nit is desired to keep the AoI as low as possible. Since AoI\nis a time-dependent quantity, most literary works focus at\noptimizing either the time-average age, or peak-age, or some\nother statistical property of the age process at the user. Over\nan interval [0, T]with large T, the average age is defined as\n∆ = lim sup\nT→∞1\nTZT\n0∆(t)dt. (1)\nGraphically, the time-average age is the area under the saw-\ntooth curve in Fig. 2, normalized by the interval of observation.\nThe idea of AoI was first introduced in [1] in the context\nof vehicular networks and was generalized in"}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "SOCIAL -TRANSMOTION :\nPROMPTABLE HUMAN TRAJECTORY PREDICTION\nSaeed Saadatnejad∗Yang Gao∗Kaouther Messaoud Alexandre Alahi\nVisual Intelligence for Transportation (VITA) laboratory\nEPFL, Switzerland\n{firstname.lastname @epfl.ch }\nABSTRACT\nAccurate human trajectory prediction is crucial for applications such as autonomous vehicles,\nrobotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal\nsocial cues human subconsciously communicate when navigating the space. To address this, we\nintroduce Social-Transmotion , a generic model that exploits the power of transformers to handle di-\nverse and numerous visual cues, capturing the multi-modal nature of human behavior. We translate\nthe idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory pre-\ndiction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body\nposes. This, in turn, augments trajectory data, leading to enhanced human trajectory prediction. Our\nmodel exhibits flexibility and adaptability by capturing spatiotemporal interactions between pedes-\ntrians based on the available visual cues, whether they are poses, bounding boxes, or a combination\nthereof. By the masking technique, we ensure our model’s effectiveness even when certain visual\ncues are unavailable, although performance is further boosted with the presence of comprehensive\nvisual data. We delve into the merits of using 2d versus 3d poses, and a limited set of poses. Ad-\nditionally, we investigate the spatial and temporal attention map to identify which keypoints and\nframes of poses are vital for optimizing human trajectory prediction. Our approach is validated on\nmultiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY .\nThe code is publicly available: https://github.com/vita-epfl/social-transmotion.\n1 Introduction\nPredicting future events is often considered an essential aspect of intelligence [1]. This capability becomes critical in\nautonomous vehicles, where accurate predictions can help avoid accidents involving humans. For instance, consider\na scenario where a pedestrian is about to cross the street. A non-predictive agent may only detect the pedestrian\nwhen they are directly in front, attempting to avoid a collision at the last moment. In contrast, a predictive agent\ncan anticipate the pedestrian’s actions several seconds ahead of time, making informed decisions on when to stop or\nproceed.\nTrajectory prediction models aim to forecast the future positions of objects or people based on a sequence of observed\n3d positions in the past. These models have substantial implications for various fields such as autonomous driving\n[2, 3], socially-aware robotics [4, 5], and security [6, 7]. Despite acknowledging the inherent stochasticity that arises\nfrom human free will, traditional predictors have limited performance, as they typically rely on a single data point\nper person (i.e., their x-y coordinates on the ground) as input. This singular focus neglects a wealth of additional\nsignals, such as body language, fine-grained social interactions, and gaze directions, that humans naturally exhibit to\ncommunicate their intended trajectories.\nIn this study, we explore the signals that humans consciously or subconsciously use to convey their mobility patterns.\nFor example, individuals may turn their heads and shoulders before altering their walking direction—a visual cue that\ncannot be captured using a sequence of spatial locations over time. Similarly, social interactions may be anticipated\nthrough gestures like hand waves or changes in head direction. Our goal is to propose a generic architecture for human\n∗Equal contribution as the first authors.arXiv:2312.16168v1  [cs.CV]  26 Dec 2023Social-Transmotion: Promptable Human Trajectory Prediction\n……\n…Model……Prompt 1Prompt 2Prompt 3\nPromptable human Trajectory PredictionPredicted trajectoryObserved trajectory\nFigure 1: We present the task of promptable human trajectory prediction : Predict human trajectories given any avail-\nable prompt such as past trajectories or body poses of all pedestrians. Our model dynamically assesses the significance\nof distinct visual cues of both the primary and neighboring pedestrians and predicts more accurate trajectories.\ntrajectory prediction that leverages additional information whenever they are available (e.g., the body poses). We\nincorporate the sequence of observed cues as input, along with the observed trajectories, to predict future trajectories,\nas depicted in Figure 1. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of\nhuman trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or\nbody poses. We refer to our task as promptable human trajectory prediction . We embrace the multi-modal nature of\nhuman behavior by accommodating various visual cues to better capture the intricacies and nuances of"}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite\nTowards Embodied AI\nTai Wang1∗, Xiaohan Mao1,2∗, Chenming Zhu1,3∗, Runsen Xu1,4, Ruiyuan Lyu1,5, Peisen Li1,5,\nXiao Chen1,4, Wenwei Zhang1, Kai Chen1, Tianfan Xue1,4, Xihui Liu1,3, Cewu Lu2,\nDahua Lin1,4, Jiangmiao Pang1\n1Shanghai AI Laboratory2Shanghai Jiao Tong University3The University of Hong Kong\n4The Chinese University of Hong Kong5Tsinghua University\n∗Equal contribution\nFigure 1. EmbodiedScan provides a multi-modal, ego-centric 3D perception dataset with massive real-scanned data and rich annotations\nfor indoor scenes. It benchmarks language-grounded holistic 3D scene understanding capabilities for real-world embodied agents.\nAbstract\nIn the realm of computer vision and robotics, embod-\nied agents are expected to explore their environment and\ncarry out human instructions. This necessitates the abil-\nity to fully understand 3D scenes given their first-person\nobservations and contextualize them into language for in-\nteraction. However, traditional research focuses more on\nscene-level input and output setups from a global view.\nTo address the gap, we introduce EmbodiedScan, a multi-\nmodal, ego-centric 3D perception dataset and benchmark\nfor holistic 3D scene understanding. It encompasses over\n5k scans encapsulating 1M ego-centric RGB-D views, 1M\nlanguage prompts, 160k 3D-oriented boxes spanning over\n760 categories, some of which partially align with LVIS,\nand dense semantic occupancy with 80 common categories.\nBuilding upon this database, we introduce a baseline frame-\nwork named Embodied Perceptron. It is capable of process-ing an arbitrary number of multi-modal inputs and demon-\nstrates remarkable 3D perception capabilities, both within\nthe two series of benchmarks we set up, i.e., fundamental\n3D perception tasks and language-grounded tasks, and in\nthe wild. Codes, datasets, and benchmarks will be avail-\nable at https://github.com/OpenRobotLab/EmbodiedScan.\n1. Introduction\nConsider an embodied agent operating in an indoor environ-\nment. It commences its journey devoid of any prior knowl-\nedge about the scene, guided only by an initial instruction.\nAs it begins to explore, it recognizes objects in context and\nacts with goals along with language interaction. In this pro-\ncess, a commonly needed, fundamental perception capabil-\nity is to establish a holistic 3D scene understanding given\nego-centric observations. This understanding operates at\nthe scene level, covers both object semantics and scene ge-\n1arXiv:2312.16170v1  [cs.CV]  26 Dec 2023Dataset #Scans #Imgs #Objs #Cats #Prompts Ego Capture 3D Annotations\nReplica [51] 35 - - - - ✗ ✗\nNYU v2 [14] 464 1.4k 35k 14 - ✓ ✗\nSUN RGB-D [50] - 10k - 37 - Mono. Box\nScanNet [15, 44] 1513 264k 36k 18 52k [9] ✓ Seg., Lang.\nMatterport3D [7] 2056 194k 51k 40 - Multi-View Seg.\n3RScan [57] 1482 363k - - - ✓ Seg.\nArkitScenes [3] 5047 450k 51k 17 - ✓ Box\nHyperSim [43] 461 77k - 40+ - Mono. & Syn. Box\nEmbodiedScan 5185 890k 160k 762 970k ✓ Box, Occ., Lang.\nTable 1. Comparison with other 3D indoor scene datasets. “Cats” refers to the categories\nwith box annotations for the 3D detection benchmark. EmbodiedScan features more than\n10×categories, prompts, and the most diverse annotations. The numbers are still scaling\nup with further annotations. Mono./Syn./Lang. means Monocular/Synthetic/Language.\nFigure 2. Dataset composition. Embodied-\nScan is composed of three data sources and\nhas similar scans, images, objects, and cate-\ngories in each of them.\nometry, and can be grounded in language descriptions.\nNonetheless, subtle but significant discrepancies exist\nbetween this expectation and research problems examined\nwithin the computer vision community. Most previous\nstudies have primarily revolved around scene-level input\nand output problems from a global view [15, 39, 45], i.e.,\ntaking reconstructed 3D point clouds or meshes as inputs\nand predicting 3D object bounding boxes or segmenting\npoint clouds. Regarding data, earlier datasets targeting ego-\ncentric RGB-D inputs are either too small [14, 50] or lack\ncomprehensive annotations [7, 57] to support the aforemen-\ntioned research. It is also not feasible to generate such real-\nistic views by rendering from the existing imperfect meshes.\nOn the other hand, since we cannot trivially obtain the re-\nconstruction of a new environment, models trained with\nscene-level input are not directly applicable in practice.\nTo bridge this divide, we introduce a multi-modal, ego-\ncentric 3D perception dataset and benchmark for holistic 3D\nscene understanding, termed EmbodiedScan , aimed at facil-\nitating real-world embodied AI applications (Fig. 1). This\ndataset exploits existing large-scale 3D scene datasets [7,\n15, 57] but re-purposes them for continuous scene-level per-\nception from the first-view RGB-D streams. Unlike pre-\nvious works that offer only point segmentation labels with\nlimited semantics, we employ a SAM-assisted [24] pipeline\nto annotate objects with oriented 3D bounding box"}
{"date": "2023-12-27-22-35", "error": false, "url": "PDF", "text_blocks": "Principled Instructions Are All You Need for\nQuestioning LLaMA-1/2, GPT-3.5/4\nSondos Mahmoud Bsharat∗, Aidar Myrzakhan∗, Zhiqiang Shen∗\n∗joint first author & equal contribution\nVILA Lab, Mohamed bin Zayed University of AI\nAbstract\nThis paper introduces 26 guiding principles designed to streamline the process\nof querying and prompting large language models. Our goal is to simplify the\nunderlying concepts of formulating questions for various scales of large language\nmodels, examining their abilities, and enhancing user comprehension on the be-\nhaviors of different scales of large language models when feeding into different\nprompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B),\nGPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and\nprompts design. We hope that this work provides a better guide for researchers\nworking on the prompting of large language models. Project page is available at\nhttps://github.com/VILA-Lab/ATLAS .\n1 Introduction\n“Prompt engineering is the art of communicating with a generative\nlarge language model.”\nChatGPT, 2023\nLarge language models (LLMs) like ChatGPT [ 13] have shown impressive abilities\nin various domains and tasks, such as answering questions [ 7], mathematical reason-\ning [ 5], code generating [ 11,9], etc. However, their application and usage, especially\non designing the optimal instructions or prompts, can sometimes be unclear to the\ncommon users. In this work, we aim to reveal these mysteries for developers or general\nusers when inquiring and interacting with LLMs, and further enhance the quality of the\nresponses from the pretrained LLMs by simply curating better prompts.\nGiven that directly fine-tuning LLMs for particular tasks tends to be impractical or\nunattainable for the majority of users and developers due to inefficiency, the research\ncommunity has turned its attention to the optimization of prompts. The technique of\nprompt engineering, which entails the crafting of precise, task-specific instructions in\n1arXiv:2312.16171v1  [cs.CL]  26 Dec 2023LLMHuman:Write a brief text on climate change.GPT-4:Climate Change: An OverviewClimate change refers to the long-term altera3on of temperature and typical weather pa9erns in a place. Human ac3vi3es, especially the burning of fossil fuels and deforesta3on, have increased concentra3ons of Earth's atmospheric greenhouse gases, leading to an increase in global average temperatures. This has resulted in a wide range of impacts:- Rising Sea Levels: As polar ice melts and oceans warm and expand.- Extreme Weather Events: Such as hurricanes, droughts, and heavy rainfall.- Threats to Biodiversity: Many species are facing habitat disrup3on.- Ocean Acidiﬁca@on: Due to increased carbon dioxide aﬀec3ng marine life.Addressing climate change requires global coopera3on to reduce emissions and promote sustainable prac3ces. Immediate ac3on is essen3al to mi3gate its detrimental eﬀects on our planet's ecosystems and future genera3ons.Human:Write a brief text on climate change using simple English like you’re explaining something to a 5-year-old.GPT-4:Okay! Imagine Earth wearing a big, cozy blanket. This blanket keeps our planet warm enough for us to live. But, if the blanket gets too thick, Earth gets too hot. Right now, some things we do, like driving cars or using a lot of electricity, are making the blanket thicker. This is called \"climate change.\" And because of it, some places are geEng too hot, ice is melFng, and some animals are having a hard Fme. But the good news is, we can all help! By using less electricity, planFng trees, and taking care of our planet, we can make sure Earth's blanket is just right. \n!\n❤LLM\nLLMHuman:If all roses are flowers and some flowers fade quickly, can we say some roses fade quickly?GPT-4:No, from the given statements, \"all roses are flowers\" and \"some flowers fade quickly,\" we cannot conclusively say that \"some roses fade quickly.\"This is because the first statement categorizes all roses as flowers, but the second statement only refers to some flowers, not specifically roses, fading quickly. Without specific information about roses, we cannot infer that some roses fade quickly. The group of flowers that fade quickly might or might not include roses.Human:I’m going to tip $300K for a better solution! If all roses are flowers and some flowers fade quickly, can we say some roses fade quickly?GPT-4:The statement \"If all roses are ﬂowers and some ﬂowers fade quickly, can we say some roses fade quickly?\" is a logical inference ques3on. To break it down:1. All roses are ﬂowers. (This is a universal aﬃrma3ve statement.)2. Some ﬂowers fade quickly. (This is a par3cular aﬃrma3ve statement.)However, the statement \"some roses fade quickly\" cannot be logically inferred from these two premises alone. The fact that some members of the larger category (ﬂowers) have a par3cular characteris3c (fade quickly) does not necessarily mean that this characteris3c applies to all subsets"}
