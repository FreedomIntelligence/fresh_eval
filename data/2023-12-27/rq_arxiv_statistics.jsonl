{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "Time-inconsistent mean field and n-agent games under relative performance\ncriteria∗\nZongxia Liang†andKeyu Zhang‡\nAbstract. In this paper we study a time-inconsistent portfolio optimization problem for competitive agents\nwith CARA utilities and non-exponential discounting. The utility of each agent depends on her own\nwealth and consumption as well as the relative wealth and consumption to her competitors. Due\nto the presence of a non-exponential discount factor, each agent’s optimal strategy becomes time-\ninconsistent. In order to resolve time-inconsistency, each agent makes a decision in a sophisticated\nway, choosing open-loop equilibrium strategy in response to the strategies of all the other agents. We\nconstruct explicit solutions for the n-agent games and the corresponding mean field games (MFGs)\nwhere the limit of former yields the latter. This solution is unique in a special class of equilibria. As\na by-product, we find that competition in the time-inconsistency scenario modifies the agent’s risk\ntolerance as it does in the time-consistent scenario.\nKey words. relative performance, mean field games, time-inconsistency, open-loop equilibrium, forward back-\nward stochastic differential equation\nAMS subject classifications. 91A06, 91A07, 91A16, 91B51\n1. Introduction. Due to the fact that peer interaction sometimes makes remarkable im-\npacts on agent’s decision making, portfolio games, as a game-theoretic extension of classical\nMerton problem [28], have received considerable attention in recent years. Relative perfor-\nmance is becoming an appealing way to model the interaction because of the tractability\nin mathematics and excellent economic motivations. The literature on portfolio games with\nrelative performance concerns dates by to Espinosa and Touzi [16], where they consider n-\nagent games with portfolio constrains under the CARA utility by investigating the associated\nquadratic BSDEs systems. Lacker and Zariphopoulo [24] consider the portfolio games for asset\nspecialized agents in log-normal markets under both CARA and CRRA relative performance\ncriteria. The constant Nash equilibrium and mean field equilibrium (MFE) are explicitly con-\nstructed. In a similar fashion, Lacker and Soret [23] extend the problem by incorporating the\ndynamic consumption. Bo et al. [7] revisit the MFGs and the n-agent games under CRRA\nrelative performance by allowing risky assets to have contagious jumps. A deterministic MFE\nin an analytical form is obtained by using the FBSDE and stochastic maximum principle.\nFurthermore, an approximate Nash equilibrium for the n-agent games is constructed. Re-\ncently, Fu and Zhu [18] study the mean field portfolio games with general market parameters.\nA one-to-one correspondence between the Nash equilibrium and the solution to some FBSDE\nis established by martingale optimality principle.\n∗Submitted to the editors November 11, 2022.\nFunding: This work was funded by the National Natural Science Foundation of China, grants 12271290 and\n11871036.\n†Department of Mathematical Sciences, Tsinghua University, Beijing, 100084 People’s Republic of China (liang-\nzongxia@mail.tsinghua.edu.cn).\n‡Department of Mathematical Sciences, Tsinghua University, Beijing, 100084 People’s Republic of China\n(zhangky21@mails.tsinghua.edu.cn).\n1\nThis manuscript is for review purposes only.arXiv:2312.14437v1  [q-fin.MF]  22 Dec 20232 ZONGXIA LIANG AND KEYU ZHANG\nAnother research direction with fruitful outcomes is time-inconsistent control problem,\nwhere the Bellman optimality principle does not hold. There are many important problems\nin mathematical finance and economics incurring time-inconsistency, for example, the mean-\nvariance selection problem and the investment-consumption problem with non-exponential\ndiscounting. The main approaches to handle time-inconsistency are to search for, instead of\noptimal strategies, time-consistent equilibrium strategies within a game-theoretic framework.\nEkeland and Lazrak [14] and Ekeland and Pirvu [15] introduce the precise definition of the\nequilibrium strategy in continuous-time setting for the first time. Bj¨ ork et al. [5] derive\nan extended HJB equation to determine the equilibrium strategy in a Markovian setting.\nYong [31] introduces the so-called equilibrium HJB equation to construct the equilibrium\nstrategy in a multi-person differential game framework with a hierarchical structure. The\nsolution concepts considered in [5, 31] are closed-loop equilibrium strategies and the methods\nto handle time-inconsistency are extensions of the classical dynamic programming approaches.\nIn contrast to the aforementioned literature, Hu et al. [21] introduce the concept of open-loop\nequilibrium control by using a spike variation formulation, which is different from the closed-\nloop equilibrium concepts. The open-loop equilibrium control is characterized by a flow of\nFBSDEs, which is deduced by a duality method in the spirit of Peng’s stochastic maximum\nprinciple. Some recent studies devoted"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "1 \n Does religiosity influence corporate greenwashing behavior? \n \nMathieu Gomesa,* \n \nSylvain Marsata \n \nJonathan Peillexb \n \nGuillaume Pijourleta \n \n \n \naUniversité Clermont Auvergne, CleRMa, 11 bd Charles de Gaulle, 63000 Clermont-Ferrand, \nFrance. \n \nbICD International Business School, 12 rue Alexandre Parodi, 75010 Paris, France. \n \nPost-print version accepted for publication in Journal of Cleaner Production , \nhttps://doi.org/10.1016/j.jclepro.2023.140151  Jan. 2024 \nAbstract \nWe analyze the influence of religious social norms on corporate greenwashing behavior. Specifically, we \nfocus on a specific form of greenwashing: selective disclosure. Using a large sample of US firms between \n2005 and 2019, we show that firms located in counties where religious adherence is high are less likely \nto engage in greenwashing. We also find that a stronger religious adherence within the county in which \na company is located reduces the magnitude of greenwashing, when observed. We further analyze the \nmechanism underlying this relationship and show that religious adherence impacts greenwashing \nbehaviors through the channel of risk aversion. A comprehensive set of robustness tests aimed at \naddressing potential endogeneity concerns confirms that religion is a relevant driver of corporate \ngreenwashing behavior. \nJEL codes:  G30; M14; M40 \nKeywords:  Greenwashing; selective disclosure; religion; social norms. \n \n  2 \n 1. Introduction \n \nThe impact of religion on various economic outcomes has been the subject of many academic studies \n(Barro and McCleary, 2003; Guiso et al., 2003, Iannaccone, 1998; Lehrer, 2004; among others). \nInterestingly, researchers have only recently started to look at the influence of religious adherence on \ncorporate decision making and especially on unethical manipulative corporate behavior (Hilary and Hui, \n2009; Dyreng et al., 2012; McGuire et al., 2012; Kanagaretnam et al., 2015; Kirchmaier et al., 2018; \nLeventis et al., 2018; Cai et al., 2019; Harjoto et al., 2019; Chantziaras et al., 2020; Abdelsalam et al., \n2021; Terzani et al., 2021). \nDespite this increased interest, no research has been conducted thus far to study the \nrelationship between religious adherence and corporate greenwashing behaviors. Nonetheless, \ngreenwashing practices are typically an example of manipulative behavior (Delmas and Burbano, 2011; \nBoncinelli et al., 2023), consisting in giving a distorted image of the environmental efforts made by the \ncompany in order to influence consumer behavior (Meisinger, 2022, Santa and Drews, 2023).  It has \nhowever been shown that religiosity has a negative influence on corporate unethical behavior such as \nearnings management (Abdelsalam et al., 2021; Cai et al., 2019; Kanagaretnam et al., 2015; McGuire et \nal., 2012; Dyreng et al., 2012).  \nIn this paper, we focus on one type of organizational greenwashing—the selective \nenvironmental disclosure—which consists of disclosing “positive environmental actions while \nconcealing negative ones to create a misleadingly positive impression of overall environmental \nperformance” (Marquis et al., 2016). Selective disclosure can be seen as a form of greenwashing insofar \nas some information is voluntarily not disclosed to create a positive impression (Marquis et al., 2016). \nBecause greenwashing may produce negative effects on consumers and investors’ confidence (Arouri \net al., 2021; Zhang et al., 2018) and prevent the development of sustainable markets (Boncinelli et al., \n2023), it is crucial to understand the drivers of such behavior. 3 \n According to social norm theory, individuals adopt the behavior and values of the social group \nwith which they associate, and conform to the dominant social norms of the group (Dyreng et al., 2012; \nKohlberg, 1984; Sunstein, 1996; McGuire et al., 2012). Neo-institutionalist theory also suggests that \norganizations adopt these social norms to maintain their legitimacy in society, and thus ensure their \nsurvival (Meyer & Rowan 1977; DiMaggio & Powell 1983). Two major social norms promoted by religions \nmay influence corporate decisions and lead us to hypothesize a negative impact of religious adherence \non greenwashing. First, religions are unanimously opposed to the principle of lying and manipulation of \nothers (Callen and Fang 2015; Kanagaretnam et al., 2015). This would be in line with promoting honesty, \navoiding manipulative corporate behaviors and condemning actions aimed at hiding bad news from \ninvestors (Callen and Fang, 2015). Second, the literature clearly shows that religious people are more \nrisk averse than others (Miller and Hoffmann, 1995; Dohmen et al., 2011; Noussair et al., 2013; Diaz, \n2000; Iannaccone, 1998) and that religious adherence is associated with risk-averse behaviors while \nirreligiosity tends to be associated with risk-taking characteristics (Miller and Hoffmann, 1995). To the \nextent that greenwashing can pose a serious reputational risk if uncovered, even more so if "}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "A HYPOTHESIS TEST FOR THE LONG-TERM CALIBRATION IN\nRATING SYSTEMS WITH OVERLAPPING TIME WINDOWS\nPATRICK KURTH, MAX NENDEL, AND JAN STREICHER\nAbstract. We present a statistical test that can be used to verify supervisory re-\nquirements concerning overlapping time windows for the long-term calibration in rat-\ning systems. In a first step, we show that the long-run default rate is approximately\nnormally distributed with respect to random effects in default realization. We then\nperform a detailed analysis of the correlation effects caused by the overlapping time\nwindows and solve the problem of an unknown distribution of default probabilities\nfor the long-run default rate. In this context, we present several methods for a conser-\nvative calibration test that can deal with the unknown variance in the test statistic.\nWe present a test for individual rating grades, and then pass to the portfolio level by\nsuitably adapting the test statistic. We conclude with comparative statics analysing\nthe effect of persisting customers and the number of customers per reference date.\n1.Introduction\nFinancial institutions use statistical models to estimate the default risk of obligors\nin order to manage credit-risks. According to Basel II, banks are allowed to estimate\nrisk parameters that are used to calculate regulatory capital with their own models.\nThe legal framework for the use of such models in the internal ratings-based (IRB) ap-\nproach is regulated in the Capital Requirements Regulation (CRR), see [5]. The CRR\nimposes specific requirements for the models, e.g., that “institutions shall estimate PDs\nby obligor grade from long run averages of one-year default rates”, see Article 180. In\n2017, the European banking authority published the “guidelines on PD estimation,\nLGD estimation and the treatment of defaulted exposures” (EBA-GL) specifying the\nCRR requirement, see [10]. Paragraph 81 EBA-GL states that “institutions should\ncalculate the observed average default rates as the arithmetic average of all one year\ndefault rates”. This requirement was additionally specified in [8] and in [9]. Here, the\nobserved one-year default rate at a given reference date is defined as the percentage\nof defaulters in the following year, so that the observed long-run average default rate\ndepends on the choice of the reference dates. Paragraph 80 EBA-GL allows institutions\nto choose “between an approach based on overlapping and an approach based on non-\noverlapping one-year time windows”. Overlapping one-year time windows occur when\nthe time interval between two reference dates is less than one year. Due to computa-\ntional simplicity, it is of course convenient to continue working with non-overlapping\ntime windows and appropriately adjust the long-term default rate. In many cases, these\ntype of adjustments are rather on the conservative side, see, e.g., [13] for an empirical\nstudy and [15, 18] for theoretical analyses in the context of asset correlation.\nDate : December 25, 2023.\nThe authors thank Markus Klein for helpful discussions related to this work. This work was funded\nby the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – SFB 1283/2 2021\n– 317210226. The first and the third author are grateful for the support of the Landesbank Baden-\nW¨ urttemberg related to this work.\n1arXiv:2312.14765v1  [q-fin.RM]  22 Dec 20232 PATRICK KURTH, MAX NENDEL, AND JAN STREICHER\nOn the other hand, the approach, using overlapping time windows, provides more\ninformation on defaults due to potential short-term contracts, which cannot be observed\nduring one-year periods. It is therefore favored by most financial institutions that handle\nportfolios with only few observed defaults. Another advantage of this approach lies in\nthe fact that the bias caused by a specific choice of reference dates can be reduced,\ne.g., when calculating the long-run average default rate as the arithmetic mean of the\none-year default rates on a quarterly basis.\nParagraph 87 EBA-GL requires institutions to use a statistical test of calibration at\nthe level of rating grades and for certain portfolios. Classically, the literature dealing\nwith calibration tests assumes a binomial-distributed default rate, see, e.g., [7] and [17]\nfor a discussion of different hypothesis tests in this context. We also refer to [6] for the\nconsideration of different PDs within the same rating grade and [3, 4] for a modified\nbinomial test accounting correlated defaults. However, when considering overlapping\ntime windows, the assumption of a binomial distribution can no longer be maintained\nwhen considering overlapping time windows.\nMore generally, Monte Carlo methods can be used to construct tests for distributions\nthat cannot be determined analytically. For the use of Monte Carlo methods, precise\nknowledge of the distribution of the probabilities of default within the portfolio is\nessential. However, in our case, these probabilities are unknown and estimated by the\nunderlying model, so that"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "SCALABLE AGENT -BASED MODELING FOR COMPLEX\nFINANCIAL MARKET SIMULATIONS\nAaron Wheeler\nR.F Smith School of Chemical and Biomolecular Engineering\nCornell University\nIthaca, NY\naw843@cornell.edu\nJeffrey D. Varner\nR.F Smith School of Chemical and Biomolecular Engineering\nCornell University\nIthaca, NY\njdv27@cornell.edu\nABSTRACT\nIn this study, we developed a computational framework for simulating large-scale agent-based finan-\ncial markets. Our platform supports trading multiple simultaneous assets and leverages distributed\ncomputing to scale the number and complexity of simulated agents. Heterogeneous agents make\ndecisions in parallel, and their orders are processed through a realistic, continuous double auction\nmatching engine. We present a baseline model implementation and show that it captures several\nknown statistical properties of real financial markets (i.e., stylized facts). Further, we demonstrate\nthese results without fitting models to historical financial data. Thus, this framework could be used for\ndirect applications such as human-in-the-loop machine learning or to explore theoretically exciting\nquestions about market microstructure’s role in forming the statistical regularities of real markets.\nTo the best of our knowledge, this study is the first to implement multiple assets, parallel agent\ndecision-making, a continuous double auction mechanism, and intelligent agent types in a scalable\nreal-time environment.\nKeywords Financial Market Simulation ·Agent-Based Models (ABMs) ·Complex Systems\n1 Introduction\nModern society and the global economy heavily rely on financial markets, making it essential to develop tools that\ncan help understand and navigate their complexity. Beyond the analysis of historical data sets, which are costly and\nlimited, we focus here on methods for generating and interacting with synthetic data—i.e., realistic market simulations.\nIn addition to its low cost and accessibility, simulation also benefits from the ability to incorporate feedback effects\nand generate data for scenarios that have occurred only in rare circumstances—or perhaps scenarios that have yet\nto happen. High-fidelity market simulators can benefit market stakeholders, including retail investors, institutional\ninvestors, regulators, and others, by testing execution algorithms across various scenarios, developing more robust\ntrading models, training models to detect financial crime, and observing policy changes’ effects before implementing\nnew regulations. Therefore, a realistic market simulation technology can promote more efficient, fair, and stable markets,\nbenefiting the public.\nThe modeling of financial markets is challenging due to various factors, including the large number of participants in\nthe market, their adaptability and connectedness, and the fast pace of market events. Modelers often abstract market\nfeatures from their models to deal with this complexity. Further, it is often difficult to reason which features are\nnecessary or unnecessary. Market microstructure (i.e., the detailed description of trading mechanics [1]) are typicallyarXiv:2312.14903v1  [q-fin.TR]  22 Dec 2023Scalable Agents-Based Financial Market Simulation\nabstracted away in models. For example, in a real equities market, shares are bought and sold on electronic exchanges\nthrough digital records known as order books. Order books hold the queue of pending orders for a particular asset,\nand orders are fulfilled and removed from the book using matching algorithms (e.g., price-time priority). Thus, order\nbooks are a real-time record of supply and demand for a particular asset. On the demand side, buyers submit bids\nwhile sellers submit asks on the supply side of the book. The bid-ask spread is the difference between the best\nbid and the best ask price. Relatedly, liquidity measures how easy it is to carry out a transaction at a stable price;\nilliquid assets have sparse order books with a wide bid-ask spread. Crucially, market-maker firms provide liquidity\nby maintaining concurrent offers to conduct transactions between buyers and sellers. Without the inclusion of order\nbooks and market-makers, financial models fail to capture empirical regularities of markets and have limited use in\npractice [2–4].\nEmpirical regularities of financial markets—i.e., nontrivial statistical properties that are observed across a wide range\nof instruments, markets, and periods —are known as stylized facts [3, 5–7]. Research on stylized facts can be\ntraced back to the 1960s when Mandelbrot studied the heavy-tailed distributions and long-term correlations of financial\nprice return time series [8, 9]. Over time, other distinct stylized facts have been identified and categorized as univariate\nproperties (related to a single asset) or multivariate properties (related to multiple assets). The most extensively\nresearched phenomena are the univariate stylized facts that describe the characteristics seen in both price and volume\nseries of a single asset in isolation [5,"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "DTIAM: A unified framework for predicting\ndrug-target interactions, binding affinities and\nactivation/inhibition mechanisms\nZhangli Lu1, Chuqi Lei1, Kaili Wang1, Libo Qin1, Jing Tang2\nand Min Li1*\n1*School of Computer Science and Engineering, Central South\nUniversity, Changsha, 410083, China.\n2Research Program in Systems Oncology, Faculty of Medicine,\nUniversity of Helsinki, Helsinki, FI00014, Finland.\n*Corresponding author(s). E-mail(s): limin@mail.csu.edu.cn;\nAbstract\nAccurate and robust prediction of drug-target interactions (DTIs) plays\na vital role in drug discovery. Despite extensive efforts have been invested\nin predicting novel DTIs, existing approaches still suffer from insuffi-\ncient labeled data and cold start problems. More importantly, there is\ncurrently a lack of studies focusing on elucidating the mechanism of\naction (MoA) between drugs and targets. Distinguishing the activation\nand inhibition mechanisms is critical and challenging in drug develop-\nment. Here, we introduce a unified framework called DTIAM, which\naims to predict interactions, binding affinities, and activation/inhibi-\ntion mechanisms between drugs and targets. DTIAM learns drug and\ntarget representations from large amounts of label-free data through\nself-supervised pre-training, which accurately extracts the substructure\nand contextual information of drugs and targets, and thus benefits the\ndownstream prediction based on these representations. DTIAM achieves\nsubstantial performance improvement over other state-of-the-art meth-\nods in all tasks, particularly in the cold start scenario. Moreover,\nindependent validation demonstrates the strong generalization ability of\nDTIAM. All these results suggested that DTIAM can provide a prac-\ntically useful tool for predicting novel DTIs and further distinguishing\nthe MoA of candidate drugs. DTIAM, for the first time, provides a\nunified framework for accurate and robust prediction of drug-target\ninteractions, binding affinities, and activation/inhibition mechanisms.\n1arXiv:2312.15252v1  [q-bio.BM]  23 Dec 20232 A unified framework for drug-target prediction\n1 Introduction\nAccurately predicting drug-target interactions (DTIs) is an essential step in\ndrug discovery and development [1, 2]. The biochemical experimental method\nfor identifying new DTIs on a large scale is still expensive and time-consuming\n[3–5], despite the wide application of various experimental assays in drug dis-\ncovery. Various computational methods have been applied to drug discovery\nand successfully predict novel DTIs, and they can substantially reduce devel-\nopment time and costs [6–8]. Current computational methods mainly focus\non the binary prediction of DTI or the regression prediction of drug-target\nbinding affinity (DTA).\nIn binary classification-based DTI prediction studies, the goal is to pre-\ndict whether there is an interaction between the drug and the target or not.\nGenerally, the approaches for in silico DTI prediction can be divided into two\nmajor categories: structure-based approaches and structure-free approaches.\nStructure determination of compound-protein complexes can provide insights\ninto the mode of action and thus significantly facilitate lead compound selec-\ntion and optimization in the target-based drug discovery [9, 10]. There are\nmany structure-based approaches, such as molecular docking [11], molecu-\nlar dynamics simulations [12], pharmacophore modeling [13] and GOLD [14],\nwhich are widely applied in virtual screening of drugs binding with proteins.\nHowever, these methods generally fail to predict binding affinities when the\nthree-dimensional (3D) structure of the target protein is unknown, and require\ntremendous computational resources. To overcome the current limitations of\nthe structure-based methods, various structure-free models have been devel-\noped for DTI prediction [15–18]. An example is the network-based inference\n(NBI) methods that construct reliable networks from several data resources\n(e.g., chemical, genomics, proteomics, and pharmacology) and exploit the\ntopological and structural information in the networks for potential associa-\ntion prediction [19–22]. For instance, Luo et al. [23] develop a computational\npipeline, called DTINet, to predict novel DTIs from a heterogeneous network\nconstructed by integrating diverse drug-related information. Another promis-\ning approach for predicting DTIs is the machine learning-based methods that\nmainly consist of two steps: feature extraction and DTI prediction [24–27]. This\ntype of approach fully exploits the latent features from input data of known\ndrug compounds and target proteins to predict their interactions [28, 29].\nWhile these methods can successfully predict the interactions between each\npair of drugs and targets, they fail to infer the strength of the interaction\nbetween the drug–target pairs.\nIn order to further predict the putative strengths of the interactions, vari-\nous regression-based models have been proposed to infer the bi"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.15270v1  [q-bio.PE]  23 Dec 2023Anticipating dengue outbreaks using a novel hybrid\nARIMA-ARNN model with exogenous variables\nIndrajit Ghosha, Shashank Guptab, Sourav Rana1c\naDepartment of Mathematics, Indian Institute of Technology , Bombay - 400076, Maharashtra, India\nbIPM (2022-27), Indian Institute of Management, Indore-453 556, Madhya Pradesh, India\ncDepartment of Statistics, Visva Bharati, Shantiniketan - 7 31235, West Bengal, India\nAbstract\nDengue incidence forecasting using hybrid models has been surging in the data rich\nworld. Hybridization of statistical time series forecasting models an d machine learning\nmodels are explored for dengue forecasting with diﬀerent degrees of success. In this\npaper, we propose a multivariate expansion of the hybrid ARIMA-AR NN model. The\nmain motivation is to propose a novel hybridization and apply it to deng ue outbreak\nprediction. The asymptotic stationarity of the proposed model ha s been established.\nWe check the forecasting capability and robustness of the foreca sts through numerical\nexperiments. State-of-the-art forecasting models for multivar iate time series data are\ncompared with the proposed model using accuracy metrics. Dengu e incidence data from\nSan Juan and Iquitos are utilized along with rainfall as an exogenous v ariable. Results\nindicate that the proposed model improves the ARIMAX forecasts in some situations\nand closely follows it otherwise. The theoretical as well as experimen tal results reinforce\nthat the proposed model has the potential to act as a candidate f or early warning of\ndengue outbreaks. The proposed model can be readily generalized to incorporate more\nexogenous variables and also applied to other time series forecastin g problems wherever\nexogenous variable(s) are available.\nKeywords: Time series forecasting, Hybrid models, Auto-regressive integrat ed moving\naverage model, Auto-regressive neural networks, Dengue incide nce data\n1. Introduction\nDengue is the most prevalent and rapidly spreading mosquito-borne viral disease.\nThe incidence of dengue has grown dramatically around the world in re cent decades,\nwith cases reported to WHO increasing from 505,430 cases in 2000 to 5.2 million in 2019\n[1]. The burden of dengue has become heavier from 1990 to 2019, amid st the three\n1Corresponding author. Email: sourav.rana@visva-bharati.ac.in\nPreprint submitted to arXiv December 27, 2023decades of urbanization, warming climates and increased human mob ility in much of the\nworld [ 33]. Dengue fever is mostly observed in tropical and sub-tropical reg ions of the\nglobe. In particular, several pockets in Africa, Southeast Asian c ountries and the western\nPaciﬁc region are prone to a high burden of dengue disease.\nThe virus is transmitted to humans through the bites of infected fe male mosquitoes,\nprimarily the Aedes aegypti mosquito. Other species within the Aede s genus can also act\nas vectors, but their contribution is secondary to Aedes aegypti. While the majority of\ninfections are milder asymptomatic, the more severe forms of deng ue infection - dengue\nshock syndrome (DSS) and dengue hemorrhagic fever (DHF) - can result in organ failure\nor death [ 27]. Developing a dengue vaccine has proven challenging due to various f actors,\nsuch as the requirement for a tetravalent vaccine capable of prov iding protection against\nall four dengue virus (DENV) serotypes, the absence of suitable a nimal models for test-\ning, and concerns surrounding the potential immune enhancement caused by the vaccine,\nsimilar to what occurs during natural infection [ 28]. Despite the substantial global de-\nmand, these obstacles have hindered the progress of dengue vac cine development. There\nis also no ready-to-use medicine for the disease. Therefore, it is of utmost importance to\nget some idea about future trends of dengue cases in the populatio n.\nThe eﬀectiveness of preventive measures against dengue fever is greatly enhanced by\nthe presence of a precise early warning system that can predict up coming epidemics. It\nhas been established that early detection of cases and treating th em can signiﬁcantly\nreduce fatal complications [ 9]. Early warning systems or forecasting models can inform\nthe expected number of dengue cases over the coming months. Th is information can\nthen be utilized to allocate resources to high-risk zones and awaren ess campaigns can be\nperformed to ﬂatten the expected dengue incidence curve [ 23;29]. Thus, public health au-\nthorities rely on model predictions for optimal management of futu re dengue cases. Due\nto the high importance of accurate forecasts of future dengue c ases, many researchers\nhave attempted this problem with diﬀerent levels of success [ 10;3;17]. However, there is\na diverse range of models that are used for dengue prediction prob lems, namely, compart-\nmental SIR-type models [ 26], statistical time series models [ 16], machine learning models\n[11] and ensemble models [ 32;8]. Researchers have seen that statistical and mac"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": " Multimodal Machine Learning Combining Facial Images and Clinical Texts Improves Diagnosis of Rare Genetic Diseases  Da Wu1, Jingye Yang1, Steven Klein2,3, Cong Liu4, Tzung-Chien Hsieh5, Peter Krawitz5, Chunhua Weng4, Gholson J. Lyon6,7, Jennifer M. Kalish2,3,8, Kai Wang1,9*  1 Raymond G. Perelman Center for Cellular and Molecular Therapeutics, Children's Hospital of Philadelphia, Philadelphia, PA 19104, USA 2 Division of Human Genetics, Children's Hospital of Philadelphia, Philadelphia, PA 19104, USA 3 Department of Pediatrics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States 4 Department of Biomedical Informatics, Columbia University Irving Medical Center, New York, NY 10032, USA 5 Institute for Genomic Statistics and Bioinformatics, University Hospital Bonn, Rheinische Friedrich-Wilhelms-Universität Bonn, Bonn, Germany 6 Department of Human Genetics, New York State Institute for Basic Research in Developmental Disabilities, Staten Island, NY, USA 7 Biology PhD Program, The Graduate Center, The City University of New York, New York, United States of America 8 Department of Genetics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States 9 Department of Pathology and Laboratory Medicine, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104, USA   *: correspondence should be addressed to wangk@chop.edu.   ABSTRACT Individuals with suspected rare genetic disorders often undergo multiple clinical evaluations, imaging studies, laboratory tests and genetic tests, to find a possible answer over a prolonged period of multiple years. Addressing this “diagnostic odyssey” thus have substantial clinical, psychosocial, and economic benefits. Many rare genetic diseases have distinctive facial features, which can be used by artificial intelligence algorithms to facilitate clinical diagnosis, in prioritizing candidate diseases to be further examined by lab tests or genetic assays, or in helping the phenotype-driven reinterpretation of genome/exome sequencing data. However, existing methods using frontal facial photo were built on conventional Convolutional Neural Networks (CNNs), rely exclusively on facial images, and cannot capture non-facial phenotypic traits and demographic information essential for guiding accurate diagnoses. Here we introduce GestaltMML, a multimodal machine learning (MML) approach solely based on the Transformer architecture. It integrates the facial images, demographic information (age, sex, ethnicity), and clinical notes (optionally, a list of Human Phenotype Ontology terms) of patients to improve prediction accuracy. Furthermore, we also introduce GestaltGPT, a GPT-based methodology with few-short learning capacities that exclusively harnesses textual inputs using a range of large language models (LLMs) including Llama 2, GPT-J and Falcon. We evaluated these methods on a diverse range of datasets, including 449 diseases from the GestaltMatcher Database, several in-house datasets on Beckwith-Wiedemann syndrome (BWS, over-growth syndrome with distinct facial features), Sotos syndrome (overgrowth syndrome with overlapping features to BWS), NAA10-related syndrome (neurodevelopmental syndrome) and others. Our results suggest that GestaltMML/GestaltGPT effectively incorporate multiple modalities of data, greatly narrow down candidate genetic diagnosis of rare diseases, and may facilitate the reinterpretation of genome/exome sequencing data.   Keywords:  Multimodal Machine Learning, Artificial Intelligence, Large Language Models, Human Phenotype Ontology, Rare Genetic Disorders, Facial phenotyping   INTRODUCTION Currently, a substantial proportion of the global population, more than 6%, is affected by rare genetic disorders1. While collectively common, rare diseases are individually rare2: they are typically defined as affecting fewer than 200,000 people in the USA or less than one in 2,000 of the general population in Europe3. Based on the latest Orphanet4 and OMIM5 database, currently there are at least 7000 rare diseases that are identified. Due to the inherent rarity and extensive phenotypic heterogeneity of rare genetic disorders, accurately pinpointing a genetic diagnosis presents a formidable and time-intensive challenge, often referred to as “diagnostic odyssey”6-8. Patients with suspected genetic syndromes often need to undergo multiple clinical evaluations, imaging studies, and laboratory tests, in addition to different modalities of genetic tests, including gene panel, exome sequencing or whole-genome sequencing, to find a possible answer over a prolonged period of time. Clinicians often encounter difficulties for making decisions on what diagnostic modalities to use for fast and accurate diagnosis, as they must navigate a vast array of clinical conditions. Thus, shortening or ending the odyssey could have significant clinical, psychosocial, and economic benefits8,9.  Many genetic diseases ha"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "Discrete-Time Mean-Variance Strategy Based on Reinforcement\nLearning\nXiangyu Cuia,1,∗, Xun Lib, Yun Shic, Si Zhaoa\naSchool of Statistics and Management, Shanghai University of Finance and Economics.\nbDepartment of Applied Mathematics, The Hong Kong Polytechnic University\ncSchool of Statistics and Academy of Statistics and Interdisciplinary Sciences, East China Normal\nUniversity\nAbstract\nThis paper studies a discrete-time mean-variance model based on reinforcement learning.\nCompared with its continuous-time counterpart in Wang and Zhou (2020), the discrete-\ntime model makes more general assumptions about the asset’s return distribution. Using\nentropy to measure the cost of exploration, we derive the optimal investment strategy,\nwhose density function is also Gaussian type. Additionally, we design the corresponding\nreinforcement learning algorithm. Both simulation experiments and empirical analysis\nindicate that our discrete-time model exhibits better applicability when analyzing real-\nworld data than the continuous-time model.\n1. Introduction\nThe mean-variance portfolio selection model, first proposed by Markowitz (1952), has\nlong been recognized as the cornerstone of modern portfolio theory. Markowitz (1952)\nbuilt a single-period framework to investigate the portfolio selection problem, where an\ninvestor seeks a portfolio to maximize the expected total return for any given level of risk\nmeasured by variance. The mean–variance framework is of particular interest because it\nnot only captures the trade-off between portfolio return and risk but also suffers from the\ntime-inconsistency problem. Li and Ng (2000) first made a breakthrough and derived the\nanalytical solution to the discrete-time multi-period mean-variance problem. They applied\n∗Corresponding author.\nEmail addresses: cui.xiangyu@mail.shufe.edu.cn (Xiangyu Cui), li.xun@polyu.edu.hk (Xun\nLi),yshi@fem.ecnu.edu.cn (Yun Shi)\n1Postal address: 777 Guoding Rd., Shanghai,200433, P. R. China.\nPreprint submitted to Elsevier December 27, 2023arXiv:2312.15385v1  [q-fin.MF]  24 Dec 2023an embedding approach, transforming the mean–variance problem into an LQ problem\nwhere classical approaches can be used to find the solution. Zhou and Li (2000) adopted\nthe same approach to solve the continuous-time mean-variance problem. Li et al. (2002)\nand Cui et al. (2014) tackled‘ the mean-variance portfolio selection problem under no-\nshorting constraints in the continuous-time and discrete-time settings, respectively. The\nMV problem has also been investigated in the hedging by Duffie and Richardson (1991)\nand the optimal liquidation by Almgren and Chriss (2001) among many other variants.\nCui et al. (2022) provided a comprehensive survey on the multi-period mean–variance\nportfolio selection model.\nThe classical stochastic control approach for solving portfolio optimization across\nmultiple assets requires representations of the dynamics of individual assets and their\nco-movements, which are difficult to describe and estimate accurately. Applications of\nreinforcement learning to finance, such as high-frequency trading and portfolio manage-\nment, have attracted much attention in recent years. RL describes methods by which\nagents learn to make optimal decisions through interacting with the environment. Ham-\nbly et al. (2023) provided a comprehensive review of the recent advances in reinforcement\nlearning in finance and discussed in detail the applications of these RL algorithms in\nvarious decision-making problems in finance.\nThe portfolio optimization problem can be reformulated as a discrete-time Partially\nObservable Markov Decision Process (POMDP). Maringer and Ramtohul (2012) and\nMaringer and Zhang (2014) presented the regime-switching recurrent reinforcement learn-\ning model (RSRRL), a combined technique of statistical modeling and machine learning,\nto tackle investment problems. Asiain et al. (2018) provided a reinforcement learning\nframework for computing the mean-variance customer portfolio in POMDPs with trans-\naction costs. Liu et al. (2020) formulated the quantitative trading process as a POMDP\nand presented a model based on deep reinforcement learning and imitation learning tech-\nniques. Mannor and Tsitsiklis (2013) concluded that seeking the global optimum for MDP\nproblems under the MV criterion is computationally challenging. The computational com-\nplexity can be attributed to two reasons: the absence of a principle of optimality leading\nto simple recursive algorithms and the variance, which is not a linear function of the\nprobability measure of the underlying process. According to Sato (2019), in the portfolio\n2optimization, neither the future returns of investments nor the state transition probabil-\nities are known. Thus, model-free RL methods can be applied to the problem because\none can solve a Bellman optimality equation approximately without understanding the\nunderlying dynamics but relying solely on the sample data.\nModel-free RL methods can be divided into tw"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "MotifPiece: A Data-Driven Approach for Effective\nMotif Extraction and Molecular Representation\nLearning\nZhaoning Yu1and Hongyang Gao1*\n1*Department of Computer Science, Iowa State University, 2434 Osborn\nDr, Ames, 50011, Iowa, United States.\n*Corresponding author(s). E-mail(s): hygao@iastate.edu;\nContributing authors: znyu@iastate.edu;\nAbstract\nMotif extraction is an important task in motif based molecular representation\nlearning. Previously, machine learning approaches employing either rule-based\nor string-based techniques to extract motifs. Rule-based approaches may extract\nmotifs that aren’t frequent or prevalent within the molecular data, which\ncan lead to an incomplete understanding of essential structural patterns in\nmolecules. String-based methods often lose the topological information inher-\nent in molecules. This can be a significant drawback because topology plays a\nvital role in defining the spatial arrangement and connectivity of atoms within a\nmolecule, which can be critical for understanding its properties and behavior. In\nthis paper, we develop a data-driven motif extraction technique known as Motif-\nPiece, which employs statistical measures to define motifs. To comprehensively\nevaluate the effectiveness of MotifPiece, we introduce a heterogeneous learning\nmodule. Our model shows an improvement compared to previously reported mod-\nels. Additionally, we demonstrate that its performance can be further enhanced\nin two ways: first, by incorporating more data to aid in generating a richer motif\nvocabulary, and second, by merging multiple datasets that share enough motifs,\nallowing for cross-dataset learning.\nKeywords: Motif extraction, Molecular representation learning, Graph Neural\nNetworks, Heterogeneous motif graph\n1arXiv:2312.15387v1  [q-bio.QM]  24 Dec 20231 Introduction\nGraph neural networks (GNNs) have showcased their proficiency in tackling various\nintricate tasks in the field of molecular property prediction. These tasks encom-\npass activities such as classifying nodes within molecular graphs [14], distinguishing\nbetween different molecular structures [16, 23, 27], and predicting intermolecular inter-\nactions [10, 22, 24]. Instead of manually designing specific features, GNNs transform\na molecular graph into a high-dimensional Euclidean space by leveraging the inherent\ntopological relationships among its constituent nodes [21].\nTraditional GNNs models primarily rely on the fundamental topology of molecu-\nlar graphs to extract structural information [9, 14, 17, 25, 30]. This is accomplished\nthrough techniques such as neighborhood feature aggregation and pooling methods.\nWhile these methods are powerful in capturing local structural features within indi-\nvidual molecular graphs, they typically do not explicitly focus on learning recurring\nmotif patterns that span across different molecular graphs.\nIn molecular chemistry, a motif, also termed as a functional group, stands for a\nunique collection of atoms that are chemically bonded together in a consistent and\nrepeating pattern [2, 6, 7, 33, 34]. This pattern can occur across various molecules.\nThe distinguishing properties and chemical behavior of a molecule are significantly\ninfluenced by its motifs [1, 4, 29], as they represent the crucial reactive elements\nwithin the molecule. Over recent years, a growing body of scientific work has been\ndedicated to identify and incorporate the structural motifs in the learning of molecular\nrepresentations [8, 28, 31–33].\nThe process of extracting motifs is an integral component in the motif-related\nstudy. The capability of a model to effectively gather valuable motif information mainly\ndepends on how well this extraction process works. If the extraction isn’t done correctly\nor accurately, a model might have trouble understanding and using the motif data\neffectively. Therefore, the model’s success in handling motif information largely relies\non how well the motif extraction process is executed. Previouos works employ either\nrule-based or string-based techniques to extract motifs. Rule-based approaches are\nbased on domain knowledge and may extract motifs that aren’t frequent or prevalent\nwithin the molecular data. This can lead to an incomplete understanding of essential\nstructural patterns in molecules. String-based methods use a string to represent a\nmolecule and directly apply Natural Language Learning method to extract motifs. It\nmay ignore the topological information inherent in molecules. This can be a significant\ndrawback because topology plays a vital role in defining the spatial arrangement and\nconnectivity of atoms within a molecule, which can be critical for understanding its\nproperties and behavior.\nIn this paper, we introduce a novel method called MotifPiece for extracting motifs\nfrom molecular data. MotifPiece is a data-driven approach that can adapt to the\nunique characteristics and patterns present in a group of molecules. To comprehen-\nsively evaluate existing motif extraction methods and"}
{"date": "2023-12-27-22-36", "error": false, "url": "PDF", "text_blocks": "Zero-Inflated Bandits\nHaoyu Wei *1Runzhe Wan *2Lei Shi2Rui Song2\nAbstract\nMany real applications of bandits have sparse\nnon-zero rewards, leading to slow learning rates.\nA careful distribution modeling that utilizes\nproblem-specific structures is known as critical\nto estimation efficiency in the statistics litera-\nture, yet is under-explored in bandits. To fill the\ngap, we initiate the study of zero-inflated bandits,\nwhere the reward is modeled as a classic semi-\nparametric distribution called zero-inflated distri-\nbution. We carefully design Upper Confidence\nBound (UCB) and Thompson Sampling (TS) al-\ngorithms for this specific structure. Our algo-\nrithms are suitable for a very general class of re-\nward distributions, operating under tail assump-\ntions that are considerably less stringent than\nthe typical sub-Gaussian requirements. Theoret-\nically, we derive the regret bounds for both the\nUCB and TS algorithms for multi-armed bandit,\nshowing that they can achieve rate-optimal regret\nwhen the reward distribution is sub-Gaussian.\nThe superior empirical performance of the pro-\nposed methods is shown via extensive numerical\nstudies.\n1. Introduction\nThe bandit problem has received increasing attention and\nhas been widely applied to areas such as clinical trials [19],\nfinance [38], recommendation systems [58], among oth-\ners. Accurate uncertainty quantification is the key to ad-\ndress the exploration-exploitation trade-off and typically\nrequires certain assumptions regarding the reward distribu-\ntion, which can be roughly divided into two groups:\n• Parametric: the reward distribution is assumed to be-\nlong to a parameterized family, such as Gaussian or\nBernoulli distributions [6, 28, 3]. The strong assump-\ntion ensures that the design and theoretical analysis\n1Department of Economics, University of California San\nDiego2Amazon. This work does not relate to the positions at\nAmazon . Correspondence to: Haoyu Wei <h8wei@ucsd.edu >.\n*: Equal contribution.can be expressed in closed form. However, in real-\nworld scenarios, there is no definitive evidence to con-\nfirm a specific distribution of rewards, as indicated by\nempirical studies [13]. The misspecification may lead\nto over- or under-exploration.\n• Non-parametric: the reward distributions need only\nto satisfy certain characteristics, such as being sub-\nGaussian [16, 23, 59] or bounded [32, 31, 25]. In\nsome instances, an appropriate non-parametric ap-\nproach can achieve a regret rate comparable to that\nof parametric methods [42, 25, 23]. However, These\nweaker assumptions are more general, yet sacrificing\ncertain statistical efficiency from ignoring prior struc-\nture information. For example, even if the rates are\nthe same, empirical performance may still exhibit a\nsignificant gap compared to the parametric approach\nwhen rewards are correctly specified in both method-\nologies [34].\nAs usual, a careful distribution modeling that utilizes\nproblem-specific features can improve the learning effi-\nciency, and in bandits, lead to lower regrets. However,\ncompared to the rich statistical literature on univariate dis-\ntribution, this direction is underexplored in bandits.\nTowards bridging this gap, this paper initiates the study of\nthis direction by focusing on the sparse reward problem.\nSpecifically, this work is motivated by the observation that\nin many real-world applications, rewards tend to be sparse,\nmeaning they are zero (or a constant) in most instances. For\ninstance, in online advertising, the majority of customers\ndo not click on advertisements, resulting in a zero reward\nin most cases; however, for those instances where there is\na click, the reward follows a specific distribution. Simi-\nlar patterns are observed in a wide range of applications,\nincluding mobile health [35] and freemium games [54].\nWhile some standard bandit algorithms can still be applied,\nthey fail to utilize the distribution property and hence can\nbe less efficient.\nTo develop efficient bandit algorithms tailored for these ap-\nplications, this paper introduces the Zero-Inflated Bandits\n(ZIB) problem. Here, the reward distribution is modeled\nas a mixture of two distributions: one being a delta dis-\ntribution at zero, and the other requiring only minimal as-\nsumptions. This makes our model semi-parametric , known\n1arXiv:2312.15595v1  [stat.ML]  25 Dec 2023Zero-Inflated Bandits\nfor its flexibility and capability to integrate valuable struc-\ntural information. We provide an extensive analysis of\nthe ZIB problem across various setups. In the realm of\nmulti-armed bandits, we propose both Upper-Confidence\nBound (UCB)-type and Thompson Sampling (TS)-type al-\ngorithms, designed for rewards with diverse tail behav-\niors. Similarly, for contextual bandits, we also develop\nboth UCB-type and TS-type algorithms, which are read-\nily adaptable under different generalizable model assump-\ntions.\nMain Contributions. The contributions of this paper are\nthree-fold. First, we propose a general design framework\nfor UC"}
