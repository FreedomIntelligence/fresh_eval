{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Implied volatility (also) is path-dependent\nHervé Andrès1,2, Alexandre Boumezoued1, and Benjamin Jourdain2\n1Milliman R&D, Paris, France\n2CERMICS, École des Ponts, INRIA, Marne-la-Vallée, France.\nDecember 27, 2023\nAbstract\nWe propose a new model for the coherent forecasting of both the implied volatility\nsurfaces and the underlying asset returns. In the spirit of Guyon and Lekeufack (2023) who\nare interested in the dependence of volatility indices (e.g. the VIX) on the paths of the\nassociated equity indices (e.g. the S&P 500), we first study how implied volatility can be\npredicted using the past trajectory of the underlying asset price. Our empirical study reveals\nthat a large part of the movements of the at-the-money-forward implied volatility for up to\ntwo years maturities can be explained using the past returns and their squares. Moreover,\nwe show that up to four years of the past evolution of the underlying price should be used\nfor the prediction and that this feedback effect gets weaker when the maturity increases.\nBuilding on this new stylized fact, we fit to historical data a parsimonious version of the\nSSVIparameterization(GatheralandJacquier, 2014)oftheimpliedvolatilitysurfacerelying\non only four parameters and show that the two parameters ruling the at-the-money-forward\nimplied volatility as a function of the maturity exhibit a path-dependent behavior with\nrespect to the underlying asset price. Finally, we propose a model for the joint dynamics\nof the implied volatility surface and the underlying asset price. The latter is modelled\nusing a variant of the path-dependent volatility model of Guyon and Lekeufack and the\nformer is obtained by adding a feedback effect of the underlying asset price onto the two\nparameters ruling the at-the-money-forward implied volatility in the parsimonious SSVI\nparameterization and by specifying a hidden semi-Markov diffusion model for the residuals\nof these two parameters and the two other parameters. Thanks to this model, we are able\nto simulate highly realistic paths of implied volatility surfaces that are arbitrage-free.\nKeywords: implied volatility modelling, SSVI, path-dependent volatility\n1. Introduction\nOne of the many reasons of the success of the Black-Scholes model (Black and Scholes, 1973) is the\nexistence of a one-to-one correspondence between the price C(K, T)of an European call option with\nstrike Kand maturity Tand the volatility σof the geometric Brownian motion modelling the dynamics\nof the underlying asset price (St)t≥0provided that (S0−Ke−rT)+< C(K, T)< S 0(ris the constant\nrisk-free rate) which is guaranteed by absence of arbitrage opportunities. When this condition is satis-\nfied, the unique parameter σsatisfying CBS(K, T, σ ) =C(K, T), where CBSdenotes the Black-Scholes\ncall option price, is called the implied volatility of the call option. By the put-call parity, the implied\nvolatility of the put option is equal to the one of the call option with same maturity and strike. Although\nthe implied volatility does not add any new information with respect to the option price, it is commonly\nused to quote option prices on the markets mainly because it allows to easily compare the value of two\noptions with different underlying assets while the option price heavily depends on the underlying asset\n1arXiv:2312.15950v1  [q-fin.CP]  26 Dec 2023price level, making the comparison more difficult. If the Black-Scholes model was an accurate description\nof financial markets, the implied volatility should be the same for all options on a given asset regardless\nof the maturity and the strike. The computation of the implied volatility from market option prices\nshows that the implied volatility actually depends on the maturity and the strike which invalidates the\nBlack-Scholes model. The so-called implied volatility surface (IVS) (K, T)7→σBS(K, T)permits to fully\ndescribe the option prices on a given asset.\nIt is also well-known that the level and the shape of the IVS varies with time. To be able to jointly\nmodel the time evolution of the IVS and the underlying asset price is key for applications covering asset\nallocation, risk management and hedging. First, such a model allows to backtest or study the P&L\ndistribution of an investment stragegy involving options and the underlying asset. One can think for\nexample of the strategy consisting in buying a stock and a put of strike K1and selling a put of strike\nK2with K2< K 1but with same maturity (this is called a put spread). This strategy protects the\ninvestor against a drop in the underlying asset price down to the K2threshold in exchange to a lower\npremium in comparison to just buying a put of strike K1. By extension, the modelling of the IVS and\nthe underlying asset price makes it possible to optimize an asset allocation strategy involving options.\nAnother application relates to the design and the backtesting of hedging strategies for financial products\n(e.g. volatility swaps, options on the VIX, etc.) having a volatili"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Unsupervised Learning of Phylogenetic Trees via\nSplit-Weight Embedding\nYibo Kong\nDepartment of Computer Science\nUniversity of Wisconsin-Madison\nMadison, WI 53706George P. Tiley\nKew Royal Botanic Gardens\nKew, Richmond, TW9 3AE,\nLondon, UK\nClaudia Sol´ ıs-Lemus∗\nWisconsin Institute for Discovery\nDepartment of Plant Pathology\nUniversity of Wisconsin-Madison\nMadison, WI 53706\nJanuary 2023\nAbstract\nUnsupervised learning has become a staple in classical machine learning, successfully identi-\nfying clustering patterns in data across a broad range of domain applications. Surprisingly,\ndespite its accuracy and elegant simplicity, unsupervised learning has not been sufficiently\nexploited in the realm of phylogenetic tree inference. The main reason for the delay in adop-\ntion of unsupervised learning in phylogenetics is the lack of a meaningful, yet simple, way of\nembedding phylogenetic trees into a vector space. Here, we propose the simple yet power-\nful split-weight embedding which allows us to fit standard clustering algorithms to the space\nof phylogenetic trees. We show that our split-weight embedded clustering is able to recover\nmeaningful evolutionary relationships in simulated and real ( Adansonia baobabs) data.\n1 Introduction\nThe Tree of Life is a massive graphical structure which represents the evolutionary process from single\ncell organisms into the immense biodiversity of living species in present time. Estimating the Tree of Life\nwould not only represent the greatest accomplishment in evolutionary biology and systematics, but it would\nalso allow us to fully understand the development and evolution of important biological traits in nature, in\nparticular, those related to resilience to extinction when exposed to environmental threats such as climate\nchange. Therefore, the development of statistical and machine-learning theory to reconstruct the Tree of Life,\nespecially those scalable to big data, are paramount in evolutionary biology, systematics, and conservation\nefforts against mass extinctions.\nGraphical structures that represent evolutionary processes are denoted phylogenetic trees . A phylogenetic\ntree is a binary tree whose internal nodes represent ancestral species that over time differentiate into two\nseparate species giving rise to its two children nodes (see Figure 1 left). The evolutionary process is then\ndepicted by this bifurcating tree from the root (the origin of life) to the external nodes of the tree (also\ndenoted leaves) which represent the living organisms today. Mathematically, a rooted phylogenetic tree Ton\ntaxon set Xis a connected directed acyclic graph with vertices V={r} ∪VL∪VT, edges Eand a bijective\nleaf-labeling function f:VL→Xsuch that the root rhas indegree 0 and outdegree 2; any leaf v∈VLhas\nindegree 1 and outdegree 0, and any internal node v∈VThas indegree 1 and outdegree 2. An unrooted tree\n∗Corresponding author: solislemus@wisc.edu\n1arXiv:2312.16074v1  [q-bio.PE]  26 Dec 2023results from the removal of the root node rand the merging of the two edges leading to the outgroup (taxon\n4 in Figure 1 left). Traditionally, phylogenetic trees are drawn without nodes (Figure 1 center) given that\nonly the bifurcating pattern is necessary to understand the evolutionary process. The specific bifurcating\npattern (without edge weights) is denoted the tree topology. Edges in the tree have weight we∈(0,∞) that\ncan represent different units, evolutionary time or expected substitutions per site being the most common.\nFigure 1: Left: Phylogenetic tree in 4 taxa. Internal (gray) nodes represent speciation events in which an\nancestral species differentiates into two. External (blue) nodes, also denoted leaves, represent living species\n(here denoted 1,2,3,4). Edge weights (in gray) also denoted branch lengths can represent evolutionary time\nor expected substitutions per site. Center: Different phylogenetic tree on the same 4 taxa in which taxon\n2 is grouped with taxon 3 rather than with taxon 1. Nodes are no longer drawn as is the most common\nrepresentation of phylogenetic trees. Right: Phylogenetic tree with gene flow event depicted as a green\narrow. This biological scenario represents the possibility that some genes have the evolutionary history of\nthe phylogenetic tree on the left (with clade (1 ,2)) and some genes, the evolutionary history of the center\ntree (with clade (2 ,3)).\nOne of the main challenges when inferring phylogenetic trees is the fact that different genes in the data\ncan have different evolutionary histories due to biological processes such as introgression, hybridization or\nhorizontal gene transfer [33, 12, 28]. An example is depicted in Figure 1 (right) which has one gene flow\nevent drawn as a green arrow. This gene flow event represents the biological scenario in which some genes in\ntaxon 2 get transferred from the lineage of taxon 3, and thus, when reconstructing the evolutionary history\nof this group of four taxa, some genes will depict the phylogenetic tree that clusters "}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.16086v1  [q-bio.NC]  26 Dec 2023NOTES ON RETROACTIVE INTERFERENCE MODEL OF\nFORGETTING\nMikhail Katkov\nDepartment of Brain Sciences\nWeizmann Institute of Science\nRehovot, 76100 Israel\nSchool of Natural Sciences\nInstitute for Advanced Study\nPrinceton, NJ\nmikhail.katkov@gmail.com\nABSTRACT\nWe present analytical derivation of the minimal and maximal number of items retained in recently\nintroduced Retroactive Interference Model of Forgetting. Also we computed the probability that two\nitems presented at different times are retained in the memor y at a later time analytically.\nKeywords retrograde interference ·Markov process ·retention curve\nIntroduction\nStudying human memory is a complex task, since there are pres umably many interacting processes that are hard to\nisolate. Traditionally models in psychology of memory are a ttempting to describe many processes at once by creating\na very complicated mathematical model that have a lot of para meters, hard to analyse, and usually require a separate\nﬁtting of parameters for each experiment. Therefore, since parameters have to adjusted for each measurement, it is\nnot clear how good these models would describe memory proces ses outside of laboratory settings. We have recently\nproposed a different type of models to describe human memory that are based on few assumptions and have zero or\nfew parameters that describe the class of stimuli, but indep endent of experimental settings. These types of models,\nif validated, have a much broader applicability in everyday life settings. We have recently presented mathematical\nmodels for memory forgetting and retrieval [1]. In this publ ication we have asked about mathematical properties of\nforgetting model that may help design experiments to valida te the underlying mathematical construction. In this note\nwe provide answers to 2 questions raised in that publication regarding the forgetting model.\nExperimentally, forgetting is traditionally measured in t he form of retention function ( RC(t)) - the probability that\nmemory is retained for time tsince acquisition[2]. People observe that retention funct ion monotonically decreasing\nwith time and although there are debates on the form of retent ion function one of the best candidate is power function\nof time. One of the popular explanation of memory forgetting in humans is retrograde interference [3]. It assumes\nthat new incoming memories are interacting with stored memo ries and cause some past memories disappear. There\nare different approaches to model forgetting (see for examp le [4]), but here we are concentrating on consequences of\nour mathematical model [5] for possible experimental valid ating of underlying assumptions behind our model. The\nmodel assumes that each incoming memory (one memory at one di screte time step) has ndimensional valence vector,\nwith components being iid random variables. Every incoming memory is added to the memory pool, erasing all\nstored memories that have smaller valences in all dimension s. This model can be solved analytically, and the resulting\nretention curve agree well with experiment. Nevertheless, it is not clear to what extent the underlying principles hold s\nduring retention of memory items. We have posed recently sev eral mathematical questions that may provide additional\nexperimental tests related to this issue[1].Forgetting Calculations\nWe consider the forgetting model III from [1]. It states that there is a retention process, where at each time step a\nnew memory is presented to the process. Memory in the model is characterized by a vector of valences vk∈R,k=\n1..n, wherenis a single integer parameter of the model representing the d imensionality of the model. Valences of\nincoming memory is assumed to be sampled from arbitrary stat ionary distribution (absolutely continuous). Incoming\nmemory erases all currently stored memories that have all va lences smaller that corresponding valencies of incoming\nmemory. Finally, incoming memory is added to stored memorie s. Formally, all incoming memories are described by\ncollection of valences V={vk,t∈R,k= 1..n,t∈N}. For each time Twe can deﬁne a set of stored memories\nM(T;V) ={t1,...t|M(T)|},tk< T which contains presentation times ( t1,...t|M(T)|) of stored memories, where the\ncardinality of M(T;V)(|M(T;V)|) represents a number of stored memories at time T. At time T+1a new memory\nwith valences vk,T+1is presented, and the set of stored memories is updated M(T+ 1;V) ={T+ 1;tm:tm∈\nM(T;V),∃k|vk,tm> vk,T+1}. One can ask what is expected value of |M(T)|that is referred to as retention curve\nRCn(T) =E(|M(T;V)|)with respect to distribution of vk,t. It turns out that retention curve does not depend on the\nparticular distribution of valences, since it depends only on the order statistics of memory items in each dimension[5,\n1].\nMinimal number of retained items\nWe proposed two kind of tests that can potentially check the v alidity of the model [1]. Both are related to partialy\nordered set (poset) nature o"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "1 \n Expanding to Arbitrary Study Designs: ANOVA to Estimate Limits of \nAgreement for MRMC Studies  \nSi Wena* and Brandon D. Gallasa \naDivision of Imaging, Diagnostics, and Software Reliability, Office of Science and Engineering \nLaboratories, Center for Devices and Radiological Health, U.S. FDA, Silver Spring, USA  \n* Correspondence to: Si Wen. Email: si.wen@fda.hhs.gov \n  2 \n Expanding to Arbitrary Study Designs: ANOVA to Estimate Limits of \nAgreement for MRMC Studies  \nA multi-reader multi -case (MRMC) analysis i s applied to account for both reader and case \nvariability when evaluating the clinical performance of a medical imaging device or reader \nperformance under different reading modalities. For a clinical task that measures a \nquantitative biomarker an agreement  analysis, such as limits of agreement (LOA), can be \nused. In this work, we decompose the total variation in the data using a three -way mixed \neffect ANOVA model to estimate the MRMC variance of individual differences and the \nLOA between different reading m odalities. There are rules for writing down the \nexpectation of the mean squares in terms of the variance components for fully -crossed data, \ni.e. data where all the readers read all the cases in all modalities being studied. Sometimes \nthe annotation task is  labor -intensive and time -consuming or distributed across sites, so that \na fully -crossed study is not practical. In this work, we focus on estimating the MRMC \nvariance in the within - and between -readers and within - and between -modalities LOA for \nan arbitrary study design. Simulation studies were conducted to validate the LOA variance \nestimates. The method was also applied to a dataset to compare pathologist performance for assessing the density of stromal tumor infiltrating lymphocytes on different platform s. \nKeywords: limits of agreement ; MRMC study ; ANOVA for unbalanced data;  variance \ncomponents  \n1.Introduction  \nA multi -reader multi -case (MRMC)  study is usually appl ied to evaluate whether a medical \nimaging device can improve the clinical performance of the image readers.(Wagner et al. 2007; \nGallas et al. 2012; Obuc howski and Bullen 2022)  In the study, qualitative or quantitative \nassessment s are collected and compared from multiple readers  (radiologists/ pathologists) \nreviewing multiple  cases  (images)  under different reading modalities . For example,  suppose that \nthere is an AI/ML algorithm enabled medical device supporting the pathologist s to evaluate the \ndensity of stromal tumor infiltrating lymphocytes  (sTIL s) on digital slides . The density of sTILs  3 \n is prognostic for survival in breast cancers and  is visual ly assessed  on routine hematoxylin and \neosin (H&E)- stained slides.(Kos et al. 2020)  To validate the performance of the device  in the \nhands of the reader s we collect estimates of the density of sTILs from  pathologists with and \nwithout the assistance of the de vice. Then  we compare the closeness or agreement between  the \nquantitative  measurements , sTIL scores, from different modalities  or readers . We refer to such \nstudies as agreement studies.  \nOur p revious work(Wen and Gallas 2022)  generalized the limits of agreement (LOA) \nmethod (Bland and Altman 1986, 1999) , a widely used agreement analysis, to MRMC analysis \nthat account s for reader and case variability for fully -crossed study designs . In a fully -crossed \nstudy, all the readers provide  measurements for  all the cases for both modalities  as shown in  \nFigure 1.e . In some studies,  the annotation tasks  are labor -intensive and time -consuming or \ndistributed across sites, so that a fully -crossed study is not practical. In this work, we focus on \nMRMC stu dies that are not fully crossed , and we generalize the MRMC analysis to treat  \narbitrary study designs . Some examples are described  in Figure 1.a -Figure 1.d.  \nAn ANOVA model is commonly used to analyze MRMC data,(Beiden et al. 2000; \nDorfman et al. 1992; Gallas et al. 2009; Obuchowski 1995)  as the data are likely correlated when \nthe readers evaluat e the same set of cases and each case is reviewed by multiple readers. A  three -\nway mixed effect ANOVA is  used to estimate different types of LOA for a fully -crossed MRMC \nstudy.(Wen and Gallas 2022)  In ANOVA, th e data generated from a fully -crossed MRMC study \nare called balanced data , as each reader or case has the same number of readings . There are rul es \nfor calculating the sum of squares (SS ) and expected mean squares (MS)  when the data are \nbalanced .(Montgomery 2012)  The expected mean squares link the variance components for the \nrandom effects in the ANOVA model to the SS computed from the data . Then, the variance 4 \n components can be estimated from the data and used to estimate the MRMC variance in  LOA.  \nHowever, in an arbitrary MRMC study, t he data are unbalanced. To determine the relationship \nbetween expected MS  and the variance components in the ANOVA model, we fi"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Randomized Signature Methods in Optimal Portfolio Selection\nErdin¸ c Akyildirima,b, Matteo Gambaraa, Josef Teichmanna, Syang Zhoua\naDepartment of Mathematics, ETH, Zurich, Switzerland\nbDepartment of Banking and Finance, University of Zurich, Zurich, Switzerland\nAbstract\nWe present convincing empirical results on the application of Randomized Signature Methods\nfor non-linear, non-parametric drift estimation for a multi-variate financial market. Even\nthough drift estimation is notoriously ill defined due to small signal to noise ratio, one can still\ntry to learn optimal non-linear maps from data to future returns for the purposes of portfolio\noptimization. Randomized Signatures, in constrast to classical signatures, allow for high\ndimensional market dimension and provide features on the same scale. We do not contribute\nto the theory of Randomized Signatures here, but rather present our empirical findings on\nportfolio selection in real world settings including real market data and transaction costs.\nKeywords: Machine Learning, Randomized Signature, Drift estimation, Returns forecast,\nPortfolio Optimization, Path-dependent Signal\nJEL: C21, C22, G11, G14, G17\n1. Introduction\nOptimal portfolio construction is one of the most fundamental problems in quantitative\nfinance. It refers to selecting and allocating assets to achieve a balance between risk and\nreturn. An optimal portfolio aligns with an investor’s specific objectives, risk tolerance, and\ntime horizon. In that sense, optimal implies achieving the best trade-off between expected\nreturn and risk based on the fact that different investors will have different optimal portfolios\ndepending on their unique goals and risk tolerances. Notice that a priori neither the precise\noptimization problem nor the underlying model for the evolution of the market are known\nto the investor. The former needs a quantification of risk tolerance, time horizon, the latter\nneeds an estimation of model parameters.\nThere are several fundamental methods for constructing an optimal portfolio. Mod-\nern Portfolio Theory (MPT) which is developed by Harry Markowitz in his seminal work\nEmail addresses: erdinc.akyildirim@bf.uzh.ch (Erdin¸ c Akyildirim), matteo.gambara@gmail.com\n(Matteo Gambara), josef.teichmann@math.ethz.ch (Josef Teichmann), syang.zhou@math.ethz.ch\n(Syang Zhou)arXiv:2312.16448v1  [q-fin.PM]  27 Dec 2023(Markovitz, 1959) provides a foundational approach given a model. MPT uses mean-variance\noptimization to construct the portfolio that maximizes the expected return while minimizing\nthe associated risk (expressed in terms of variance or standard deviation), or, equivalently,\nminimizes the risk for a given level of expected return.\nThe Capital Asset Pricing Model (CAPM) (Treynor, 1961a,b; Sharpe, 1964; Lintner,\n1975; Mossin, 1966) is another cornerstone in portfolio management which helps investors\ndetermine the expected return of an investment, particularly for individual stocks or portfo-\nlios of stocks. The Capital Market Line (CML) and Security Market Line (SML) represent\nportfolios derived from MPT. The former shows the relationship between risk and return\nfor a portfolio of all possible investments and the latter illustrates the relationship between\nthe expected return and the systematic risk of an individual asset or portfolio of assets.\nAnother method called the Maximum Sharpe Ratio Portfolio finds the optimal weights by\nmaximizing the portfolio’s Sharpe ratio, where the Sharpe ratio measures excess portfolio\nreturn over the risk-free rate relative to its standard deviation.\nAs an enhancement to the traditional MPT, the Black-Litterman model (Black and\nLitterman, 1992) was developed to incorporate investors’ subjective views and market equi-\nlibrium returns into optimal asset allocation in a portfolio. Similarly, factor-based portfolio\noptimization is used to improve CAPM by systematically considering factors that are believed\nto affect asset returns. Recently, Auh and Cho (2023) show that a parsimonious factor model\nmitigates idiosyncratic noise in historical data for portfolio optimization. They also prove\nthat a combination of the factor model and forward-looking returns improves out-of-sample\nperformance. As opposed to the Black-Litterman model, Risk-Parity distributes portfolio\nrisk equally across assets and does not look at investor views or expected return projections.\nEspecially after the Global Financial Crisis in 2008, Risk-Parity became a widely followed\nstrategy (Roncalli and Weisang, 2016; Costa and Kwon, 2019; Bai et al., 2016).\nAnother recent yet classical approach for optimal portfolio allocation is using Monte Carlo\nsimulations. This involves generating a large number of random scenarios to model the range\nof possible future returns for different assets. Of course this again is based on a given model.\nBy repeatedly simulating portfolio performance under various market conditions, investors\ncan assess the distribution of potential outcomes and make informe"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "T cell receptor binding prediction: A machine\nlearning revolution\nAnna Weber * Aurélien Pélissier†María Rodríguez Martínez∗‡\nDecember 29, 2023\nAbstract\nRecent advancements in immune sequencing and experimental techniques are generat-\ning extensive T cell receptor (TCR) repertoire data, enabling the development of mod-\nels to predict TCR binding specificity. Despite the computational challenges due to\nthe vast diversity of TCRs and epitopes, significant progress has been made. This pa-\nper discusses the evolution of the computational models developed for this task, with\na focus on machine learning efforts, including the early unsupervised clustering ap-\nproaches, supervised models, and the more recent applications of Protein Language\nModels (PLMs). We critically assess the most prominent models in each category, and\ndiscuss recurrent challenges, such as the lack of generalization to new epitopes, dataset\nbiases, and biases in the validation design of the models.\nFurthermore, our paper discusses the transformative role of transformer-based pro-\ntein models in bioinformatics. These models, pretrained on extensive collections of\nunlabeled protein sequences, can convert amino acid sequences into vectorized em-\nbeddings that capture important biological properties. We discuss recent attempts to\nleverage PLMs to deliver very competitive performances in TCR-related tasks. Finally,\nwe address the pressing need for improved interpretability in these often opaque mod-\nels, proposing strategies to amplify their impact in the field.\nKeyword : Machine Learning; T cell Receptor; Specificity Prediction; Protein Lan-\nguage Models; Interpretability.\n*IBM Research Europe (Switzerland).\n†ZHAW, Life Sciences und Facility Management (Switzerland).\n‡Correspondence to maria.rodriguezmartinez@yale.edu.\n1arXiv:2312.16594v1  [q-bio.QM]  27 Dec 20231 Background\nT cells are an essential component of the adaptive immune system, due to their ability\nto orchestrate targeted, effective immune responses through cell-based and cytokine-\nrelease mechanisms. While T cell functions are diverse, their activation, differentiation,\nproliferation, and function are all governed by their T cell receptors (TCR), which\nenable them to recognize non-self antigens arising from infectious agents or diseased\ncells [1].\nTo face a diverse and ever-evolving array of antigens, the immune system has\nevolved the capability to generate a huge array of distinct TCRs. This diversity is\nachieved through a random process of DNA rearrangement, which involves the recom-\nbination of the germline V , D, and J gene segments and the deletion and insertion of\nnucleotides at the V(D)J junctions. While the theoretical diversity of different TCRs\nis estimated to be as high as 1019[2], the realized diversity in an individual is much\nsmaller, typically ranging between 106and 1010[3].\nAt the molecular level, TCRs interact with peptides presented on the major histo-\ncompatibility complex (MHC), a complex commonly referred to as pMHC. Although\nthe interaction between pMHC and TCR is highly specific, a single TCR can often rec-\nognize multiple pMHC complexes. Indeed, some TCRs have been shown to recognize\nup to a million different epitopes [4]. This multivalency is necessary to ensure that\nthe realized diversity in one individual can recognize a significantly broader array of\npotential antigens.\n2 T cell receptor specificity prediction.\nThe precise prediction of TCR-pMHC binding is essential for accurately quantifying\nand predicting immune responses. If effectively represented, it has the potential to\ntransform the field of personalized medicine. For instance, the accurate identification\nof epitopes recognized by expanded TCR clones can aid in identifying the auto-antigens\ninvolved in T-cell-associated autoimmune diseases [5], assessing responses to vaccines\nor identifying the pathogenic agents responsible for eliciting T-cell responses [6]. In\nthe context of cancer, an improved predictive power of TCR specificity can not only\naid in the design of more effective T cell-based therapies [7], but also minimize toxic\nside-effects produced by TCRs off-target binding [8].\nHowever, as experimental methods cannot encompass the vast space of potential\nTCRs and epitopes, significant emphasis has been placed on the development of reliable\ncomputational methods to predict TCR specificity. Existing methods can accurately\nclassify in-distribution samples, i.e. they can predict TCR binding to epitopes already\nencountered by the model [9]. However, the pivotal challenge is to develop models\nwith the capacity to generalize to novel epitopes. A major challenge stems from the\nscarcity of datasets containing experimentally validated TCR-epitope interactions, and\nin particular, regarding the diversity of sampled epitopes.\n23 Limitations of available datasets.\nTCR specificity data can be collected from various databases, such as the VDJdb [10],\nwith over ∼70,000 TCR sequences and ∼1100 different epitopes "}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "scRNA-seq Data Clustering by Cluster-aware Iterative\nContrastive Learning\nWeikang Jiang1∗, Jinxian Wang1∗, Jihong Guan2, Shuigeng Zhou1,†\n1Shanghai Key Lab of Intelligent Information Processing,\nand School of Computer Science, Fudan University\n2Department of Computer Science and Technology, Tongji University\n{sgzhou, 20210240030}@fudan.edu.cn\nwangjx22@m.fudan.edu.cn, jhguan@tongji.edu.cn\nAbstract\nSingle-cell RNA sequencing (scRNA-seq) enables researchers to analyze gene\nexpression at single-cell level. One important task in scRNA-seq data analysis is\nunsupervised clustering, which helps identify distinct cell types, laying down the\nfoundation for other downstream analysis tasks. In this paper, we propose a novel\nmethod called Cluster-aware Iterative Contrastive Learning (CICL in short) for\nscRNA-seq data clustering, which utilizes an iterative representation learning and\nclustering framework to progressively learn the clustering structure of scRNA-seq\ndata with a cluster-aware contrastive loss. CICL consists of a Transformer encoder,\na clustering head, a projection head and a contrastive loss module. First, CICL\nextracts the feature vectors of the original and augmented data by the Transformer-\nencoder. Then, it computes the clustering centroids by K-means and employs the\nstudent’s t-distribution to assign pseudo-labels to all cells in the clustering head.\nThe projection-head uses a Multi-Layer Perceptron (MLP) to obtain projections\nof the augmented data. At last, both pseudo-labels and projections are used in the\ncontrastive loss to guide the model training. Such a process goes iteratively so that\nthe clustering result becomes better and better. Extensive experiments on 25 real-\nworld scRNA-seq datasets show that CICL outperforms the state-of-the-art (SOTA)\nmethods. Concretely, CICL surpasses the existing methods by from 14% to 280%,\nand from 5% to 133% on average in terms of performance metrics ARI and NMI\nrespectively. Source code is available at https://github.com/Alunethy/CICL.\n1 Introduction\nEach cell possesses unique characteristics and biological functions defined by its gene transcription\nactivities. Conventional bulk RNA sequencing measures the average transcription levels of a multitude\nof cells, thereby obscuring the heterogeneity among individual cells. In the past decade, the rapid\nprogress of single-cell RNA sequencing (scRNA-seq) technologies [Tang et al., 2009] enables\ntranscriptome-wide gene expression measurement in individual cells, which greatly helps deepen\nour understanding of cellular heterogeneity and propels the research on cell biology, immunology,\nand complex diseases [van Galen et al., 2019]. Identifying cell types is a fundamental step in\nunraveling complex biological processes such as cellular differentiation, lineage commitment, and\ngene regulation [Deng et al., 2021]. As such, cell clustering becomes an important task in scRNA-seq\nanalysis. However, the inherent high-dimensionality, noise, and sparsity of scRNA-seq data present\nsevere challenges for scRNA-seq data clustering analysis [Kiselev et al., 2019, Stegle et al., 2015].\n∗Co-first authors who contribute equally to this work.\n†Corresponding author.\nPreprint. Under review.arXiv:2312.16600v1  [q-bio.GN]  27 Dec 2023Up to now, many models or algorithms have been developed for scRNA-seq data clustering. Early\nscRNA-seq clustering methods mainly rely on traditional dimensionality reduction and clustering\nmethods. For example, pcaReduce [Žurauskien ˙e and Yau, 2016] combines PCA and K-means,\niteratively merging cluster pairs based on related probability density function. Recognizing the\nimportance of similarity metrics in the clustering task, SIMLR [Wang et al., 2018a] amalgamates\nmultiple kernels to learn sample similarity and perform spectral clustering. Seurat [Satija et al., 2015]\nemploys a graph-based community detection algorithm, while Louvain [Blondel et al., 2008] is based\non the shared nearest neighbor graph to identify cell types.\nIn the past decade, with the rapid development of deep learning, deep neural networks (DNN) have\nbeen extensively applied to scRNA-seq data clustering to address the limitations of conventional\nmethods [Flores et al., 2022]. DEC [Xie et al., 2016] and IDEC [Guo et al., 2017], based on\nautoencoders (AE), use KL divergence as the clustering loss, achieving simultaneous learning of\nfeature representations and cluster assignments. To address the pervasive dropout events in scRNA-\nseq data, DCA [Eraslan et al., 2019] proposes a zero-inflated negative binomial (ZINB) model to better\ncharacterize the distribution of scRNA-seq data, and uses the negative likelihood as the reconstruction\nloss instead of the frequently-used mean-square error (MSE) loss in autoencoders. scVI [Lopez et al.,\n2018] is a deep generative model based on variational autoencoders, which can do various scRNA-seq\ndata analyses such as data imputation, clustering, and visualization. scDeepCluster [Tian et al., 2019]\nintroduce"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "DOSA-MO: Dual-stage Optimizer for Systematic overestimation\nAdjustment in Multi-Objective problems improves biomarker discovery\nLuca Cattelani∗1and Vittorio Fortino†1\n1Institute of Biomedicine, School of Medicine, University of Eastern Finland, Kuopio, 70211, Finland.\nAbstract\nThe challenge in biomarker discovery and validation using machine learning from omics data lies in the abun-\ndance of molecular features but scarcity of samples. Most machine learning-based feature selection methods ne-\ncessitate of hyperparameter tuning, typically performed by evaluating numerous alternatives on a validation set.\nEvery evaluation has a performance estimation error and when the selection takes place between many models\nthe best ones are almost certainly overestimated. Biomarker identification is a typical multi-objective problem\nwith trade-offs between the predictive ability and the parsimony in the number of molecular features. Genetic\nalgorithms are a popular tool for multi-objective optimization but they evolve numerous solutions and are prone to\noverestimation. Methods have been proposed to reduce the overestimation after a model has already been selected\nin single-objective problems, but to the best of our knowledge no algorithm existed that was capable of reducing the\noverestimation during the optimization, leading to a better model selection, or that had been applied in the more\ngeneral domain of multi-objective problems. We propose DOSA-MO, a novel multi-objective optimization wrapper\nalgorithm that learns how the original estimation, its variance, and the feature set size of the solutions predict the\noverestimation, and adjusts the expectation of the performance during the optimization, improving the composition\nof the solution set. We verify that DOSA-MO improves the performance of a state-of-the-art genetic algorithm\non left-out or external sample sets, when predicting cancer subtypes and/or patient overall survival, using three\ntranscriptomics datasets for kidney and breast cancer. Since to the best of our knowledge there was no measure\nof estimation error for multi-objective solution sets, we propose two such measures. According to both of them,\nDOSA-MO provides more realistic performance estimates in all the considered scenarios.\nData and source code availability. The gene expression data used in this study is from public repositories.\nThe source code and detailed numerical results will be available in a public server after peer review.\n∗Corresponding Author: luca.cattelani@uef.fi\n†Corresponding Author: vittorio.fortino@uef.fi\n1arXiv:2312.16624v1  [q-bio.QM]  27 Dec 20231 Introduction\nMolecular biomarker discovery with machine learning (ML)\nis usually limited by data that includes many features\nbut few samples. This renders the models prone to over-\nfitting and the evaluation prone to estimation error. Many\nML approaches involve hyperparameter tuning and fea-\nture selection, with model selection based on the perfor-\nmance measured on a validation set within a training,\nvalidation, and test paradigm. Comparing many mod-\nels and choosing the best is prone to overestimation, i.e.\nthe difference between the performance measured on the\nvalidation set and the real performance, a.k.a. “winners\ncourse”. The models that fit the noise present in the\nvalidation set are advantaged, a phenomena sometimes\nreferred as overfitting on the validation set [1]. This is\nalso exacerbated by data scarcity. In the quest to reach\nhigher accuracies, increasing the number of hyperparam-\neter configurations may lead to selected models with bet-\nter performance on the validation set, but a correspond-\ning increase in overestimation may determine diminish-\ning or even negative improvements on the test set.\nIn biomarker selection both the predictive performance\n(in terms of disease type, survivability, etc.) and the\npractical applicability are crucial factors. A biomarker\nthat includes a small number of molecular features re-\nquires less resources when applied clinically, thus might\nbe preferred over a slightly more accurate but more ex-\npensive or time consuming alternative.\nCharacterizing all the best compromises (or trade-\noffs) between predictive value and feature set size is a\nmulti-objective (MO) optimization problem [2], and it\ncan be solved by means of MO feature selection (MOFS)\ntechniques [3, 4, 5, 6]. These techniques aim to iden-\ntify not just a single best solution, as in single-objective\n(SO) problems, but rather a Pareto front of solutions.\nThis front is the set of optimal solutions that illustrate\nthe trade-offs between different objectives in MO opti-\nmization. However, all candidate solutions (or biomarker\nmodels selected by the employed MO feature selection\ntechnique) are evaluated on the validation set, which can\nresult in the overestimation of the performance of the se-\nlected models.\nK-fold cross-validation (CV) is the most common method-\nology for ML assessment and its benefits and limitations\nhave received much"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.16637v1  [q-fin.ST]  27 Dec 2023Price predictability at ultra-high frequency:\nEntropy-based randomness test\nAndrey Shternshis1,2, Stefano Marmi1\n1. Scuola Normale Superiore, Piazza dei Cavalieri 7, Pisa, I taly, 56126\n2. Uppsala University, L¨ agerhyddsv¨ agen 1, Uppsala, Swed en, 75105\nAbstract\nWe use the statistical properties of Shannon entropy estima tor and\nNeyman-Pearson statistics to study the predictability of u ltra-high fre-\nquency ﬁnancial data. We develop a statistical test for the p redictability\nof a sequence based on empirical frequencies. We study styli zed facts\nthat cause price predictability such as persistence of orde r signs, auto-\ncorrelation of returns, and volatility clustering. We show that the degree\nof randomness grows with the increase of aggregation level i n transaction\ntime. We also ﬁnd that predictable days are usually characte rized by high\ntrading activity, i.e., days with unusually high trading vo lumes and the\nnumber of price changes. We ﬁnd a group of stocks for which pre dictabil-\nity is caused by a frequent change of price direction. We perf orm multiple\ntesting for sub-intervals of days to identify whether there is predictability\nat a speciﬁc time period during the day.\nKeywords test for predictability, limit order book, ultra-high freq uency, entropy,\nempirical frequencies, Eﬃcient market hypothesis\n1 Introduction\nOne of the fundamental questions in ﬁnance is the predictabi lity of asset prices. Cur-\nrently, there is a set of tools designed to predict prices in t he markets. For instance,\ntraders rely on technical analysis of stocks to make a proﬁt. The prediction power of\ntechnical analysis has been the object of many investigatio ns, starting with the pio-\nneering work [1]. We refer to [2] for a review of academic stud ies on this subject until\n2007. For example, it has been found to generate excess proﬁt s in currency markets [3]\nincluding cryptocurrencies [4]. Proﬁtable trading strate gies can be constructed with\nthe usage of forecasting methods such as neural networks [5] . Various trading methods\ncan be developed in order to increase proﬁts starting from pr ediction of future prices\nwhich are even only slightly more accurate than a martingale . For instance, an auto-\nmatic trading method proposed in [6] outperforms a range of a lternative approaches1.\n1According to [7], abnormal returns based on price behavior i n the market decline after\nacademic publications about the proﬁtable strategies asso ciated with this behavior. It is\n1Randomness of ﬁnancial data aggregated to daily frequency w as investigated in\nthe literature [10, 11, 12]. Studies analyzing the predicta bility of intraday prices were\nconducted using hourly [13, 14] and minute [15, 16] frequenc ies. Prices at millisecond\nand second frequencies were analyzed in [17]. This research progresses further toward\namicroscopic examination of ﬁnancial time series. Ultra-h igh frequencydata is deﬁned\nas the full record of transactions and their associated char acteristics. This research is\ndedicated to the predictability of ultra-high frequency da ta.\nLong memory of price returns at ultra-high frequency is a sty lized fact that incor-\nporates predictability into the time series. Lillo and Farm er [18] conducteda statistical\ntest, concluding that both the signs of market orders and exe cuted limit orders exhibit\nlong-memory processes. They attributed this long memory of order signs to news ar-\nrivals and order splitting, which is oﬀset by ﬂuctuations in market liquidity. Therefore,\nthe long memory does not contradict eﬃciency of markets when prices incorporate all\navailable information about future values [19]. Moreover, the high speed of occurrence\nof new orders makes it diﬃcult to predict the next price befor e it appears in such\na short period of time. Without taking into account transact ion costs, we can not\nensure that predictability at ultra-high frequency indica tes the presence of proﬁtable\ntrading strategies. However, we investigate the level of pr edictability as a function of\nthe length of steps in transaction time. Then, we examine the periods in which the\npredictability of prices is presented. We explore what pric e characteristics distinguish\ndays with predictability from others. For instance, we show that the autocorrelation\nof price returns is statistically signiﬁcant during predic table days. Moreover, for most\nstocks, we observe a high probability of consecutive price d irections across several\ntransactions, aligning with the long-memory characterist ics of price return signs.\nWe devise a test for randomness of data starting from the esti mation of Shannon\nentropy. Entropy is deﬁned as an averaged measure of uncerta inty about a symbol\nappearing in a sequence generated by a random source [20]. Ma ximum uncertainty\narises when all symbols from a ﬁnite alphabet are independen tly generated with equal\nprobabilities. A common method for entropy estimation is ca lculating empirical fre-\n"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "bioRxiv 2023.12.24.XXXXX\narXiv 2312.XXXXX\nWhales in Space:\nExperiencing Aquatic Animals in Their Natural Place\nwith the Hydroambiphone\nJames P. Crutchfield,1,∗David D. Dunn,2,†and Alexandra M. Jurgens3,‡\n1Complexity Sciences Center, Physics and Astronomy Department\nUniversity of California, Davis, California 95616\n2Art & Science Laboratory\n613 C Street, Davis, California 95616\n3Inria Centre, University of Bordeaux, France\n(Dated: December 29, 2023)\nRecording the undersea three-dimensional bioacoustic sound field in real-time promises major\nbenefits to marine behavior studies. We describe a novel hydrophone array—the hydroambiphone\n(HAP)—that adapts ambisonic spatial-audio theory to sound propagation in ocean waters to realize\nmany of these benefits through spatial localization and acoustic immersion. Deploying it to monitor\nthe humpback whales ( Megaptera novaeangliae ) of southeast Alaska demonstrates that HAP recording\nprovides a qualitatively-improved experience of their undersea behaviors; revealing, for example,\nnew aspects of social coordination during bubble-net feeding. On the practical side, spatialized\nhydrophone recording greatly reduces post-field analytical and computational challenges—such\nas the “cocktail party problem” of distinguishing single sources in a complicated and crowded\nauditory environment—that are common to field recordings. On the scientific side, comparing the\nHAP’s capabilities to single-hydrophone and nonspatialized recordings yields new insights into the\nspatial information that allows animals to thrive in complex acoustic environments. Spatialized\nbioacoustics markedly improves access to the humpbacks’ undersea acoustic environment and expands\nour appreciation of their rich vocal lives.\nKeywords: marine mammals, humpback whales, undersea vocalizations, ambisonics, hydrophone array, spatial\nsound\nI. INTRODUCTION\nMarine mammals spend the bulk of their active lives\nsubmerged beneath the sea surface. Given the relatively\npoor propagation of light compared to sound in the ocean\ndepths, the world of these animals is primarily acous-\ntic. These factors greatly complicate relying solely on\nsurface observations to address the full diversity of their\nbehaviors. Fortunately, in the last decade or so scientists\ndemonstrated the substantial benefit of undersea, com-\nprehensive tracking with, for example, skillful attachment\nof digital devices that monitor animal behavior via sen-\nsors that record video, sound, location, depth, pressure,\ntemperature, and the like [ 1]. The following describes\ncomplementary benefits that come from recording the\nunderwater three-dimensional bioacoustic sound field in\nreal-time.\nA. Whale Bioacoustics\nSound propagation in water differs from that in air:\nsoundtravelsfivetimesfasterinwaterthaninair, acoustic\n∗chaos@ucdavis.edu\n†artscilab@gmail.com\n‡amjurgens@ucdavis.eduwaves in water propagate with much less dissipation, and\ndifferent frequencies travel at different speeds. (See Table\nI.) These phenomena make undersea sound markedly\nmore complex to analyze, understand, and harness. They\ncomplicate directly monitoring and interpreting sound\nin the ocean. That said, these properties also mean\nthere is additional information available in ocean acoustic\nwaves to be harnessed for environmental sensing and for\ncommunication. (See App. A.)\nTo begin to address these challenges, we applied spa-\ntial bioacoustics to monitor humpback whales ( Megaptera\nnovaeangliae ) of southeast Alaska, demonstrating that it\nmarkedly improves understanding their undersea behav-\niors. As one example, the following describes how acoustic\nspatialization revealed previously unreported aspects of\nsocial coordination during bubble-net feeding [2].\nCetaceans exhibit compelling evidence for advanced\nintentional behaviors and conscious awareness through\ntheir raw intelligence, song generation [ 3,4] and sharing\n[5,6], communication and interactions with their own\nand other species [ 7,8], and empathy (concern for others’\nwell-being) [ 9]. Humpback whales, in addition, are known\nto be very vocal and social [10].\nEvolving over a time span ten times that of humans,\ncetaceans developed tools (socially-coordinated bubble-\nnet feeding by humpbacks) and region- (and possibly\nhemisphere-) spanning ocean-acoustic communication net-\nworks [11]. Over the last half century humpback whales,arXiv:2312.16662v1  [q-bio.PE]  27 Dec 20232\nMedium Density Bulk Modulus Sound Velocity Wavelength Wavelength\n(kg/m3) (Pa) (m/s)**** 100Hz (m) **** 1000Hz (m) ****\nAir 1.225 * 1.42×105343 3.43 0.343\nFresh Water 1000 ** 2.15×1091482 14.82 1.482\nSeawater 1025 *** 2.29×1091500 15 1.5\nTABLE I: Sound propagation differences in air and water: (i) Medium density, (ii) medium bulk modulus, (iii) sound\nvelocity, and (iv) sound wavelengths at two different frequencies. Unlisted, but important is sound dispersion: the\nrange of frequency-dependent velocities is markedly large in water. *Standard atmospheric conditions (0 C or 32 F, at"}
