{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Unsupervised Learning of Phylogenetic Trees via\nSplit-Weight Embedding\nYibo Kong\nDepartment of Computer Science\nUniversity of Wisconsin-Madison\nMadison, WI 53706George P. Tiley\nKew Royal Botanic Gardens\nKew, Richmond, TW9 3AE,\nLondon, UK\nClaudia Sol´ ıs-Lemus∗\nWisconsin Institute for Discovery\nDepartment of Plant Pathology\nUniversity of Wisconsin-Madison\nMadison, WI 53706\nJanuary 2023\nAbstract\nUnsupervised learning has become a staple in classical machine learning, successfully identi-\nfying clustering patterns in data across a broad range of domain applications. Surprisingly,\ndespite its accuracy and elegant simplicity, unsupervised learning has not been sufficiently\nexploited in the realm of phylogenetic tree inference. The main reason for the delay in adop-\ntion of unsupervised learning in phylogenetics is the lack of a meaningful, yet simple, way of\nembedding phylogenetic trees into a vector space. Here, we propose the simple yet power-\nful split-weight embedding which allows us to fit standard clustering algorithms to the space\nof phylogenetic trees. We show that our split-weight embedded clustering is able to recover\nmeaningful evolutionary relationships in simulated and real ( Adansonia baobabs) data.\n1 Introduction\nThe Tree of Life is a massive graphical structure which represents the evolutionary process from single\ncell organisms into the immense biodiversity of living species in present time. Estimating the Tree of Life\nwould not only represent the greatest accomplishment in evolutionary biology and systematics, but it would\nalso allow us to fully understand the development and evolution of important biological traits in nature, in\nparticular, those related to resilience to extinction when exposed to environmental threats such as climate\nchange. Therefore, the development of statistical and machine-learning theory to reconstruct the Tree of Life,\nespecially those scalable to big data, are paramount in evolutionary biology, systematics, and conservation\nefforts against mass extinctions.\nGraphical structures that represent evolutionary processes are denoted phylogenetic trees . A phylogenetic\ntree is a binary tree whose internal nodes represent ancestral species that over time differentiate into two\nseparate species giving rise to its two children nodes (see Figure 1 left). The evolutionary process is then\ndepicted by this bifurcating tree from the root (the origin of life) to the external nodes of the tree (also\ndenoted leaves) which represent the living organisms today. Mathematically, a rooted phylogenetic tree Ton\ntaxon set Xis a connected directed acyclic graph with vertices V={r} ∪VL∪VT, edges Eand a bijective\nleaf-labeling function f:VL→Xsuch that the root rhas indegree 0 and outdegree 2; any leaf v∈VLhas\nindegree 1 and outdegree 0, and any internal node v∈VThas indegree 1 and outdegree 2. An unrooted tree\n∗Corresponding author: solislemus@wisc.edu\n1arXiv:2312.16074v1  [q-bio.PE]  26 Dec 2023results from the removal of the root node rand the merging of the two edges leading to the outgroup (taxon\n4 in Figure 1 left). Traditionally, phylogenetic trees are drawn without nodes (Figure 1 center) given that\nonly the bifurcating pattern is necessary to understand the evolutionary process. The specific bifurcating\npattern (without edge weights) is denoted the tree topology. Edges in the tree have weight we∈(0,∞) that\ncan represent different units, evolutionary time or expected substitutions per site being the most common.\nFigure 1: Left: Phylogenetic tree in 4 taxa. Internal (gray) nodes represent speciation events in which an\nancestral species differentiates into two. External (blue) nodes, also denoted leaves, represent living species\n(here denoted 1,2,3,4). Edge weights (in gray) also denoted branch lengths can represent evolutionary time\nor expected substitutions per site. Center: Different phylogenetic tree on the same 4 taxa in which taxon\n2 is grouped with taxon 3 rather than with taxon 1. Nodes are no longer drawn as is the most common\nrepresentation of phylogenetic trees. Right: Phylogenetic tree with gene flow event depicted as a green\narrow. This biological scenario represents the possibility that some genes have the evolutionary history of\nthe phylogenetic tree on the left (with clade (1 ,2)) and some genes, the evolutionary history of the center\ntree (with clade (2 ,3)).\nOne of the main challenges when inferring phylogenetic trees is the fact that different genes in the data\ncan have different evolutionary histories due to biological processes such as introgression, hybridization or\nhorizontal gene transfer [33, 12, 28]. An example is depicted in Figure 1 (right) which has one gene flow\nevent drawn as a green arrow. This gene flow event represents the biological scenario in which some genes in\ntaxon 2 get transferred from the lineage of taxon 3, and thus, when reconstructing the evolutionary history\nof this group of four taxa, some genes will depict the phylogenetic tree that clusters "}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.16086v1  [q-bio.NC]  26 Dec 2023NOTES ON RETROACTIVE INTERFERENCE MODEL OF\nFORGETTING\nMikhail Katkov\nDepartment of Brain Sciences\nWeizmann Institute of Science\nRehovot, 76100 Israel\nSchool of Natural Sciences\nInstitute for Advanced Study\nPrinceton, NJ\nmikhail.katkov@gmail.com\nABSTRACT\nWe present analytical derivation of the minimal and maximal number of items retained in recently\nintroduced Retroactive Interference Model of Forgetting. Also we computed the probability that two\nitems presented at different times are retained in the memor y at a later time analytically.\nKeywords retrograde interference ·Markov process ·retention curve\nIntroduction\nStudying human memory is a complex task, since there are pres umably many interacting processes that are hard to\nisolate. Traditionally models in psychology of memory are a ttempting to describe many processes at once by creating\na very complicated mathematical model that have a lot of para meters, hard to analyse, and usually require a separate\nﬁtting of parameters for each experiment. Therefore, since parameters have to adjusted for each measurement, it is\nnot clear how good these models would describe memory proces ses outside of laboratory settings. We have recently\nproposed a different type of models to describe human memory that are based on few assumptions and have zero or\nfew parameters that describe the class of stimuli, but indep endent of experimental settings. These types of models,\nif validated, have a much broader applicability in everyday life settings. We have recently presented mathematical\nmodels for memory forgetting and retrieval [1]. In this publ ication we have asked about mathematical properties of\nforgetting model that may help design experiments to valida te the underlying mathematical construction. In this note\nwe provide answers to 2 questions raised in that publication regarding the forgetting model.\nExperimentally, forgetting is traditionally measured in t he form of retention function ( RC(t)) - the probability that\nmemory is retained for time tsince acquisition[2]. People observe that retention funct ion monotonically decreasing\nwith time and although there are debates on the form of retent ion function one of the best candidate is power function\nof time. One of the popular explanation of memory forgetting in humans is retrograde interference [3]. It assumes\nthat new incoming memories are interacting with stored memo ries and cause some past memories disappear. There\nare different approaches to model forgetting (see for examp le [4]), but here we are concentrating on consequences of\nour mathematical model [5] for possible experimental valid ating of underlying assumptions behind our model. The\nmodel assumes that each incoming memory (one memory at one di screte time step) has ndimensional valence vector,\nwith components being iid random variables. Every incoming memory is added to the memory pool, erasing all\nstored memories that have smaller valences in all dimension s. This model can be solved analytically, and the resulting\nretention curve agree well with experiment. Nevertheless, it is not clear to what extent the underlying principles hold s\nduring retention of memory items. We have posed recently sev eral mathematical questions that may provide additional\nexperimental tests related to this issue[1].Forgetting Calculations\nWe consider the forgetting model III from [1]. It states that there is a retention process, where at each time step a\nnew memory is presented to the process. Memory in the model is characterized by a vector of valences vk∈R,k=\n1..n, wherenis a single integer parameter of the model representing the d imensionality of the model. Valences of\nincoming memory is assumed to be sampled from arbitrary stat ionary distribution (absolutely continuous). Incoming\nmemory erases all currently stored memories that have all va lences smaller that corresponding valencies of incoming\nmemory. Finally, incoming memory is added to stored memorie s. Formally, all incoming memories are described by\ncollection of valences V={vk,t∈R,k= 1..n,t∈N}. For each time Twe can deﬁne a set of stored memories\nM(T;V) ={t1,...t|M(T)|},tk< T which contains presentation times ( t1,...t|M(T)|) of stored memories, where the\ncardinality of M(T;V)(|M(T;V)|) represents a number of stored memories at time T. At time T+1a new memory\nwith valences vk,T+1is presented, and the set of stored memories is updated M(T+ 1;V) ={T+ 1;tm:tm∈\nM(T;V),∃k|vk,tm> vk,T+1}. One can ask what is expected value of |M(T)|that is referred to as retention curve\nRCn(T) =E(|M(T;V)|)with respect to distribution of vk,t. It turns out that retention curve does not depend on the\nparticular distribution of valences, since it depends only on the order statistics of memory items in each dimension[5,\n1].\nMinimal number of retained items\nWe proposed two kind of tests that can potentially check the v alidity of the model [1]. Both are related to partialy\nordered set (poset) nature o"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "1 \n Expanding to Arbitrary Study Designs: ANOVA to Estimate Limits of \nAgreement for MRMC Studies  \nSi Wena* and Brandon D. Gallasa \naDivision of Imaging, Diagnostics, and Software Reliability, Office of Science and Engineering \nLaboratories, Center for Devices and Radiological Health, U.S. FDA, Silver Spring, USA  \n* Correspondence to: Si Wen. Email: si.wen@fda.hhs.gov \n  2 \n Expanding to Arbitrary Study Designs: ANOVA to Estimate Limits of \nAgreement for MRMC Studies  \nA multi-reader multi -case (MRMC) analysis i s applied to account for both reader and case \nvariability when evaluating the clinical performance of a medical imaging device or reader \nperformance under different reading modalities. For a clinical task that measures a \nquantitative biomarker an agreement  analysis, such as limits of agreement (LOA), can be \nused. In this work, we decompose the total variation in the data using a three -way mixed \neffect ANOVA model to estimate the MRMC variance of individual differences and the \nLOA between different reading m odalities. There are rules for writing down the \nexpectation of the mean squares in terms of the variance components for fully -crossed data, \ni.e. data where all the readers read all the cases in all modalities being studied. Sometimes \nthe annotation task is  labor -intensive and time -consuming or distributed across sites, so that \na fully -crossed study is not practical. In this work, we focus on estimating the MRMC \nvariance in the within - and between -readers and within - and between -modalities LOA for \nan arbitrary study design. Simulation studies were conducted to validate the LOA variance \nestimates. The method was also applied to a dataset to compare pathologist performance for assessing the density of stromal tumor infiltrating lymphocytes on different platform s. \nKeywords: limits of agreement ; MRMC study ; ANOVA for unbalanced data;  variance \ncomponents  \n1.Introduction  \nA multi -reader multi -case (MRMC)  study is usually appl ied to evaluate whether a medical \nimaging device can improve the clinical performance of the image readers.(Wagner et al. 2007; \nGallas et al. 2012; Obuc howski and Bullen 2022)  In the study, qualitative or quantitative \nassessment s are collected and compared from multiple readers  (radiologists/ pathologists) \nreviewing multiple  cases  (images)  under different reading modalities . For example,  suppose that \nthere is an AI/ML algorithm enabled medical device supporting the pathologist s to evaluate the \ndensity of stromal tumor infiltrating lymphocytes  (sTIL s) on digital slides . The density of sTILs  3 \n is prognostic for survival in breast cancers and  is visual ly assessed  on routine hematoxylin and \neosin (H&E)- stained slides.(Kos et al. 2020)  To validate the performance of the device  in the \nhands of the reader s we collect estimates of the density of sTILs from  pathologists with and \nwithout the assistance of the de vice. Then  we compare the closeness or agreement between  the \nquantitative  measurements , sTIL scores, from different modalities  or readers . We refer to such \nstudies as agreement studies.  \nOur p revious work(Wen and Gallas 2022)  generalized the limits of agreement (LOA) \nmethod (Bland and Altman 1986, 1999) , a widely used agreement analysis, to MRMC analysis \nthat account s for reader and case variability for fully -crossed study designs . In a fully -crossed \nstudy, all the readers provide  measurements for  all the cases for both modalities  as shown in  \nFigure 1.e . In some studies,  the annotation tasks  are labor -intensive and time -consuming or \ndistributed across sites, so that a fully -crossed study is not practical. In this work, we focus on \nMRMC stu dies that are not fully crossed , and we generalize the MRMC analysis to treat  \narbitrary study designs . Some examples are described  in Figure 1.a -Figure 1.d.  \nAn ANOVA model is commonly used to analyze MRMC data,(Beiden et al. 2000; \nDorfman et al. 1992; Gallas et al. 2009; Obuchowski 1995)  as the data are likely correlated when \nthe readers evaluat e the same set of cases and each case is reviewed by multiple readers. A  three -\nway mixed effect ANOVA is  used to estimate different types of LOA for a fully -crossed MRMC \nstudy.(Wen and Gallas 2022)  In ANOVA, th e data generated from a fully -crossed MRMC study \nare called balanced data , as each reader or case has the same number of readings . There are rul es \nfor calculating the sum of squares (SS ) and expected mean squares (MS)  when the data are \nbalanced .(Montgomery 2012)  The expected mean squares link the variance components for the \nrandom effects in the ANOVA model to the SS computed from the data . Then, the variance 4 \n components can be estimated from the data and used to estimate the MRMC variance in  LOA.  \nHowever, in an arbitrary MRMC study, t he data are unbalanced. To determine the relationship \nbetween expected MS  and the variance components in the ANOVA model, we fi"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "T cell receptor binding prediction: A machine\nlearning revolution\nAnna Weber * Aurélien Pélissier†María Rodríguez Martínez∗‡\nDecember 29, 2023\nAbstract\nRecent advancements in immune sequencing and experimental techniques are generat-\ning extensive T cell receptor (TCR) repertoire data, enabling the development of mod-\nels to predict TCR binding specificity. Despite the computational challenges due to\nthe vast diversity of TCRs and epitopes, significant progress has been made. This pa-\nper discusses the evolution of the computational models developed for this task, with\na focus on machine learning efforts, including the early unsupervised clustering ap-\nproaches, supervised models, and the more recent applications of Protein Language\nModels (PLMs). We critically assess the most prominent models in each category, and\ndiscuss recurrent challenges, such as the lack of generalization to new epitopes, dataset\nbiases, and biases in the validation design of the models.\nFurthermore, our paper discusses the transformative role of transformer-based pro-\ntein models in bioinformatics. These models, pretrained on extensive collections of\nunlabeled protein sequences, can convert amino acid sequences into vectorized em-\nbeddings that capture important biological properties. We discuss recent attempts to\nleverage PLMs to deliver very competitive performances in TCR-related tasks. Finally,\nwe address the pressing need for improved interpretability in these often opaque mod-\nels, proposing strategies to amplify their impact in the field.\nKeyword : Machine Learning; T cell Receptor; Specificity Prediction; Protein Lan-\nguage Models; Interpretability.\n*IBM Research Europe (Switzerland).\n†ZHAW, Life Sciences und Facility Management (Switzerland).\n‡Correspondence to maria.rodriguezmartinez@yale.edu.\n1arXiv:2312.16594v1  [q-bio.QM]  27 Dec 20231 Background\nT cells are an essential component of the adaptive immune system, due to their ability\nto orchestrate targeted, effective immune responses through cell-based and cytokine-\nrelease mechanisms. While T cell functions are diverse, their activation, differentiation,\nproliferation, and function are all governed by their T cell receptors (TCR), which\nenable them to recognize non-self antigens arising from infectious agents or diseased\ncells [1].\nTo face a diverse and ever-evolving array of antigens, the immune system has\nevolved the capability to generate a huge array of distinct TCRs. This diversity is\nachieved through a random process of DNA rearrangement, which involves the recom-\nbination of the germline V , D, and J gene segments and the deletion and insertion of\nnucleotides at the V(D)J junctions. While the theoretical diversity of different TCRs\nis estimated to be as high as 1019[2], the realized diversity in an individual is much\nsmaller, typically ranging between 106and 1010[3].\nAt the molecular level, TCRs interact with peptides presented on the major histo-\ncompatibility complex (MHC), a complex commonly referred to as pMHC. Although\nthe interaction between pMHC and TCR is highly specific, a single TCR can often rec-\nognize multiple pMHC complexes. Indeed, some TCRs have been shown to recognize\nup to a million different epitopes [4]. This multivalency is necessary to ensure that\nthe realized diversity in one individual can recognize a significantly broader array of\npotential antigens.\n2 T cell receptor specificity prediction.\nThe precise prediction of TCR-pMHC binding is essential for accurately quantifying\nand predicting immune responses. If effectively represented, it has the potential to\ntransform the field of personalized medicine. For instance, the accurate identification\nof epitopes recognized by expanded TCR clones can aid in identifying the auto-antigens\ninvolved in T-cell-associated autoimmune diseases [5], assessing responses to vaccines\nor identifying the pathogenic agents responsible for eliciting T-cell responses [6]. In\nthe context of cancer, an improved predictive power of TCR specificity can not only\naid in the design of more effective T cell-based therapies [7], but also minimize toxic\nside-effects produced by TCRs off-target binding [8].\nHowever, as experimental methods cannot encompass the vast space of potential\nTCRs and epitopes, significant emphasis has been placed on the development of reliable\ncomputational methods to predict TCR specificity. Existing methods can accurately\nclassify in-distribution samples, i.e. they can predict TCR binding to epitopes already\nencountered by the model [9]. However, the pivotal challenge is to develop models\nwith the capacity to generalize to novel epitopes. A major challenge stems from the\nscarcity of datasets containing experimentally validated TCR-epitope interactions, and\nin particular, regarding the diversity of sampled epitopes.\n23 Limitations of available datasets.\nTCR specificity data can be collected from various databases, such as the VDJdb [10],\nwith over ∼70,000 TCR sequences and ∼1100 different epitopes "}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "scRNA-seq Data Clustering by Cluster-aware Iterative\nContrastive Learning\nWeikang Jiang1∗, Jinxian Wang1∗, Jihong Guan2, Shuigeng Zhou1,†\n1Shanghai Key Lab of Intelligent Information Processing,\nand School of Computer Science, Fudan University\n2Department of Computer Science and Technology, Tongji University\n{sgzhou, 20210240030}@fudan.edu.cn\nwangjx22@m.fudan.edu.cn, jhguan@tongji.edu.cn\nAbstract\nSingle-cell RNA sequencing (scRNA-seq) enables researchers to analyze gene\nexpression at single-cell level. One important task in scRNA-seq data analysis is\nunsupervised clustering, which helps identify distinct cell types, laying down the\nfoundation for other downstream analysis tasks. In this paper, we propose a novel\nmethod called Cluster-aware Iterative Contrastive Learning (CICL in short) for\nscRNA-seq data clustering, which utilizes an iterative representation learning and\nclustering framework to progressively learn the clustering structure of scRNA-seq\ndata with a cluster-aware contrastive loss. CICL consists of a Transformer encoder,\na clustering head, a projection head and a contrastive loss module. First, CICL\nextracts the feature vectors of the original and augmented data by the Transformer-\nencoder. Then, it computes the clustering centroids by K-means and employs the\nstudent’s t-distribution to assign pseudo-labels to all cells in the clustering head.\nThe projection-head uses a Multi-Layer Perceptron (MLP) to obtain projections\nof the augmented data. At last, both pseudo-labels and projections are used in the\ncontrastive loss to guide the model training. Such a process goes iteratively so that\nthe clustering result becomes better and better. Extensive experiments on 25 real-\nworld scRNA-seq datasets show that CICL outperforms the state-of-the-art (SOTA)\nmethods. Concretely, CICL surpasses the existing methods by from 14% to 280%,\nand from 5% to 133% on average in terms of performance metrics ARI and NMI\nrespectively. Source code is available at https://github.com/Alunethy/CICL.\n1 Introduction\nEach cell possesses unique characteristics and biological functions defined by its gene transcription\nactivities. Conventional bulk RNA sequencing measures the average transcription levels of a multitude\nof cells, thereby obscuring the heterogeneity among individual cells. In the past decade, the rapid\nprogress of single-cell RNA sequencing (scRNA-seq) technologies [Tang et al., 2009] enables\ntranscriptome-wide gene expression measurement in individual cells, which greatly helps deepen\nour understanding of cellular heterogeneity and propels the research on cell biology, immunology,\nand complex diseases [van Galen et al., 2019]. Identifying cell types is a fundamental step in\nunraveling complex biological processes such as cellular differentiation, lineage commitment, and\ngene regulation [Deng et al., 2021]. As such, cell clustering becomes an important task in scRNA-seq\nanalysis. However, the inherent high-dimensionality, noise, and sparsity of scRNA-seq data present\nsevere challenges for scRNA-seq data clustering analysis [Kiselev et al., 2019, Stegle et al., 2015].\n∗Co-first authors who contribute equally to this work.\n†Corresponding author.\nPreprint. Under review.arXiv:2312.16600v1  [q-bio.GN]  27 Dec 2023Up to now, many models or algorithms have been developed for scRNA-seq data clustering. Early\nscRNA-seq clustering methods mainly rely on traditional dimensionality reduction and clustering\nmethods. For example, pcaReduce [Žurauskien ˙e and Yau, 2016] combines PCA and K-means,\niteratively merging cluster pairs based on related probability density function. Recognizing the\nimportance of similarity metrics in the clustering task, SIMLR [Wang et al., 2018a] amalgamates\nmultiple kernels to learn sample similarity and perform spectral clustering. Seurat [Satija et al., 2015]\nemploys a graph-based community detection algorithm, while Louvain [Blondel et al., 2008] is based\non the shared nearest neighbor graph to identify cell types.\nIn the past decade, with the rapid development of deep learning, deep neural networks (DNN) have\nbeen extensively applied to scRNA-seq data clustering to address the limitations of conventional\nmethods [Flores et al., 2022]. DEC [Xie et al., 2016] and IDEC [Guo et al., 2017], based on\nautoencoders (AE), use KL divergence as the clustering loss, achieving simultaneous learning of\nfeature representations and cluster assignments. To address the pervasive dropout events in scRNA-\nseq data, DCA [Eraslan et al., 2019] proposes a zero-inflated negative binomial (ZINB) model to better\ncharacterize the distribution of scRNA-seq data, and uses the negative likelihood as the reconstruction\nloss instead of the frequently-used mean-square error (MSE) loss in autoencoders. scVI [Lopez et al.,\n2018] is a deep generative model based on variational autoencoders, which can do various scRNA-seq\ndata analyses such as data imputation, clustering, and visualization. scDeepCluster [Tian et al., 2019]\nintroduce"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "DOSA-MO: Dual-stage Optimizer for Systematic overestimation\nAdjustment in Multi-Objective problems improves biomarker discovery\nLuca Cattelani∗1and Vittorio Fortino†1\n1Institute of Biomedicine, School of Medicine, University of Eastern Finland, Kuopio, 70211, Finland.\nAbstract\nThe challenge in biomarker discovery and validation using machine learning from omics data lies in the abun-\ndance of molecular features but scarcity of samples. Most machine learning-based feature selection methods ne-\ncessitate of hyperparameter tuning, typically performed by evaluating numerous alternatives on a validation set.\nEvery evaluation has a performance estimation error and when the selection takes place between many models\nthe best ones are almost certainly overestimated. Biomarker identification is a typical multi-objective problem\nwith trade-offs between the predictive ability and the parsimony in the number of molecular features. Genetic\nalgorithms are a popular tool for multi-objective optimization but they evolve numerous solutions and are prone to\noverestimation. Methods have been proposed to reduce the overestimation after a model has already been selected\nin single-objective problems, but to the best of our knowledge no algorithm existed that was capable of reducing the\noverestimation during the optimization, leading to a better model selection, or that had been applied in the more\ngeneral domain of multi-objective problems. We propose DOSA-MO, a novel multi-objective optimization wrapper\nalgorithm that learns how the original estimation, its variance, and the feature set size of the solutions predict the\noverestimation, and adjusts the expectation of the performance during the optimization, improving the composition\nof the solution set. We verify that DOSA-MO improves the performance of a state-of-the-art genetic algorithm\non left-out or external sample sets, when predicting cancer subtypes and/or patient overall survival, using three\ntranscriptomics datasets for kidney and breast cancer. Since to the best of our knowledge there was no measure\nof estimation error for multi-objective solution sets, we propose two such measures. According to both of them,\nDOSA-MO provides more realistic performance estimates in all the considered scenarios.\nData and source code availability. The gene expression data used in this study is from public repositories.\nThe source code and detailed numerical results will be available in a public server after peer review.\n∗Corresponding Author: luca.cattelani@uef.fi\n†Corresponding Author: vittorio.fortino@uef.fi\n1arXiv:2312.16624v1  [q-bio.QM]  27 Dec 20231 Introduction\nMolecular biomarker discovery with machine learning (ML)\nis usually limited by data that includes many features\nbut few samples. This renders the models prone to over-\nfitting and the evaluation prone to estimation error. Many\nML approaches involve hyperparameter tuning and fea-\nture selection, with model selection based on the perfor-\nmance measured on a validation set within a training,\nvalidation, and test paradigm. Comparing many mod-\nels and choosing the best is prone to overestimation, i.e.\nthe difference between the performance measured on the\nvalidation set and the real performance, a.k.a. “winners\ncourse”. The models that fit the noise present in the\nvalidation set are advantaged, a phenomena sometimes\nreferred as overfitting on the validation set [1]. This is\nalso exacerbated by data scarcity. In the quest to reach\nhigher accuracies, increasing the number of hyperparam-\neter configurations may lead to selected models with bet-\nter performance on the validation set, but a correspond-\ning increase in overestimation may determine diminish-\ning or even negative improvements on the test set.\nIn biomarker selection both the predictive performance\n(in terms of disease type, survivability, etc.) and the\npractical applicability are crucial factors. A biomarker\nthat includes a small number of molecular features re-\nquires less resources when applied clinically, thus might\nbe preferred over a slightly more accurate but more ex-\npensive or time consuming alternative.\nCharacterizing all the best compromises (or trade-\noffs) between predictive value and feature set size is a\nmulti-objective (MO) optimization problem [2], and it\ncan be solved by means of MO feature selection (MOFS)\ntechniques [3, 4, 5, 6]. These techniques aim to iden-\ntify not just a single best solution, as in single-objective\n(SO) problems, but rather a Pareto front of solutions.\nThis front is the set of optimal solutions that illustrate\nthe trade-offs between different objectives in MO opti-\nmization. However, all candidate solutions (or biomarker\nmodels selected by the employed MO feature selection\ntechnique) are evaluated on the validation set, which can\nresult in the overestimation of the performance of the se-\nlected models.\nK-fold cross-validation (CV) is the most common method-\nology for ML assessment and its benefits and limitations\nhave received much"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "bioRxiv 2023.12.24.XXXXX\narXiv 2312.XXXXX\nWhales in Space:\nExperiencing Aquatic Animals in Their Natural Place\nwith the Hydroambiphone\nJames P. Crutchfield,1,∗David D. Dunn,2,†and Alexandra M. Jurgens3,‡\n1Complexity Sciences Center, Physics and Astronomy Department\nUniversity of California, Davis, California 95616\n2Art & Science Laboratory\n613 C Street, Davis, California 95616\n3Inria Centre, University of Bordeaux, France\n(Dated: December 29, 2023)\nRecording the undersea three-dimensional bioacoustic sound field in real-time promises major\nbenefits to marine behavior studies. We describe a novel hydrophone array—the hydroambiphone\n(HAP)—that adapts ambisonic spatial-audio theory to sound propagation in ocean waters to realize\nmany of these benefits through spatial localization and acoustic immersion. Deploying it to monitor\nthe humpback whales ( Megaptera novaeangliae ) of southeast Alaska demonstrates that HAP recording\nprovides a qualitatively-improved experience of their undersea behaviors; revealing, for example,\nnew aspects of social coordination during bubble-net feeding. On the practical side, spatialized\nhydrophone recording greatly reduces post-field analytical and computational challenges—such\nas the “cocktail party problem” of distinguishing single sources in a complicated and crowded\nauditory environment—that are common to field recordings. On the scientific side, comparing the\nHAP’s capabilities to single-hydrophone and nonspatialized recordings yields new insights into the\nspatial information that allows animals to thrive in complex acoustic environments. Spatialized\nbioacoustics markedly improves access to the humpbacks’ undersea acoustic environment and expands\nour appreciation of their rich vocal lives.\nKeywords: marine mammals, humpback whales, undersea vocalizations, ambisonics, hydrophone array, spatial\nsound\nI. INTRODUCTION\nMarine mammals spend the bulk of their active lives\nsubmerged beneath the sea surface. Given the relatively\npoor propagation of light compared to sound in the ocean\ndepths, the world of these animals is primarily acous-\ntic. These factors greatly complicate relying solely on\nsurface observations to address the full diversity of their\nbehaviors. Fortunately, in the last decade or so scientists\ndemonstrated the substantial benefit of undersea, com-\nprehensive tracking with, for example, skillful attachment\nof digital devices that monitor animal behavior via sen-\nsors that record video, sound, location, depth, pressure,\ntemperature, and the like [ 1]. The following describes\ncomplementary benefits that come from recording the\nunderwater three-dimensional bioacoustic sound field in\nreal-time.\nA. Whale Bioacoustics\nSound propagation in water differs from that in air:\nsoundtravelsfivetimesfasterinwaterthaninair, acoustic\n∗chaos@ucdavis.edu\n†artscilab@gmail.com\n‡amjurgens@ucdavis.eduwaves in water propagate with much less dissipation, and\ndifferent frequencies travel at different speeds. (See Table\nI.) These phenomena make undersea sound markedly\nmore complex to analyze, understand, and harness. They\ncomplicate directly monitoring and interpreting sound\nin the ocean. That said, these properties also mean\nthere is additional information available in ocean acoustic\nwaves to be harnessed for environmental sensing and for\ncommunication. (See App. A.)\nTo begin to address these challenges, we applied spa-\ntial bioacoustics to monitor humpback whales ( Megaptera\nnovaeangliae ) of southeast Alaska, demonstrating that it\nmarkedly improves understanding their undersea behav-\niors. As one example, the following describes how acoustic\nspatialization revealed previously unreported aspects of\nsocial coordination during bubble-net feeding [2].\nCetaceans exhibit compelling evidence for advanced\nintentional behaviors and conscious awareness through\ntheir raw intelligence, song generation [ 3,4] and sharing\n[5,6], communication and interactions with their own\nand other species [ 7,8], and empathy (concern for others’\nwell-being) [ 9]. Humpback whales, in addition, are known\nto be very vocal and social [10].\nEvolving over a time span ten times that of humans,\ncetaceans developed tools (socially-coordinated bubble-\nnet feeding by humpbacks) and region- (and possibly\nhemisphere-) spanning ocean-acoustic communication net-\nworks [11]. Over the last half century humpback whales,arXiv:2312.16662v1  [q-bio.PE]  27 Dec 20232\nMedium Density Bulk Modulus Sound Velocity Wavelength Wavelength\n(kg/m3) (Pa) (m/s)**** 100Hz (m) **** 1000Hz (m) ****\nAir 1.225 * 1.42×105343 3.43 0.343\nFresh Water 1000 ** 2.15×1091482 14.82 1.482\nSeawater 1025 *** 2.29×1091500 15 1.5\nTABLE I: Sound propagation differences in air and water: (i) Medium density, (ii) medium bulk modulus, (iii) sound\nvelocity, and (iv) sound wavelengths at two different frequencies. Unlisted, but important is sound dispersion: the\nrange of frequency-dependent velocities is markedly large in water. *Standard atmospheric conditions (0 C or 32 F, at"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": " 1Cellular forgetting,  desensitisation,  stress and aging in signalling  \nnetworks.  When do cells refuse to learn more? \n \nTamás Veres1,†, Márk Kerestély1,†, Borbála M. Kovács1, Dávid Keresztes1, Klára \nSchulc1,2, Erik Seitz1, Zsolt Vassy1, Dániel V. Veres1,3, Peter Csermely1* \n \nAbstract \nRecent findings show that single, non-neuronal cells are also a ble to learn signalling \nresponses developing cellular me mory. In cellular learning node s of signalling networks \nstrengthen their interactions  e.g. by the conformational memory  of intrinsically disordered \nproteins, protein translocation, m iRNAs, lncRNAs, chromatin mem ory and signalling \ncascades. This can be described by a generalized, unicellular H ebbian learning process, where \nthose signalling connections, w hich participate in learning, be come stronger. Here we review \nthose scenarios, whe re cellular signalling is not only repeated  in a few times (when learning \noccurs), but becomes too frequent , too large, or too complex an d overloads the cell. This leads \nto desensitisation of signall ing networks by decoupling signall ing components, receptor \ninternalization, and consequent dow nregulation. These molecular  processes are examples of \nanti-Hebbian learning and ‘forget ting’ of signalling networks. Stress can be perceived as \nsignalling overload inducing the d esensitisation of signalling pathways. Aging occurs by the \nsummative effects of cumulative stress downregulating signallin g. We propose that cellular \nlearning desensitisation, stress and aging may be placed along the same axis of more and \nmore intensive (prolonged or repea ted) signalling. We discuss h ow cells might discriminate \nbetween repeated and unexpected s ignals, and highlight the Hebb ian and anti-Hebbian \nmechanisms behind the fold-change detection in the NF- κB signalling pathway. We list drug \ndesign methods using Hebbian lear ning (such as chemically-induc ed proximity) and clinical \ntreatment modalities inducing ( cancer, drug allergies) desensit isation or avoiding drug-\ninduced desensitisation. A better discrimination between cellul ar learning, desensitisation and \nstress may open novel directi ons in drug design, e.g., helping to overcome drug-resistance. \n Keywords allergy, asthma, diabetes, habitu ation, heat shock, heart failu re, incoherent type-1 \nfeedforward loop; metabolon, mne mon, prion, protein translocati on; receptor dow nregulation, \nscaffold proteins  \nLearning of signalling networks  – at the level of their compone nts \n \nMolecular mechanisms of neuronal learning became well establish ed [1]. However, much less \nis known about the regulation of lea rning at the individual, no n-neuronal cells. Recent \nfindings gave further evidence th at learning, indeed occurs in unicellular organisms, as well as \nin individual cells of various ti ssues other than neurons, even  in rather sophisticated forms [2]. \nIn our paper we define cellular l earning as an adaptive respons e to a stimulus, when the \nstimulus is repeated in a short time. This leaves out many clas sical models of learning (such \nas Pavlovian conditional learni ng) from our discussion. However , such a simplification \ngreatly helps the iden tification of molecular mechanisms, which\n become increasingly \n                                                 \n* Peter Csermely, csermely.peter@med.s emmelweis-univ.hu   \n† These authors contributed equally  to the paper: Tamás Veres, Má rk Kerestély \n1 Department of Molecular Biology , Semmelweis University, Budapes t, Hungary \n2 Division of Oncology, Department of Internal Medicine and Onco logy, Semmelweis University, Budapest, \nHungary \n3 Turbine Ltd, Budapest, Hungary  2obscured when long-term, multistep adaptation phenomena are exa mined, such as cell \ndifferentiation or tumour developm ent. Several experiments in b udding yeast, Arabidopsis  or \nrice cells, mouse fibroblasts or murine CD8+ memory cells showed the formation of \nmolecular memory resulting in a f aster, larger, more sensitive and/or more robust response \nafter the second signal than the first [3-9]. \n Various molecular mechanisms induce a faster and stronger respo nse after a repeated signal in \nsingle cells. We mention the conf ormational memory of intrinsic ally disordered proteins \n(IDPs) first, where the IDP tra nsiently keeps its ordered confo rmation acquired after the first \nsignal, and if the second signal arrives within the time window  of the IDPs relaxation back to \nthe disordered state, than the s econd signal finds the IDP in a  ‘conformationally-primed’, \n‘memory’-state [10,11]. IDPs may act  like molecular switches ch anging the direction of \nsignal transmission [12]. Prions a re an important class of IDPs . Prion proteins may transform \nthemselves to a β-sheet enriche d prion form, which forms aggreg ates. In budding yeast cells \nthe prion form of Pin1 maintain ed the molecular memory of a pre vious heat shock for many \ngenerations [4]. Oligomerizing pr oteins invol"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.17189v1  [math.CV]  28 Dec 2023Characteristic Function, Schur Parameters\nand Pseudocontinuation of Schur functions\nVladimir K. Dubovoy, Bernd Fritzsche, Bernd Kirstein, Conrad\nM¨ adler and Karsten M¨ uller\nDedicated to Damir Zjamovich Arov\nAbstract. In [19] there is an approach to the investigation of the pseud o-\ncontinuability of Schur functions in terms of Schur paramet ers. In par-\nticular, there was obtained a criterion for the pseudoconti nuability of\nSchur functions and the Schur parameters of rational Schur f unctions\nwere described. This approach is based on the description in terms of\nthe Schur parameters of the relative position of the largest shift and\nthe largest coshift in a completely nonunitary contraction . It should be\nmentionedthattheseresults receivedafurtherdevelopmen tin [8,21–24].\nThis paper is aimed to give a survey about essential results o n\nthis direction. The main object in the approach is based on co nsidering\na Schur function as characteristic function of a contractio n (see Sec-\ntion 1.2). This enables us outgoing from Schur parameters to construct\na model of the corresponding contraction (see Section 2). In this model,\nthe relative position of the largest shift and the largest co shift in a com-\npletely nonunitary contraction is described in Section 3 an d then, based\non this model, to ﬁnd characteristics which are responsible for the pseu-\ndocontinuability of Schur functions (see Sections 4 and 5). The further\nparts of this paper (see Sections 6–8) admit applications of the above re-\nsults to the study of properties of Schur functions and quest ions related\nwith them.\nMathematics Subject Classiﬁcation (2010). Primary 30J10; Secondary\n47A48, 47A45, 30E05, 47A57.\nKeywords. Schur function, Schur parameters, characteristic operato r\nfunction, pseudocontinuability of Schur functions, contr action, unitary\ncolligation, open system.\nContents\n1. Introduction 3\n1.1. Pseudocontinuability of Schur functions 32 Dubovoy, Fritzsche, Kirstein, M¨ adler and M¨ uller\n1.2. Schurfunctionasacharacteristicfunctionofaunitarycolligat ion 5\n1.3. Naimark dilations 10\n1.4. Schur algorithm, Schur parameters 13\n2. Construction of a model of a unitary colligation via the Schur\nparameters of its characteristic function in the scalar case 14\n2.1. General form of the model 14\n2.2. Description of the model of a unitary colligation if∞/producttext\nj=0(1−|γj|2)\nconverges 19\n2.3. Description of the model of a unitary colligation in the case of\ndivergence of the series∞/summationtext\nj=0|γj|221\n2.4. Description of the model in the case that the function θis a\nﬁnite Blaschke product 22\n2.5. Comments 24\n3. A model representation of the largest shift VTcontained in a\ncontraction T 27\n3.1. The conjugate canonical basis 27\n3.2. A model representation of the maximal unilateral shift VT\ncontained in a contraction T 28\n4. Some criteria for the pseudocontinuability of a Schur function in\nterms of its Schur parameters 34\n4.1. On some connections between the largest shifts VTandVT∗and\nthe pseudocontinuability of the corresponding c.o.f. θ34\n4.2. Construction of a countable closed vector system in HGFand\ninvestigation of the properties of the sequence ( σn)∞\nn=1\nof Gram determinants of this system 36\n4.3. Some criteria of pseudocontinuability of Schur functions 43\n5. General properties of the Schur parameter sequences of\npseudocontinuable Schur functions 46\n5.1. On some properties of the Schur parameter sequences of\npseudocontinuable Schur functions 46\n5.2. The structure of pure Π–sequences of rank 0 or 1 52\n6. TheS–recurrence property of the Schur parameter sequences\nassociated with non–inner rational Schur functions 56\n7. Characterization of Schur parameter sequences of polynomial\nSchur functions 70\n7.1. First observations on Schur functions of polynomial type 70\n7.2. On a theorem of R. I. Teodorescu 70\n7.3. The matrix ofthe operator TGFwith respect to the basis ( hk)m\nk=1\nin the case dim HGF=m<+∞ 73\n7.4. Schur parameter sequences of polynomial Schur functions.\nExamples 75\n8. Characterization of Helson-Szeg˝ o measures in terms of the Sc hur\nparameters of the associated Schur function 78Characteristic Function, Schur Parameters and Pseudocontinua tion 3\n8.1. Interrelated quadruples consisting of a probability measure, a\nnormalized Carath´ eodoryfunction, a Schur function and\na sequence of contractive complex numbers 78\n8.2. A unitary colligation associated with a Borelian probability\nmeaseure on the unit circle 81\n8.3. On the connection between the Riesz projection P+and the\nprojectionPF\nµ,Gwhich projects HµontoHµ,Gparallel to\nHµ,F 84\n8.4. OntheconnectionoftheRieszprojection P+withtheorthogonal\nprojections from Hµonto the subspaces H⊥\nµ,FandH⊥\nµ,G87\n8.5. Matrix Representation of the Operator Bµ,Gin Terms of the\nSchur Parameters Associated With the Measure µ88\n8.6. Characterization of Helson-Szeg˝ o measures in terms of the S chur\nparameters of the associated Schur function 93\nAcknowledgment 99\nReferences 100\n1"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Single particle algorithms to reveal cellular nanodomain\norganization\nP. Parutto1, J. Heck2, M. Heine2and D. Holcman3,4∗\nDecember 29, 2023\nAbstract\nFormation, maintenance and physiology of high-density protein-enriched organized\nnanodomains, first observed in electron microscopy images, remains challenging to\ninvestigate due to their small sizes. However, these regions regulate molecular traf-\nficking, assembly and sorting required for higher cell functions, such as communication\nor plastic changes. Over the past ten years, super-resolution single-particle trajecto-\nries (SPTs) have been used to sample these sub-cellular environments at a nanometer\nresolution for both membrane and soluble proteins. We present here data analysis\ndevelopments and algorithms that convert high-throughput molecular trajectories into\nmaps of molecular density, diffusion and local drift organization. These approaches\ntransform intrinsic trajectory properties into statistics of the underlying cellular orga-\nnization. The automatic identification of large numbers of high-density regions allows\nquantifying their boundary location and organization, their stability over time and\ntheir ability to transiently retain molecules. To conclude recent automated algorithms\ncan now be used to extract biophysical parameters of sub-cellular nanodomains over a\nlarge amount of trajectories.\nKeywords: Single particle trajectories; high density regions; spatial maps; neuronal synapses,\nstochastic models, phase separation, condensates, aggregates, molecular trafficking, machine-\nlearning algorithms; optimal estimators, Maximum Likelihood Estimators\n1 Introduction\nHigh-density nanodomains in neurons and in cells in general have already been character-\nized from the early ultrastructure-electron microscopy images of Palade, de Robertis, and\n∗1UK Dementia Research Institute at the University of Cambridge and Department of Clinical Neuro-\nsciences, University of Cambridge, Cambridge CB2 0AH, UK.2Research Group Functional Neurobiology\nat the Institute of Developmental Biology and Neurobiology, Johannes Gutenberg University Mainz, Mainz,\nGermany3DAMPT, University Of Cambridge, DAMPT and Churchill College CB30DS, United Kingdom.4\nGroup of Data Modeling and Computational Biology, IBENS, Ecole Normale Sup´ erieure,75005 Paris, France.\nLead contact: david.holcman@ens.fr\n1arXiv:2312.17191v1  [q-bio.QM]  28 Dec 2023Bennett in 1954, suggesting more than 70 years ago [1] the heterogenous distribution of\nproteins especially at neuronal synapses such as post-synaptic density (PSD). This observa-\ntion anchored the field of neurotransmission at the molecular organization level, where local\nstructures shape function.\nDense nano-regions are ubiquitous on membranes such as the pre-synaptic active zones that\nare enriched in voltage-gated Calcium channels (VGCCs) and docking proteins and serve as\npriming sites for readily releasable synaptic vesicles [2]. Organelles such as the spine appa-\nratus (SA) [3, 4], a specialized compartment of the endoplasmic reticulum (ER), found in a\nsubpopulation of dendritic spines in neurons of the central nervous system is also organized\nin nanodomains with differential accumulation of Ryanodyne receptors (RyR) at the base\nversus SERCA pumps located inside the spine head, thus regulating calcium in and out\nfluxes [5, 6]. Made of molecular aggregates, these nano-regions are also observed in soluble\nenvironments, for example inside the nucleus with high density-regions enriched in nucleo-\nsomes, or in the cytosol where reserve and recycling pools of synaptic vesicles are transiently\naccumulated in axons and synapses [7, 8].\nCharacterizing and identifying high-density nandomains remained challenging as they are\nformed by dense protein assemblies, measuring only a few hundred of nanometers. Recent\nfindings and reviews [9, 10, 11] have proposed that these regions could result from sponta-\nneously organized condensed phase where higher protein concentration accumulate, with the\nclassical image of oil droplets in water. This representation is not as simple, as it consists\nin multimeric assemblies of various types of interacting proteins. The concept of phase sep-\naration is indeed a physical model to describe membraneless compartments also known as\nmolecular condensates. We refer to recent reviews on phase separation in synaptic biology\nfor these phase separation concepts [10].\nHowever, in the past decade, single particle trajectory (SPT) approaches revealed that these\ncondensates are permissive to proteins, and thus are not necessarily creating fully isolated\ndomains. In addition these regions are not necessarily stable over time. We present here\nhow high-density nanodomains have been characterized by SPTs, their temporal stability,\nand how they can retain molecules over time. We further develop ready-to-use automated\nalgorithms, developed to detect high-density regions from SPTs analysis and to collect their\nstatistics.\n2 Sub-cellular space explor"}
