{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Fast Inference of Mixture-of-Experts Language\nModels with Offloading\nArtyom Eliseev\nMoscow Institute of Physics and Technology\nYandex School of Data Analysis\nlavawolfiee@gmail.comDenis Mazur\nMoscow Institute of Physics and Technology\nYandex\nResearchcore\ndenismazur8@gmail.com\nAbstract\nWith the widespread adoption of Large Language Models (LLMs), many deep\nlearning practitioners are looking for strategies of running these models more\nefficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) — a\ntype of model architectures where only a fraction of model layers are active for\nany given input. This property allows MoE-based language models to generate\ntokens faster than their “dense” counterparts, but it also increases model size\ndue to having multiple “experts”. Unfortunately, this makes state-of-the-art MoE\nlanguage models difficult to run without high-end GPUs. In this work, we study\nthe problem of running large MoE language models on consumer hardware with\nlimited accelerator memory. We build upon parameter offloading algorithms and\npropose a novel strategy that accelerates offloading by taking advantage of innate\nproperties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with\nmixed quantization on desktop hardware and free-tier Google Colab instances.\n1 Introduction\nMany recent advances in natural language processing rely on large pre-trained language models, such\nas GPT-3 and 4 Brown et al. (2020); OpenAI (2023), Palm & Gemini Chowdhery et al. (2022); Team\net al. (2023) and many others. However, the rapid scientific progress in this area would be impossible\nwithout open-access LLMs such as LLaMA 1 and 2 (Touvron et al., 2023), Falcon (TII UAE, 2023),\nBLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), or NeoX/Pythia (Biderman et al., 2023). The\nkey advantage of open-access LLMs is that researchers can deploy them locally and modify them in\nways that would be impossible with proprietary APIs.\nEven though LLM parameters are openly available, it is still difficult to use these models due to their\nsheer size. State-of-the-art open-access language models require multiple high-end GPUs1even for\nbasic inference workloads. To use these LLMs on more affordable hardware setups, one must either\ncompress model parameters (Dettmers et al., 2022; Frantar et al., 2022) or offload parameters to a\ncheaper storage, be it RAM or SSD (Pudipeddi et al., 2020; Sheng et al., 2023).\nSeveral recent works modify transformer architecture by introducing sparse Mixture-of-Experts\nblocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple “experts” (layers),\nas well as a “gating function” that selects which experts are used on a given input. As a result, the\nMoE block uses a small portion of all “experts” for any single forward pass, allowing for more\ncompute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the\nlargest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While\nMixture-of-Experts models can be more efficient than their dense counterparts, many techniques for\nefficient LLM inference were not designed with MoE in mind and perform suboptimally on modern\nlarge language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires\n140GB of combined accelerator memory.arXiv:2312.17238v1  [cs.LG]  28 Dec 2023In this work, we systematically develop techniques for running large MoE language models with\nlimited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B-\nInstruct — a MoE-based chat assistant — on a desktop-grade hardware where only a fraction of\nexperts fit into the accelerator memory. To that end:\n•we observe how MoE language model accesses its experts between tokens, and find several\nregularities: i) some experts are reused between adjacent tokens and ii) the model hidden\nstates of early layers already “know” which experts are to be used at subsequent layers.\n•we design a MoE-specific offloading strategy that takes advantage of these regularities: i)\nit uses LRU cache to significantly reduces GPU-RAM communication, leading to faster\ngeneration and ii) it guesses which experts are needed ahead of time to better overlap expert\nloading with computation.\n•we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060\nand RTX 3080 Mobile and develop a practical combination of mixed quantization and\nthe proposed offloading algorithm to run this model interactively at 2-3 tokens per second\ndepending on the hardware. The source code with our implementation is available online2\n2 Background & Related Work\n2.1 Mixture-of-Experts\nThe recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan &\nJacobs, 1994) of training ensembles of specialized models (“experts”) and a gating function to select\nthe right expert for the task. To achieve specializat"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Compact Neural Graphics Primitives with Learned Hash Probing\nTOWAKI TAKIKAWA, NVIDIA, Canada and University of Toronto, Canada\nTHOMAS MÜLLER, NVIDIA, Switzerland\nMERLIN NIMIER-DAVID, NVIDIA, Switzerland\nALEX EVANS, NVIDIA, United Kingdom\nSANJA FIDLER, NVIDIA, Canada and University of Toronto, Canada\nALEC JACOBSON, University of Toronto, Canada and Adobe, Canada\nALEXANDER KELLER, NVIDIA, Germany\nJPEG \n(80.1 kB, 40.21 dB)Instant NGP \n(80.0 kB, 35.18 dB)Ours \n(80.18 kB, 38.48 dB)Reference Image\n(502.9 kB)\n [Rho et al. 2023] \n(345.52 kB, 29.8 dB)Instant NGP\n(1911.94 kB, 30.0 dB)Ours\n(373.96 kB, 29.8 dB)Reference Image\n 3D Scene from Multiview Images\nFig. 1. Compact neural graphics primitives (Ours) have an inherently small size across a variety of use cases with automatically chosen hyperparameters. In\ncontrast to similarly compressed representations like JPEG for images (top) and masked wavelet representations [Rho et al .2023] for NeRFs [Mildenhall\net al.2020] (bottom), our representation neither uses quantization nor coding, and hence can be queried without a dedicated decompression step. This is\nessential for level of detail streaming and working-memory-constrained environments such as video game texture compression. The compression artifacts\nof our method are easy on the eye: there is less ringing than in JPEG and less blur than in Rho et al .[2023] (though more noise). Compact neural graphics\nprimitives are also fast: training is only 1.2–2.6×slower (depending on compression settings) and inference is faster than Instant NGP [Müller et al .2022]\nbecause our significantly reduced file size fits better into caches.\nNeural graphics primitives are faster and achieve higher quality when their\nneural networks are augmented by spatial data structures that hold trainable\nfeatures arranged in a grid. However, existing feature grids either come with\na large memory footprint (dense or factorized grids, trees, and hash tables)\nor slow performance (index learning and vector quantization). In this paper,\nwe show that a hash table with learned probes has neither disadvantage,\nresulting in a favorable combination of size and speed. Inference is faster than\nunprobed hash tables at equal quality while training is only 1.2–2.6×slower,\nsignificantly outperforming prior index learning approaches. We arrive at\nthis formulation by casting all feature grids into a common framework: they\neach correspond to a lookup function that indexes into a table of feature\nAuthors’ addresses: Towaki Takikawa, NVIDIA, Canada and University of\nToronto, Canada, tovacinni@gmail .com; Thomas Müller, NVIDIA, Zürich, Switzer-\nland, tmueller@nvidia .com; Merlin Nimier-David, NVIDIA, Zürich, Switzerland,\nmnimierdavid@nvidia .com; Alex Evans, NVIDIA, London, United Kingdom,\nbluespoon@gmail .com; Sanja Fidler, NVIDIA, Toronto, Canada and University of\nToronto, Canada, sfidler .com; Alec Jacobson, University of Toronto, Toronto, Canada\nand Adobe, Toronto, Canada, jacobson@cs .toronto.edu; Alexander Keller, NVIDIA,\nBerlin, Germany, akeller@nvidia .com.vectors. In this framework, the lookup functions of existing data structures\ncan be combined by simple arithmetic combinations of their indices, resulting\nin Pareto optimal compression and speed.\nAdditional Key Words and Phrases: Neural graphics primitives, compression.\n/Q_uality (PSNR)\nSize (kB) 102103104262830\nInstant NGPOurs\n[Rho et al. 2023]\nFig. 2. Size vs. PSNR Pareto curves on the NeRF scene from Figure 1. Our\nwork is able to outperform Instant NGP across the board and performs\ncompetitively with masked wavelet representations [Rho et al. 2023].arXiv:2312.17241v1  [cs.CV]  28 Dec 20232 •Takikawa et al.\n1 INTRODUCTION\nThe ever increasing demand for higher fidelity immersive expe-\nriences not only adds to the bandwidth requirements of existing\nmultimedia formats (images, video, etc.), but also fosters in the use\nof higher-dimensional assets such as volumetric video and light field\nrepresentations. This proliferation can be addressed by a unified\ncompression scheme that efficiently represents both traditional and\nemerging multimedia content.\nNeural graphics primitives (NGP) are a promising candidate to\nenable the seamless integration of old and new assets across ap-\nplications. Representing images, shapes, volumetric and spatio-\ndirectional data, they facilitate novel view synthesis (NeRFs) [Milden-\nhall et al .2020], generative modeling [Lin et al .2023; Poole et al .\n2023], and light caching [Müller et al .2021], among more applica-\ntions [Xie et al .2022]. Particularly successful are those primitives\nthat represent data by a feature grid that contains trained latent em-\nbeddings to be decoded by a multi-layer perceptron (MLP). Various\nsuch feature grids have been proposed, but they usually come with\na substantial memory footprint [Chabra et al .2020], even when\nfactorized into low-rank representations [Chen et al .2022] or rep-\nresented in terms of sparse data structures [Fridovich-Keil et al .\n2022; "}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Learning to Generate Text in Arbitrary Writing Styles\nAleem Khan, Andrew Wang, Sophia Hager andNicholas Andrews\nDepartment of Computer Science, Johns Hopkins University\n{akhan141,awang116,shager2,noa}@jhu.edu\nAbstract\nPrior work in style-controlled text genera-\ntion has focused on tasks such as emulat-\ning the style of prolific literary authors, pro-\nducing formal or informal text, and the de-\ngree of toxicity of generated text. Plenti-\nful demonstrations of these styles are avail-\nable, and as a result modern language mod-\nels are often able to emulate them, either via\nprompting or discriminative control. How-\never, in applications such as writing assis-\ntants, it is desirable for language models to\nproduce text in an author-specific style on\nthe basis of a small writing sample. We find\nthat instruction-tuned language models can\nstruggle to reproduce author-specific style\ndemonstrated in a prompt. Instead, we pro-\npose to guide a language model to gener-\nate text in a target style using contrastively-\ntrained representations that capture stylo-\nmetric features. A central challenge in doing\nso is that an author’s writing is characterized\nbysurprising token choices under a generic\nlanguage model. To reconcile this tension,\nwe combine generative re-scoring to achieve\nan author-specific model, with discrimina-\ntive control to ensure style consistency at the\nsequence-level. The combination of these\napproaches is found to be particularly effec-\ntive at adhering to an author-specific style\nin a variety of conditions, including uncon-\nditional generation and style transfer, and\nis applicable to any underlying language\nmodel without requiring fine-tuning.\n1 Introduction\nWe consider the problem of generating text in the\nstyle of an arbitrary author on the basis of a small\nwriting sample, on the order of a few hundred\nwords. Although instruction-tuned language mod-\nels (LM) have demonstrated the ability to emulate\na variety of writing styles via prompting (Desh-\npande et al., 2023), particularly when a given styleis well-represented in the training data (Krishna\net al., 2020), we find that performance is less con-\nsistent in our few-shot setting, with recent large\nLMs such as GPT-3.5 performing worse than pre-\nvious generations (Ouyang et al., 2022a). A sep-\narate challenge is that large LMs can be computa-\ntionally prohibitive in certain applications, such as\non-device deployment where privacy-preserving\npersonalized generation may be needed.\nPrior work in controllable text generation has\nprimarily focused on categorical target attributes\nsuch as sentiment, formality, and topic, for which\na number of techniques have been proposed (Prab-\nhumoye et al., 2018; Sudhakar et al., 2019)—we\ndiscuss related work in more detail in §5. How-\never, author-specific textual styles cannot be sum-\nmarized using a closed set of binary or categori-\ncal attributes, since authors may be characterized\nby unique combinations of stylometric features.\nSuch features may include dialect, use of emo-\njis, punctuation and capitalization usage, as well\nas less obvious features such as syntactic prefer-\nences and use of white space. Since it is difficult\neven for forensic linguists to characterize an au-\nthor’s style, we propose to guide generation us-\ning contrastively-trained representations that ex-\ntract stylistic attributes from a given writing sam-\nple as a dense vector feature.1\nDiscriminative control methods generate text\nwith prescribed attributes guided by a classifier\nevaluating the degree to which the text satis-\nfies the target attribute, typically with a tunable\nhyper-parameter balancing the fluency of the gen-\nerated text with control success (Dathathri et al.,\n2019). However, human writing is character-\nized by “dips” into low-probability regions, unlike\nsamples from LMs which produce likely tokens at\n1A conceptually similar approach is used in certain voice\nsynthesis systems, in which speaker representations guide\nqualities of the generated speech (Fang et al., 2019; Ao et al.,\n2021).arXiv:2312.17242v1  [cs.CL]  28 Dec 2023each step (Gehrmann et al., 2019). Thus, the ob-\njective of achieving fluent generations according\nto a generic LM will in general be in tension with\nthe goal of matching an author’s style, which may\nbe characterized by such unlikely token choices.\nTo overcome this challenge, we propose a novel\napproach which combines a style-controlled au-\ntoregressive language model and a discriminative\nobjective which aims to ensure stylistic consis-\ntency at the sequence level.\nTo guide a pre-trained LM towards a target\nstyle, we generalize future discriminators (Yang\nand Klein, 2021) to regression for a target style\nrepresentation. Simply put, our approach entails\nre-scoring the predictive distribution of an exist-\ning LM using a lightweight model that assigns\nhigher likelihood to tokens that are predicted to\nbetter adhere to the target style vector. The re-\nsulting author-specific LM—the composition of a\npre-trained model"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Unsupervised Universal Image Segmentation\nDantong Niu*†Xudong Wang*†Xinyang Han*Long Lian Roei Herzig Trevor Darrell\nBerkeley AI Research, UC Berkeley\nCode: https://github.com/u2seg/U2Seg\nAP50 AR100\n18.5\n11.8\n9.3\n9CutLER\nU2SegUnsup. Semantic Seg. Unsup. Instance Seg. Unsup. Panoptic Seg\nmIoU PixelAcc\n63.9\n30.2\n56.9\n28.2STEGO\nU2Seg\nPQ SQ\n52.7\n17.6\n36.1\n12.4CutLER + STEGO U2Seg\nAP (w/ 1%) AP (w/ 2%)\n22.6\n19.2\n18.8\n14.8CutLER\nU2SegLabel-Efficient Learning\nBenchmark \nResultsSample  \nResultsTasks Unsupervised Semantic Segmentation Unsupervised Instance Segmentation Unsupervised Panoptic Segmentation\n+2.0+7.0\n+9.2\n+1.9\n+5.2+16.6\n+4.4+3.8\n24 2 10 13\nCityscapesPQ SQ\n71.1\n16.1\n64.9\n12.4+3.7+6.2\nCOCO COCO COCO COCO\nFigure 1. We present U2Seg , a unified framework for Unsupervised Universal image Segmentation that consistently outperforms previous\nstate-of-the-art methods designed for individual tasks: CutLER [60] for unsupervised instance segmentation, STEGO [24] for unsupervised\nsemantic segmentation, and the naive combination of CutLER and STEGO for unsupervised panoptic segmentation. We visualize instance\nsegmentation results with “semantic label” + confidence score and semantic predictions with “semantic label”. Zoom in for the best view.\nAbstract\nSeveral unsupervised image segmentation approaches have\nbeen proposed which eliminate the need for dense\nmanually-annotated segmentation masks; current models\nseparately handle either semantic segmentation ( e.g.,\nSTEGO) or class-agnostic instance segmentation ( e.g.,\nCutLER), but not both ( i.e., panoptic segmentation). We\npropose an Unsupervised Universal Segmentation model\n(U2Seg) adept at performing various image segmentation\ntasks—instance, semantic and panoptic—using a novel uni-\nfied framework. U2Seg generates pseudo semantic labels\nfor these segmentation tasks via leveraging self-supervised\nmodels followed by clustering; each cluster represents dif-\nferent semantic and/or instance membership of pixels. We\nthen self-train the model on these pseudo semantic labels,\nyielding substantial performance gains over specialized\n*Equal Contribution.\n†Project Lead.methods tailored to each task: a +2.6 APboxboost (vs.\nCutLER) in unsupervised instance segmentation on COCO\nand a +7.0 PixelAcc increase (vs. STEGO) in unsupervised\nsemantic segmentation on COCOStuff. Moreover, our\nmethod sets up a new baseline for unsupervised panoptic\nsegmentation, which has not been previously explored.\nU2Seg is also a strong pretrained model for few-shot\nsegmentation, surpassing CutLER by +5.0 APmaskwhen\ntrained on a low-data regime, e.g., only 1% COCO labels.\nWe hope our simple yet effective method can inspire more\nresearch on unsupervised universal image segmentation.\n1. Introduction\nThe field of image segmentation has witnessed significant\nadvancements in the recent years [4, 5, 12, 20, 26, 35,\n38, 40]. Nonetheless, the effectiveness of these segmen-\ntation methods heavily depends on the availability of exten-\nsive densely human-labeled data for training these models,arXiv:2312.17243v1  [cs.CV]  28 Dec 2023which is both labor-intensive and costly and thus less scal-\nable. In this paper, our objective is to explore the extent\nto which unsupervised image segmentation can be achieved\nwithout relying on any human-generated labels.\nSeveral recent works such as CutLER [60] and\nSTEGO [24] have emerged as promising approaches for\nunsupervised image segmentation. CutLER leverages the\nproperty of the self-supervised model DINO [8] to ‘dis-\ncover’ objects without supervision, and learns a state-of-\nthe-art localization model on pseudo instance segmenta-\ntion masks produced by MaskCut [60] (based on Normalize\nCuts [45]). Similarly leveraging DINO [8], STEGO [24]\nintroduces a novel framework that distills unsupervised fea-\ntures into discrete semantic labels. This is achieved using a\ncontrastive loss that encourages pixel features to form com-\npact clusters while preserving their relationships across the\ncorpora [24]. However, these methods have limitations:\n• The output of unsupervised instance segmentation meth-\nods such as CutLER [60] comprises class-agnostic seg-\nments for “things”, ignoring the “stuff” categories that\nrepresent pixel semantics. Moreover, CutLER often treats\nseveral overlapping instances as one instance, especially\nwhen these instances belong to the same semantic class.\n• On the other hand, unsupervised semantic segmentation\nmethods such as STEGO [24] focus on the segmentation\nof semantically coherent regions, lacking the capability to\ndistinguish between individual instances.\n•Unsupervised panoptic segmentation has not been ad-\ndressed. Supervised panoptic segmentation methods [12,\n29, 31] predict both “stuff” and “things” classes simulta-\nneously; to the best of our knowledge there has not been\nwork on unsupervised panoptic segmentation heretofore.\nTo address these limitations, we propose U2Seg , a\nnovel Unsupervised Universal image Segmentation model.\nU2Seg offers comprehensive s"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "THELLM S URGEON\nTycho F.A. van der Ouderaa1∗, Markus Nagel2, Mart van Baalen2,\nYuki M. Asano3, Tijmen Blankevoort2\n1Imperial College London ,2Qualcomm AI Research†,3QUV A Lab, University of Amsterdam\nABSTRACT\nState-of-the-art language models are becoming increasingly large in an effort to\nachieve the highest performance on large corpora of available textual data. How-\never, the sheer size of the Transformer architectures makes it difficult to deploy\nmodels within computational, environmental or device-specific constraints. We\nexplore data-driven compression of existing pretrained models as an alternative\nto training smaller models from scratch. To do so, we scale Kronecker-factored\ncurvature approximations of the target loss landscape to large language models.\nIn doing so, we can compute both the dynamic allocation of structures that can\nbe removed as well as updates of remaining weights that account for the removal.\nWe provide a general framework for unstructured, semi-structured and structured\npruning and improve upon weight updates to capture more correlations between\nweights, while remaining computationally efficient. Experimentally, our method\ncan prune rows and columns from a range of OPT models and Llamav2-7B by\n20%-30%, with a negligible loss in performance, and achieve state-of-the-art re-\nsults in unstructured and semi-structured pruning of large language models.\n1 I NTRODUCTION\nRecent advancements in language modeling (Vaswani et al., 2017) allow fitting large language mod-\nels (LLMs) with millions or even billions of parameters (such as OPT (Zhang et al., 2022) and\nLlama 2 (Touvron et al., 2023)) on big text corpora achieving high performance. Unfortunately,\nthe size of these LLMs often makes it hard to deploy them within practical constraints. Cloud-\nbased deployment can get very expensive for larger models, and efficient devices such as phones are\nfrequently limited in the memory size to host a model.\nA body of literature extending back to the late 1980s, e.g., Optimal Brain Damage (OBD, LeCun\net al. (1989)) and Optimal Brain Surgeon (OBS, Hassibi & Stork (1992)), phrases pruning as a con-\nstraint optimization problem to reduce a model’s footprint and runtime requirements. The Hessian\nrequired for this approach grows with the square of the number of parameters, and can only be com-\nputed in practice for unrealistically small networks. To overcome this issue, Eigendamage (Wang\net al., 2019) introduces a Kronecker factorization of a blockwise-diagonal approximation of the\nHessian. Recent works, like Optimal Brain Compression (Frantar & Alistarh, 2022) and SparseGPT\n(Frantar & Alistarh, 2023), demonstrate practical post-training pruning of LLMs, but only consider\na loss curvature of a pruned layer’s squared output reconstruction error, ignoring gradients that relate\nlocal removal costs to the target loss. As a result, their approximation to the target loss landscape\n∗Work done while doing an internship at Qualcomm AI Research\n†Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\nStructured compression (rows and columns) Unstructured compression (matrix elements)\n1.3b 2.7b 6.7b model sizes:14.62\n12.47\n10.86\ntest set\nperplexity\npretrained\nK-OBD\nLLM Surgeon\n1.3b 2.7b 6.7b model sizes:14.62\n12.47\n10.86\ntest set\nperplexity\npretrained\nSparseGPT\nLLM Surgeon\nFigure 1: LLM Surgeon allows interpolation of model size between existing pretrained models.\n1arXiv:2312.17244v1  [cs.LG]  28 Dec 2023is inaccurate, leading to a significant performance degradation for pruned LLMs. Further, these\nmethods do not readily extend to structured pruning.\nThis work introduces LLM Surgeon, a general framework for unstructured, semi-structured and\nstructured pruning of LLMs. To the best of our knowledge, this is the first method to successfully\nperform structured pruning for LLMs. This is achieved by scaling up the block-diagonal Kronecker-\nfactorized approximations to the empirical Fisher from Eigendamage to LLMs. We further expand\nupon the work by deriving OBS-like weight pruning costs and updates for structured pruning of\nmultiple rows and columns, and provide a general framework that also incorporates semi-structured\nand unstructured pruning. Instead of treating individual weight updates independently, we strive\nto consider as many correlations between weights as practically possible and derive joint weight\nupdates for pruning multiple weights (or multiple sets of structured weights) at once. Unlike prior\nwork in LLM pruning, LLM Surgeon prunes in multiple shots, updating weights and curvature es-\ntimates between shots. We use global thresholding for unstructured, semi-structured and structured,\ni.e., instead of pruning layers by a fixed amount, more sensitive layers are pruned less than those\nthat are more robust. Lastly, we propose to mitigate possible first-order gradients not being zero by\nusing optional low-rank first-order updates between shots. A key advantage of LLM Surgeon is that\nit allows tradin"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Amodal Ground Truth and Completion in the Wild\nGuanqi Zhan1, Chuanxia Zheng1, Weidi Xie1,2, Andrew Zisserman1\nVisual Geometry Group, University of Oxford1\nCoop. Medianet Innovation Center, Shanghai Jiao Tong University2\n{guanqi,cxzheng,weidi,az}@robots.ox.ac.uk\nAbstract\nThe problem we study in this paper is amodal image\nsegmentation: predicting entire object segmentation masks\nincluding both visible and invisible (occluded) parts. In\nprevious work, the amodal segmentation ground truth on\nreal images is usually predicted by manual annotaton and\nthus is subjective. In contrast, we use 3D data to estab-\nlish an automatic pipeline to determine authentic ground\ntruth amodal masks for partially occluded objects in real\nimages. This pipeline is used to construct an amodal com-\npletion evaluation benchmark, MP3D-Amodal , consisting of\na variety of object categories and labels. To better handle\nthe amodal completion task in the wild, we explore two ar-\nchitecture variants: a two-stage model that first infers the\noccluder, followed by amodal mask completion; and a one-\nstage model that exploits the representation power of Stable\nDiffusion for amodal segmentation across many categories.\nWithout bells and whistles, our method achieves a new state-\nof-the-art performance on Amodal segmentation datasets\nthat cover a large variety of objects, including COCOA and\nour new MP3D-Amodal dataset. The dataset, model, and\ncode are available at https://www.robots.ox.ac.\nuk/~vgg/research/amodal/ .\n1. Introduction\nThe vision community has rapidly improved instance seg-\nmentation performance over the last few years with a suc-\ncession of powerful models, such as Mask-RCNN [ 13],\nMask2Former [ 4], and Seg-Anything (SAM) [ 21]. How-\never, despite this remarkable progress, these models only\nprovide pixel-level modal segmentations for objects in the\nimages, i.e., the instance masks are for the visible pixels.\nThe models lack the human ability to infer the full extent of\nthe object in an image, when it is partially occluded. The\nprediction of amodal masks , which covers the full extent\nof the object, can assist several downstream tasks includ-\ning object detection [ 42], smart image editing [ 27,44], 3D\nreconstruction from a single image [ 15,16,39,48], objectpermanence in video segmentation [ 36,41], predicting sup-\nport relationships between objects [ 33,47], and more gen-\nerally for planning in manipulation and navigational tasks\nwhere reasoning on the full extent of the object could lead to\nimprovements [14, 18, 19, 37, 38].\nPredicting amodal masks for objects in 2D images is\nchallenging: this is because real scenes contain a vast col-\nlection of different types of objects, resulting in complex\nocclusions when they are projected to 2D images from a 3D\nphysical world. To accurately complete the amodal shape\nof partially occluded objects requires a prediction of occlu-\nsion relations (in order to infer if and where the object is\npartially occluded), as well as predicting the shape of the\noccluded regions. This challenge is also reflected in the type\nof amodal datasets available which are mostly synthetic –\nfor real images, amodal masks are almost always ‘imagined’\nby human annotators providing the contour of the amodal\nmask, and there is no dataset available to evaluate amodal\ncompletions with authentic ground truth annotations for a\nlarge variety of object categories (see Table 1).\nIn this paper, our first contribution is to provide a new\namodal benchmark evaluation dataset based on authentic\nground truth amodal segmentation for real images, and cov-\nering a large variety of objects. The new dataset is called\nMP3D-Amodal , and the amodal mask is obtained by pro-\njecting the 3D structure of objects in the scene onto the\nimage (Figure 1 bottom). We build the dataset from Mat-\nterPort3D [ 2] that has both 3D structure and real images\nof indoor scenes. The dataset and generation method is\ndescribed in Section 3.\nIn most prior work, amodal completion algorithms re-\nquired the occluder mask to be specified [ 29,44] (Figure 1\ntop). Our second contribution is to propose two architecture\nvariants that do not require the occluder mask to be supplied:\nOccAmodal , a two-stage model that first infers the occluder,\nfollowed by amodal mask compeletion; and SDAmodal , a\none-stage model that exploits the power of pre-trained Stable\nDiffusion for amodal mask completion. These architectures\nare described in Section 4.\nWe achieve state-of-the-art amodal completion perfor-\n1arXiv:2312.17247v1  [cs.CV]  28 Dec 2023AmodalCompletionModal Segmentation𝑀!\nAmodal Segmentation𝐴!\n3DModelandProjection\nOriginalImage\nOccluderMask𝐹!\nFigure 1. Amodal Ground Truth and Completion in the Wild.\nTop: The task of amodal completion is to predict the amodal mask\nAifor an object ‘ i’ in the image specified by the modal mask Mi\n(here the object is the rear motorbike). Previous methods [ 29,44]\nrequire the mask of the occluder Fito be also provided to do the\ntask; but our goal is "}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Rethinking Model-based, Policy-based, and Value-based\nReinforcement Learning via the Lens of Representation Complexity\nGuhao Feng∗†Han Zhong∗‡\nAbstract\nReinforcement Learning (RL) encompasses diverse paradigms, including model-based\nRL, policy-based RL, and value-based RL, each tailored to approximate the model,\noptimal policy, and optimal value function, respectively. This work investigates the\npotential hierarchy of representation complexity — the complexity of functions to be\nrepresented — among these RL paradigms. We first demonstrate that, for a broad class\nof Markov decision processes (MDPs), the model can be represented by constant-depth\ncircuits with polynomial size or Multi-Layer Perceptrons (MLPs) with constant layers\nand polynomial hidden dimension. However, the representation of the optimal policy and\noptimal value proves to be NP-complete and unattainable by constant-layer MLPs with\npolynomial size. This demonstrates a significant representation complexity gap between\nmodel-based RL and model-free RL, which includes policy-based RL and value-based\nRL. To further explore the representation complexity hierarchy between policy-based RL\nand value-based RL, we introduce another general class of MDPs where both the model\nand optimal policy can be represented by constant-depth circuits with polynomial size or\nconstant-layer MLPs with polynomial size. In contrast, representing the optimal value is\nP-complete and intractable via a constant-layer MLP with polynomial hidden dimension.\nThis accentuates the intricate representation complexity associated with value-based\nRL compared to policy-based RL. In summary, we unveil a potential representation\ncomplexity hierarchy within RL — representing the model emerges as the easiest task,\nfollowed by the optimal policy, while representing the optimal value function presents\nthe most intricate challenge.\n1 Introduction\nThe past few years have witnessed the tremendous success of Reinforcement Learning (RL) (Sutton\nand Barto, 2018) in solving intricate real-world decision-making problems, such as Go (Silver et al.,\n2016) and robotics (Kober et al., 2013). These successes can be largely attributed to powerful\nfunction approximators, particularly Neural Networks (NN) (LeCun et al., 2015), and the evolution\nof modern RL algorithms. These algorithms can be categorized into model-based RL ,policy-based\n∗Alphabetical order.\n†School of EECS, Peking University. Email: fenguhao@stu.pku.edu.cn\n‡Center for Data Science, Peking University. Email: hanzhong@stu.pku.edu.cn\n1arXiv:2312.17248v1  [cs.LG]  28 Dec 2023RL, and value-based RL based on their respective objectives of approximating the underlying model,\noptimal policy, or optimal value function.\nDespite the extensive theoretical analysis of RL algorithms in terms of statistical error (e.g.,\nAzar et al., 2017; Jiang et al., 2017; Jin et al., 2020, 2021; Du et al., 2021; Foster et al., 2021; Zhong\net al., 2022; Xu and Zeevi, 2023) and optimization error (e.g., Agarwal et al., 2021; Xiao, 2022;\nCen et al., 2022; Lan, 2023) lenses, a pivotal perspective often left in the shadows is approximation\nerror . Specifically, the existing literature predominantly relies on the (approximate) realizability\nassumption, assuming that the given function class can sufficiently capture the underlying model,\noptimal value function, or optimal policy. However, limited works examine the representation\ncomplexity in different RL paradigms — the complexity of the function class needed to represent\nthe underlying model, optimal policy, or optimal value function. In particular, the following problem\nremains elusive:\nIs there a representation complexity hierarchy among different RL paradigms,\nincluding model-based RL, policy-based RL, and value-based RL?\nTo our best knowledge, the theoretical exploration of this question is limited, with only two\nexceptions (Dong et al., 2020; Zhu et al., 2023). Dong et al. (2020) employs piecewise linear\nfunctions to represent both the model and value functions, utilizing the number of linear pieces as a\nmetric for representation complexity. They construct a class of Markov Decision Processes (MDPs)\nwhere the underlying model can be represented by a constant piecewise linear function, while the\noptimal value function necessitates an exponential number of linear pieces for representation. This\ndisparity underscores that the model’s representation complexity is comparatively less than that of\nvalue functions. Recently, Zhu et al. (2023) reinforced this insight through a more rigorous circuit\ncomplexity perspective. They introduce a class of MDPs wherein the model can be represented\nby circuits with polynomial size and constant depth, while the optimal value function cannot.\nHowever, the separation between model-based RL and value-based RL demonstrated in Zhu et al.\n(2023) may not be deemed significant (cf. Remark 5.6). More importantly, Dong et al. (2020); Zhu\net al. (2023) do not consider policy-based RL and do not co"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "Do Androids Know They’re Only Dreaming of Electric Sheep?\nSky CH-Wang◦∗Benjamin Van Durme•Jason Eisner•Chris Kedzie•\n◦Department of Computer Science, Columbia University\n•Microsoft Semantic Machines\nskywang@cs.columbia.edu, chriskedzie@microsoft.com\nAbstract\nWe design probes trained on the internal repre-\nsentations of a transformer language model that\nare predictive of its hallucinatory behavior on\nin-context generation tasks. To facilitate this\ndetection, we create a span-annotated dataset of\norganic and synthetic hallucinations over sev-\neral tasks. We find that probes trained on the\nforce-decoded states of synthetic hallucinations\nare generally ecologically invalid in organic\nhallucination detection. Furthermore, hidden\nstate information about hallucination appears\nto be task and distribution-dependent. Intrin-\nsic and extrinsic hallucination saliency varies\nacross layers, hidden state types, and tasks; no-\ntably, extrinsic hallucinations tend to be more\nsalient in a transformer’s internal representa-\ntions. Outperforming multiple contemporary\nbaselines, we show that probing is a feasible\nand efficient alternative to language model hal-\nlucination evaluation when model states are\navailable.\n1 Introduction\nDo language models know when they’re hallucinat-\ning?1Detecting hallucinations in grounded gener-\nation is commonly framed as a textual entailment\nproblem. Prior work has largely focused on cre-\nating secondary detection models trained on and\napplied to surface text (Ji et al., 2023). However,\nthese secondary models are often of comparable\ncomplexity to the underlying language model that\ngenerated the error and ignore the information that\nwas already computed during generation. In this\nwork, we explore the degree to which probes on a\ndecoder-only transformer language model’s feed-\nforward and self-attention states can detect halluci-\nnations in a variety of grounded generation tasks.\nIn particular, as the language model generates a\nresponse, we probe its encoded tokens to determine\n∗Work done as an intern at Microsoft Semantic Machines.\n1That is, do they know that their electric sheep are un-\ngrounded?\n(First Token   Last Token)\nT oken In Generated ResponseTransformer Layer\n(Deep   Shallow)\n(Grounded   Hallucination)\nT oken Hallucination Prediction by Single Layer ClassifiersFigure 1: Prediction of hallucination in a generated re-\nsponse by probing the hidden states of a transformer dur-\ning decoding. The true annotated span of hallucination\nwithin the response is boxed in white; rows represent\nprobes trained to detect hallucination from layers within\na transformer’s hidden states, with columns representing\ntheir prediction for each token in a generated response.\nwhether the model has just begun to hallucinate or\nwill eventually be found to have hallucinated. It\nis plausible that these encodings are “aware” of\nwhether generation is retrieving information that is\ngrounded in the prompt versus filling in plausible\ninformation that is stored in the model’s weights.\nRelated work has studied how transformers retrieve\nrelevant memories as they encode text (Meng et al.,\n2022, 2023) and suggests that feed-forward layers\nact as associative memory stores (Geva et al., 2021).\nA probe for hallucination detection must consider\nwhether subsequent generated text is entailed by\ninformation in the prompt, given the retrieved mem-\nories, or is merely one plausible extrapolation from\nthe prompt (see also Monea et al., 2023).arXiv:2312.17249v1  [cs.CL]  28 Dec 2023We develop three probes of increasing complex-\nity across three grounded generation tasks: abstrac-\ntive summarization, knowledge-grounded dialogue\ngeneration, and data-to-text. For each task, we\ncollect hallucinations in two ways: (1) from sam-\npled responses, where we generate outputs from a\nlarge language model (LLM) conditioned on the\ninputs, or (2) by editing reference inputs or outputs\nto create discrepancies. We refer to these as organic\nand synthetic data, respectively. In both cases, we\nhave human annotators mark hallucinations in the\noutputs. Method (2) produces hallucination annota-\ntions at a higher rate, though we find that the utility\nof these synthetic examples is lower as they do not\ncome from the test distribution.\nWe summarize our contributions as follows:\n•We produce a high-quality dataset of more\nthan 15k utterances with hallucination anno-\ntations for organic and synthetic output texts\nacross three grounded generation tasks.\n•We propose three probe architectures for de-\ntecting hallucinations and demonstrate im-\nprovements over multiple contemporary base-\nlines in hallucination detection.\n•We analyze how probe accuracy is affected by\nannotation type (synthetic/organic), hallucina-\ntion type (extrinsic/intrinsic), model size, and\nwhich part of the encoding is probed.\n2 Related Work\nDefinitions. We center our study of hallucina-\ntions in the setting of in-context generation (Lewis\net al., 2020) where grounding knowledge sources\nare provided wi"}
{"date": "2023-12-31-11-11", "error": false, "url": "PDF", "text_blocks": "iFusion : Inverting Diffusion for Pose-Free Reconstruction from Sparse Views\nChin-Hsuan Wu∗1Yen-Chun Chen2Bolivar Solarte1Lu Yuan2Min Sun1,3\n1National Tsing Hua University2Microsoft3Amazon\n{chinhsuanwu, enrique.solarte.nthu }@gapp.nthu.edu.tw\n{yen-chun.chen, luyuan }@microsoft.com sunmin@ee.nthu.edu.tw\nchinhsuanwu.github.io/ifusion\nInput ViewsGenerated Novel ViewsTextured Mesh\nFigure 1. Demonstration on real-world 3D reconstruction. With only two casually taken photos without camera poses ,iFusion can\nreconstruct plausible 3D assets. The top row example is taken from DreamBooth3D [51], and we took photos for the cat statue by ourselves.\nAbstract\nWe present iFusion, a novel 3D object reconstruction\nframework that requires only two views with unknown cam-\nera poses. While single-view reconstruction yields visually\nappealing results, it can deviate significantly from the ac-\ntual object, especially on unseen sides. Additional views\nimprove reconstruction fidelity but necessitate known cam-\nera poses. However, assuming the availability of pose may\nbe unrealistic, and existing pose estimators fail in sparse-\nview scenarios. To address this, we harness a pre-trained\nnovel view synthesis diffusion model, which embeds implicit\nknowledge about the geometry and appearance of diverse\nobjects. Our strategy unfolds in three steps: (1) We invert\nthe diffusion model for camera pose estimation instead of\nsynthesizing novel views. (2) The diffusion model is fine-\ntuned using provided views and estimated poses, turned\ninto a novel view synthesizer tailored for the target object.\n(3) Leveraging registered views and the fine-tuned diffusion\nmodel, we reconstruct the 3D object. Experiments demon-\nstrate strong performance in both pose estimation and novel\nview synthesis. Moreover, iFusion seamlessly integrates\nwith various reconstruction methods and enhances them.\n∗Part of this work was done as a research intern at Microsoft.1. Introduction\nReconstructing objects from sparse views poses a signifi-\ncant challenge yet holds paramount importance for various\napplications, including 3D content creation, augmented re-\nality, virtual reality, and robotics. Recent breakthroughs,\nguided by pre-trained models, have facilitated visually plau-\nsible reconstructions from a single view, without requiring\nthe camera pose [30, 31, 36, 48, 64, 65, 79]. However, the\nreconstructed assets might not precisely capture the actual\nobjects due to the inherent single-view ambiguity, e.g., the\nobject’s side opposite to the camera can only be imagined.\nFurthermore, multiple potential 3D structures could corre-\nspond to the same input image.\nOn the other hand, sparse-view methods assume the\navailability of an accurate camera pose for each view [3, 17,\n24, 33, 63, 77, 86]. To meet this requirement, a Structure-\nfrom-Motion (SfM) pre-processing, e.g., COLMAP [55],\nis typically employed. Paradoxically, these methods de-\nmand a substantial number of images, usually more than 50\nin practice, for reliable pose estimation. Recent learning-\nbased pose estimation [27, 58, 82] and pose-free reconstruc-\ntion [19, 20] have sought to alleviate this issue. However,\nthey still require a minimum of five input views and are pri-\nmarily demonstrated on objects with simple 3D geometry\n1arXiv:2312.17250v1  [cs.CV]  28 Dec 2023or within a constrained set of object categories. A generic\nframework for pose-free, sparse-view 3D reconstruction is\nstill lacking, posing a significant obstacle to real-world ap-\nplications with casually captured photos. We hereby raise\nthe research question: How can one utilize only extremely\nsparse views without poses while maintaining the recon-\nstruction fidelity of diverse objects?\nThe key is a sparse-view pose estimator. Our motiva-\ntion stems from a recent novel view synthesis diffusion\nmodel, namely Zero123 [31], which is pre-trained on the\nmost extensive 3D object dataset to date [8]. Given a refer-\nence view image, Zero123 can generate a novel view (query\nview) from a specified pose (Fig. 2, left). This indicates\nthat the model has learned rich prior knowledge about the\ngeometry and appearance of diverse objects. We thus hy-\npothesize that it can be leveraged for pose estimation, with\nan intuition that a well-estimated pose fed into Zero123\nwill produce an image similar to the query view, and vice\nversa. Next, gradients may be back-propagated to op-\ntimize the pose with a proper loss function. Following\nthis idea, we repurpose Zero123 by inverting it to take\nthe two views and estimate the relative camera transforma-\ntion (Fig. 2, right). More specifically, we adopt an analysis-\nby-synthesis paradigm [7, 45, 78] that optimizes the trans-\nformation by minimizing the difference between the de-\nnoised latent visual features, i.e., Zero123’s output image\nfeature map, and the query view’s feature. Empirically, the\nproposed approach achieves strong pose estimation with as\nfew as 2 views, even outperforming existing approaches’\nresults with 5 views.\n"}
