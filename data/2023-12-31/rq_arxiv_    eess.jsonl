{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Implied volatility (also) is path-dependent\nHervé Andrès1,2, Alexandre Boumezoued1, and Benjamin Jourdain2\n1Milliman R&D, Paris, France\n2CERMICS, École des Ponts, INRIA, Marne-la-Vallée, France.\nDecember 27, 2023\nAbstract\nWe propose a new model for the coherent forecasting of both the implied volatility\nsurfaces and the underlying asset returns. In the spirit of Guyon and Lekeufack (2023) who\nare interested in the dependence of volatility indices (e.g. the VIX) on the paths of the\nassociated equity indices (e.g. the S&P 500), we first study how implied volatility can be\npredicted using the past trajectory of the underlying asset price. Our empirical study reveals\nthat a large part of the movements of the at-the-money-forward implied volatility for up to\ntwo years maturities can be explained using the past returns and their squares. Moreover,\nwe show that up to four years of the past evolution of the underlying price should be used\nfor the prediction and that this feedback effect gets weaker when the maturity increases.\nBuilding on this new stylized fact, we fit to historical data a parsimonious version of the\nSSVIparameterization(GatheralandJacquier, 2014)oftheimpliedvolatilitysurfacerelying\non only four parameters and show that the two parameters ruling the at-the-money-forward\nimplied volatility as a function of the maturity exhibit a path-dependent behavior with\nrespect to the underlying asset price. Finally, we propose a model for the joint dynamics\nof the implied volatility surface and the underlying asset price. The latter is modelled\nusing a variant of the path-dependent volatility model of Guyon and Lekeufack and the\nformer is obtained by adding a feedback effect of the underlying asset price onto the two\nparameters ruling the at-the-money-forward implied volatility in the parsimonious SSVI\nparameterization and by specifying a hidden semi-Markov diffusion model for the residuals\nof these two parameters and the two other parameters. Thanks to this model, we are able\nto simulate highly realistic paths of implied volatility surfaces that are arbitrage-free.\nKeywords: implied volatility modelling, SSVI, path-dependent volatility\n1. Introduction\nOne of the many reasons of the success of the Black-Scholes model (Black and Scholes, 1973) is the\nexistence of a one-to-one correspondence between the price C(K, T)of an European call option with\nstrike Kand maturity Tand the volatility σof the geometric Brownian motion modelling the dynamics\nof the underlying asset price (St)t≥0provided that (S0−Ke−rT)+< C(K, T)< S 0(ris the constant\nrisk-free rate) which is guaranteed by absence of arbitrage opportunities. When this condition is satis-\nfied, the unique parameter σsatisfying CBS(K, T, σ ) =C(K, T), where CBSdenotes the Black-Scholes\ncall option price, is called the implied volatility of the call option. By the put-call parity, the implied\nvolatility of the put option is equal to the one of the call option with same maturity and strike. Although\nthe implied volatility does not add any new information with respect to the option price, it is commonly\nused to quote option prices on the markets mainly because it allows to easily compare the value of two\noptions with different underlying assets while the option price heavily depends on the underlying asset\n1arXiv:2312.15950v1  [q-fin.CP]  26 Dec 2023price level, making the comparison more difficult. If the Black-Scholes model was an accurate description\nof financial markets, the implied volatility should be the same for all options on a given asset regardless\nof the maturity and the strike. The computation of the implied volatility from market option prices\nshows that the implied volatility actually depends on the maturity and the strike which invalidates the\nBlack-Scholes model. The so-called implied volatility surface (IVS) (K, T)7→σBS(K, T)permits to fully\ndescribe the option prices on a given asset.\nIt is also well-known that the level and the shape of the IVS varies with time. To be able to jointly\nmodel the time evolution of the IVS and the underlying asset price is key for applications covering asset\nallocation, risk management and hedging. First, such a model allows to backtest or study the P&L\ndistribution of an investment stragegy involving options and the underlying asset. One can think for\nexample of the strategy consisting in buying a stock and a put of strike K1and selling a put of strike\nK2with K2< K 1but with same maturity (this is called a put spread). This strategy protects the\ninvestor against a drop in the underlying asset price down to the K2threshold in exchange to a lower\npremium in comparison to just buying a put of strike K1. By extension, the modelling of the IVS and\nthe underlying asset price makes it possible to optimize an asset allocation strategy involving options.\nAnother application relates to the design and the backtesting of hedging strategies for financial products\n(e.g. volatility swaps, options on the VIX, etc.) having a volatili"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Unsupervised Learning of Phylogenetic Trees via\nSplit-Weight Embedding\nYibo Kong\nDepartment of Computer Science\nUniversity of Wisconsin-Madison\nMadison, WI 53706George P. Tiley\nKew Royal Botanic Gardens\nKew, Richmond, TW9 3AE,\nLondon, UK\nClaudia Sol´ ıs-Lemus∗\nWisconsin Institute for Discovery\nDepartment of Plant Pathology\nUniversity of Wisconsin-Madison\nMadison, WI 53706\nJanuary 2023\nAbstract\nUnsupervised learning has become a staple in classical machine learning, successfully identi-\nfying clustering patterns in data across a broad range of domain applications. Surprisingly,\ndespite its accuracy and elegant simplicity, unsupervised learning has not been sufficiently\nexploited in the realm of phylogenetic tree inference. The main reason for the delay in adop-\ntion of unsupervised learning in phylogenetics is the lack of a meaningful, yet simple, way of\nembedding phylogenetic trees into a vector space. Here, we propose the simple yet power-\nful split-weight embedding which allows us to fit standard clustering algorithms to the space\nof phylogenetic trees. We show that our split-weight embedded clustering is able to recover\nmeaningful evolutionary relationships in simulated and real ( Adansonia baobabs) data.\n1 Introduction\nThe Tree of Life is a massive graphical structure which represents the evolutionary process from single\ncell organisms into the immense biodiversity of living species in present time. Estimating the Tree of Life\nwould not only represent the greatest accomplishment in evolutionary biology and systematics, but it would\nalso allow us to fully understand the development and evolution of important biological traits in nature, in\nparticular, those related to resilience to extinction when exposed to environmental threats such as climate\nchange. Therefore, the development of statistical and machine-learning theory to reconstruct the Tree of Life,\nespecially those scalable to big data, are paramount in evolutionary biology, systematics, and conservation\nefforts against mass extinctions.\nGraphical structures that represent evolutionary processes are denoted phylogenetic trees . A phylogenetic\ntree is a binary tree whose internal nodes represent ancestral species that over time differentiate into two\nseparate species giving rise to its two children nodes (see Figure 1 left). The evolutionary process is then\ndepicted by this bifurcating tree from the root (the origin of life) to the external nodes of the tree (also\ndenoted leaves) which represent the living organisms today. Mathematically, a rooted phylogenetic tree Ton\ntaxon set Xis a connected directed acyclic graph with vertices V={r} ∪VL∪VT, edges Eand a bijective\nleaf-labeling function f:VL→Xsuch that the root rhas indegree 0 and outdegree 2; any leaf v∈VLhas\nindegree 1 and outdegree 0, and any internal node v∈VThas indegree 1 and outdegree 2. An unrooted tree\n∗Corresponding author: solislemus@wisc.edu\n1arXiv:2312.16074v1  [q-bio.PE]  26 Dec 2023results from the removal of the root node rand the merging of the two edges leading to the outgroup (taxon\n4 in Figure 1 left). Traditionally, phylogenetic trees are drawn without nodes (Figure 1 center) given that\nonly the bifurcating pattern is necessary to understand the evolutionary process. The specific bifurcating\npattern (without edge weights) is denoted the tree topology. Edges in the tree have weight we∈(0,∞) that\ncan represent different units, evolutionary time or expected substitutions per site being the most common.\nFigure 1: Left: Phylogenetic tree in 4 taxa. Internal (gray) nodes represent speciation events in which an\nancestral species differentiates into two. External (blue) nodes, also denoted leaves, represent living species\n(here denoted 1,2,3,4). Edge weights (in gray) also denoted branch lengths can represent evolutionary time\nor expected substitutions per site. Center: Different phylogenetic tree on the same 4 taxa in which taxon\n2 is grouped with taxon 3 rather than with taxon 1. Nodes are no longer drawn as is the most common\nrepresentation of phylogenetic trees. Right: Phylogenetic tree with gene flow event depicted as a green\narrow. This biological scenario represents the possibility that some genes have the evolutionary history of\nthe phylogenetic tree on the left (with clade (1 ,2)) and some genes, the evolutionary history of the center\ntree (with clade (2 ,3)).\nOne of the main challenges when inferring phylogenetic trees is the fact that different genes in the data\ncan have different evolutionary histories due to biological processes such as introgression, hybridization or\nhorizontal gene transfer [33, 12, 28]. An example is depicted in Figure 1 (right) which has one gene flow\nevent drawn as a green arrow. This gene flow event represents the biological scenario in which some genes in\ntaxon 2 get transferred from the lineage of taxon 3, and thus, when reconstructing the evolutionary history\nof this group of four taxa, some genes will depict the phylogenetic tree that clusters "}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.16086v1  [q-bio.NC]  26 Dec 2023NOTES ON RETROACTIVE INTERFERENCE MODEL OF\nFORGETTING\nMikhail Katkov\nDepartment of Brain Sciences\nWeizmann Institute of Science\nRehovot, 76100 Israel\nSchool of Natural Sciences\nInstitute for Advanced Study\nPrinceton, NJ\nmikhail.katkov@gmail.com\nABSTRACT\nWe present analytical derivation of the minimal and maximal number of items retained in recently\nintroduced Retroactive Interference Model of Forgetting. Also we computed the probability that two\nitems presented at different times are retained in the memor y at a later time analytically.\nKeywords retrograde interference ·Markov process ·retention curve\nIntroduction\nStudying human memory is a complex task, since there are pres umably many interacting processes that are hard to\nisolate. Traditionally models in psychology of memory are a ttempting to describe many processes at once by creating\na very complicated mathematical model that have a lot of para meters, hard to analyse, and usually require a separate\nﬁtting of parameters for each experiment. Therefore, since parameters have to adjusted for each measurement, it is\nnot clear how good these models would describe memory proces ses outside of laboratory settings. We have recently\nproposed a different type of models to describe human memory that are based on few assumptions and have zero or\nfew parameters that describe the class of stimuli, but indep endent of experimental settings. These types of models,\nif validated, have a much broader applicability in everyday life settings. We have recently presented mathematical\nmodels for memory forgetting and retrieval [1]. In this publ ication we have asked about mathematical properties of\nforgetting model that may help design experiments to valida te the underlying mathematical construction. In this note\nwe provide answers to 2 questions raised in that publication regarding the forgetting model.\nExperimentally, forgetting is traditionally measured in t he form of retention function ( RC(t)) - the probability that\nmemory is retained for time tsince acquisition[2]. People observe that retention funct ion monotonically decreasing\nwith time and although there are debates on the form of retent ion function one of the best candidate is power function\nof time. One of the popular explanation of memory forgetting in humans is retrograde interference [3]. It assumes\nthat new incoming memories are interacting with stored memo ries and cause some past memories disappear. There\nare different approaches to model forgetting (see for examp le [4]), but here we are concentrating on consequences of\nour mathematical model [5] for possible experimental valid ating of underlying assumptions behind our model. The\nmodel assumes that each incoming memory (one memory at one di screte time step) has ndimensional valence vector,\nwith components being iid random variables. Every incoming memory is added to the memory pool, erasing all\nstored memories that have smaller valences in all dimension s. This model can be solved analytically, and the resulting\nretention curve agree well with experiment. Nevertheless, it is not clear to what extent the underlying principles hold s\nduring retention of memory items. We have posed recently sev eral mathematical questions that may provide additional\nexperimental tests related to this issue[1].Forgetting Calculations\nWe consider the forgetting model III from [1]. It states that there is a retention process, where at each time step a\nnew memory is presented to the process. Memory in the model is characterized by a vector of valences vk∈R,k=\n1..n, wherenis a single integer parameter of the model representing the d imensionality of the model. Valences of\nincoming memory is assumed to be sampled from arbitrary stat ionary distribution (absolutely continuous). Incoming\nmemory erases all currently stored memories that have all va lences smaller that corresponding valencies of incoming\nmemory. Finally, incoming memory is added to stored memorie s. Formally, all incoming memories are described by\ncollection of valences V={vk,t∈R,k= 1..n,t∈N}. For each time Twe can deﬁne a set of stored memories\nM(T;V) ={t1,...t|M(T)|},tk< T which contains presentation times ( t1,...t|M(T)|) of stored memories, where the\ncardinality of M(T;V)(|M(T;V)|) represents a number of stored memories at time T. At time T+1a new memory\nwith valences vk,T+1is presented, and the set of stored memories is updated M(T+ 1;V) ={T+ 1;tm:tm∈\nM(T;V),∃k|vk,tm> vk,T+1}. One can ask what is expected value of |M(T)|that is referred to as retention curve\nRCn(T) =E(|M(T;V)|)with respect to distribution of vk,t. It turns out that retention curve does not depend on the\nparticular distribution of valences, since it depends only on the order statistics of memory items in each dimension[5,\n1].\nMinimal number of retained items\nWe proposed two kind of tests that can potentially check the v alidity of the model [1]. Both are related to partialy\nordered set (poset) nature o"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "1 \n Expanding to Arbitrary Study Designs: ANOVA to Estimate Limits of \nAgreement for MRMC Studies  \nSi Wena* and Brandon D. Gallasa \naDivision of Imaging, Diagnostics, and Software Reliability, Office of Science and Engineering \nLaboratories, Center for Devices and Radiological Health, U.S. FDA, Silver Spring, USA  \n* Correspondence to: Si Wen. Email: si.wen@fda.hhs.gov \n  2 \n Expanding to Arbitrary Study Designs: ANOVA to Estimate Limits of \nAgreement for MRMC Studies  \nA multi-reader multi -case (MRMC) analysis i s applied to account for both reader and case \nvariability when evaluating the clinical performance of a medical imaging device or reader \nperformance under different reading modalities. For a clinical task that measures a \nquantitative biomarker an agreement  analysis, such as limits of agreement (LOA), can be \nused. In this work, we decompose the total variation in the data using a three -way mixed \neffect ANOVA model to estimate the MRMC variance of individual differences and the \nLOA between different reading m odalities. There are rules for writing down the \nexpectation of the mean squares in terms of the variance components for fully -crossed data, \ni.e. data where all the readers read all the cases in all modalities being studied. Sometimes \nthe annotation task is  labor -intensive and time -consuming or distributed across sites, so that \na fully -crossed study is not practical. In this work, we focus on estimating the MRMC \nvariance in the within - and between -readers and within - and between -modalities LOA for \nan arbitrary study design. Simulation studies were conducted to validate the LOA variance \nestimates. The method was also applied to a dataset to compare pathologist performance for assessing the density of stromal tumor infiltrating lymphocytes on different platform s. \nKeywords: limits of agreement ; MRMC study ; ANOVA for unbalanced data;  variance \ncomponents  \n1.Introduction  \nA multi -reader multi -case (MRMC)  study is usually appl ied to evaluate whether a medical \nimaging device can improve the clinical performance of the image readers.(Wagner et al. 2007; \nGallas et al. 2012; Obuc howski and Bullen 2022)  In the study, qualitative or quantitative \nassessment s are collected and compared from multiple readers  (radiologists/ pathologists) \nreviewing multiple  cases  (images)  under different reading modalities . For example,  suppose that \nthere is an AI/ML algorithm enabled medical device supporting the pathologist s to evaluate the \ndensity of stromal tumor infiltrating lymphocytes  (sTIL s) on digital slides . The density of sTILs  3 \n is prognostic for survival in breast cancers and  is visual ly assessed  on routine hematoxylin and \neosin (H&E)- stained slides.(Kos et al. 2020)  To validate the performance of the device  in the \nhands of the reader s we collect estimates of the density of sTILs from  pathologists with and \nwithout the assistance of the de vice. Then  we compare the closeness or agreement between  the \nquantitative  measurements , sTIL scores, from different modalities  or readers . We refer to such \nstudies as agreement studies.  \nOur p revious work(Wen and Gallas 2022)  generalized the limits of agreement (LOA) \nmethod (Bland and Altman 1986, 1999) , a widely used agreement analysis, to MRMC analysis \nthat account s for reader and case variability for fully -crossed study designs . In a fully -crossed \nstudy, all the readers provide  measurements for  all the cases for both modalities  as shown in  \nFigure 1.e . In some studies,  the annotation tasks  are labor -intensive and time -consuming or \ndistributed across sites, so that a fully -crossed study is not practical. In this work, we focus on \nMRMC stu dies that are not fully crossed , and we generalize the MRMC analysis to treat  \narbitrary study designs . Some examples are described  in Figure 1.a -Figure 1.d.  \nAn ANOVA model is commonly used to analyze MRMC data,(Beiden et al. 2000; \nDorfman et al. 1992; Gallas et al. 2009; Obuchowski 1995)  as the data are likely correlated when \nthe readers evaluat e the same set of cases and each case is reviewed by multiple readers. A  three -\nway mixed effect ANOVA is  used to estimate different types of LOA for a fully -crossed MRMC \nstudy.(Wen and Gallas 2022)  In ANOVA, th e data generated from a fully -crossed MRMC study \nare called balanced data , as each reader or case has the same number of readings . There are rul es \nfor calculating the sum of squares (SS ) and expected mean squares (MS)  when the data are \nbalanced .(Montgomery 2012)  The expected mean squares link the variance components for the \nrandom effects in the ANOVA model to the SS computed from the data . Then, the variance 4 \n components can be estimated from the data and used to estimate the MRMC variance in  LOA.  \nHowever, in an arbitrary MRMC study, t he data are unbalanced. To determine the relationship \nbetween expected MS  and the variance components in the ANOVA model, we fi"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "arXiv:2312.16099v1  [econ.EM]  26 Dec 2023Direct Multi-Step Forecast based Comparison\nof Nested Models via an Encompassing Test\nJean-Yves Pitarakis∗\nDepartment of Economics\nUniversity of Southampton\nDecember 27, 2023\nAbstract\nWe introduce a novel approach for comparing out-of-sample m ulti-step forecasts\nobtained from a pair of nested models that is based on the fore cast encompassing\nprinciple. Our proposed approach relies on an alternative w ay of testing the population\nmoment restriction implied by the forecast encompassing pr inciple and that links the\nforecast errors from the two competing models in a particula r way. Its key advantage\nis that it is able to bypass the variance degeneracy problem a ﬄicting model based\nforecast comparisons across nested models. It results in a t est statistic whose limiting\ndistribution is standard normal and which is particularly s imple to construct and can\naccommodate both single period and longer-horizon predict ion comparisons. Inferences\nare also shown to be robust to diﬀerent predictor types, incl uding stationary, highly-\npersistent and purely deterministic processes. Finally, w e illustrate the use of our\nproposed approach through an empirical application that ex plores the role of global\ninﬂation in enhancing individual country speciﬁc inﬂation forecasts.\nKeywords: Forecast encompassing, Nested model comparisons, Predict ive accuracy testing,\nInﬂation Forecasts.\n∗I wish to thank the ESRC for its ﬁnancial support via grant ES/ W000989/1. Address for Correspon-\ndence: Jean-Yves Pitarakis, Department of Economics, Univ ersity of Southampton, Southampton SO17 1BJ,\nUnited-Kingdom.\n11. Introduction\nComparing the out-of-sample predictive ability of alterna tive models is an essential com-\nponent of empirical research. Whether one wishes to evaluat e the quality of a model based\nforecast relative to one obtained from a simpler benchmark f or the purpose of validating\na particular theory or to simply select the model that produc es the most accurate forecast\nof some outcome of interest, one needs to look beyond in-samp le speciﬁcation tests by also\nassessing the comparative out-of-sample accuracy of the fo recasts produced by competing\nand often nested models. Our goal in this paper is to propose a novel approach to testing\nwhether multi-step forecasts from one model encompass the f orecasts from a rival larger\nmodel when the two models have a nested structure. Such an enc ompassing based approach\noﬀers an intuitive way of comparing competing forecasts as i t is designed to assess whether\nor not a larger model contains relevant predictive informat ion about an outcome of interest\nnot already captured by a smaller speciﬁcation.\nIn this paper we propose an alternative way of testing a parti cular population moment\nrestriction implied by the forecast encompassing principl e and which links the forecast errors\nobtained from two candidate models in a particular way. The n ovelty lies in its ability to\nbypass the variance degeneracy problem aﬄicting nested mod el based forecast comparisons\nwithout incurring any data loss. Such degeneracy naturally arises in nested model comparison\nsettings as under the null hypothesis of equal predictive ac curacy in the population the two\nmodels are identical and this results in quantities such as M SE spreads and their variance\ncollapsing to zero asymptotically.\nThe objective of comparing out-of-sample forecasts has bee n at the centre of a vast lit-\nerature inspired by the early work in Diebold and Mariano (19 95) and West (1996) who\ndeveloped a theory for testing whether two alternative fore casts have an equal population\nlevel predictive ability under some given loss function (e. g., via mean squared prediction\nerrors). The literature that followed focused on variation s of these tests designed to broaden\ntheir scope and applicability, relax the assumptions under which their asymptotic properties\nhold etc. (see West (2006) and Clark and McCracken (2013) for a comprehensive overview of\nthis agenda). In parallel to this literature an alternative approach aimed at comparing the rel-\n2ative forecasting ability of two competing models relied on the so-called forecast encompass-\ning principle developed in the early work of Hendry and Richa rd (1982), Mizon and Richard\n(1986), Chong and Hendry (1986) amongst others. In its simpl est formulation this principle\nis based on the observation that when one of two forecasts doe s not contain any useful infor-\nmation not already present in the rival forecast, combining the two forecasts via an optimal\nconvex combination will not result in a smaller squared erro r loss. Within a nested and\nmodel based forecast setting this translates into the optim al combined forecast assigning\nzero weight to one of the forecasts. Accordingly, we say that the forecasts based on the\nlarger model are encompassed in the forecasts of the smaller model. The forecast encompass-\ning literature has considered numerous and ofte"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Can the creation of separate bidding zones within\ncountries create imbalances in PV uptake?\nEvidence from Sweden.\nJohanna Fink *\nDecember 27, 2023\nAbstract\nThis paper estimates how electricity price divergence within Sweden has affected\nincentives to invest in photovoltaic (PV) generation between 2016 and 2022 based\non a synthetic control approach. Sweden is chosen as the research subject since it is\ntogether with Italy the only EU country with multiple bidding zones and is facing\ndramatic divergence in electricity prices between low-tariff bidding zones in Northern\nand high-tariff bidding zones in Southern Sweden since 2020. The results indicate\nthat PV uptake in municipalities located north of the bidding zone border is reduced\nby 40.9-48% compared to their Southern counterparts. Based on these results, the\ncreation of separate bidding zones within countries poses a threat to the expansion of\nPV generation and other renewables since it disincentivizes investment in areas with\nlow electricity prices.\nJEL Codes: N44, N54, N74, Q41, Q48\nKeywords: Sweden, Bidding Zone, Solar Energy, Electricity price\n*Department of Economic History, Lund University, Sweden. johanna.fink@ekh.lu.se\n1arXiv:2312.16161v1  [econ.GN]  26 Dec 2023December 27, 2023 Johanna Fink\nContents\n1 Introduction 3\n2 Review of existing literature 7\n2.1 Bidding zones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Photovoltaic generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Determinants of PV uptake . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.3.1 Socio-economic factors . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.3.2 Geographic factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3 The Swedish case 16\n3.1 The Swedish PV market . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.2 Electricity price divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4 Data 22\n4.1 The three border regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n5 Synthetic Control Method 27\n6 Results 30\n7 Discussion 34\n7.1 Implications of the empirical findings . . . . . . . . . . . . . . . . . . . . . . 34\n7.2 Is the use of synthetic controls justified? . . . . . . . . . . . . . . . . . . . . . 36\n7.3 Alternative estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n7.4 The choice of the border region . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n8 Conclusion 41\nBibliography 43\n2December 27, 2023 Johanna Fink\n1 Introduction\nWith the ratification of the European Green Deal in 2021, the European Union (EU) mani-\nfested its goal to become climate neutral by 2050 (ECEEE, 2023). Even prior to its ratification\nthe European Green Deal was central to the first State of the Union speech Ursula von der\nLeyen as the president of the European commission in 2020. In her speech von der Leyen\nhighlighted fossil-free steel production in Northern Sweden as the center of the sustainable\ntransition of energy intensive industries in Europe (EU, 2020). But what makes Northern\nSweden so particularly attractive to energy-intensive green industries?\nNorthern Sweden offers companies access to vast natural resources, such as copper,\niron, and rare earth, as well as collaboration possibilities with high level research facilities\n(Smart City Sweden, 2023). Nonetheless, Johnson (2021) considers the massive surplus of\ncheap renewable electricity generated from hydro and wind power to be the main pull\nfactor. Industrial consumers in Northern Sweden faced the lowest electric tariff rates in\nEurope, amounting on some days to merely 0.13 SEK or 1 cent per kWh, a twelfth of the\nEuropean average (Thunborg, 2021; Strom-Report, 2021). Consequently, Northern Sweden\nis expected to attract massive investments of approximately 1,000 billion SEK or 120 billion\nUSD over the next decade.\nNonetheless, neither this investment boom nor the low electric tariff rate extend to the\nentire country. In fact Southern Sweden has faced electricity shortage over the last years,\nas a consequence of insufficient grid capacity and the shut down of several nuclear power\nplants (Armelius, 2012). While electric tariff rates remained low in Northern Sweden,\nelectricity shortage resulted in soaring electricity prices in the Southern part of the country.\nElectricity price divergence between Northern and Southern Sweden peaked in August\n2022, amounting to a factor of twelve (Vattenfall, 2023).\nThis divergence of electricity prices within a country surely appears strange to most\ninhabitants of Central and Southern Europe where electricity prices are identical through-\n3December 27, 2023 Johanna Fink\nout the country (ENTSO-E, 2023). What makes this divergence possible in Sweden is the\ndivision of the country into separate bidding zones (BZ) in 2011. A BZ is defined as \"the\nlargest geographical area within which market participants are able to exchange energy wit"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Incentive-Aware Synthetic Control:\nAccurate Counterfactual Estimation via Incentivized Exploration\nDaniel Ngo∗1, Keegan Harris∗2, Anish Agarwal3, Vasilis Syrgkanis4, and Zhiwei Steven\nWu2\n1University of Minnesota\n2Carnegie Mellon University\n3Columbia University\n4Stanford University\nngo00054@umn.edu, {keeganh, zstevenwu}@cmu.edu ,\naa5194@columbia.edu, vsyrgk@stanford.edu\nAbstract\nWe consider the classic panel data setting in which one observes measurements of units\nover time, under different interventions . Our focus is on the canonical family of synthetic\ncontrol methods (SCMs) which, after a pre-intervention time period when all units are under\ncontrol, estimate counterfactual outcomes for testunits in the post-intervention time period\nunder control by using data from donorunits who have remained under control for the entire\npost-intervention period. In order for the counterfactual estimate produced by synthetic\ncontrol for a test unit to be accurate, there must be sufficient overlap between the outcomes\nof the donor units and the outcomes of the test unit. As a result, a canonical assumption in\nthe literature on SCMs is that the outcomes for the test units lie within either the convex\nhull or the linear span of the outcomes for the donor units. However despite their ubiquity,\nsuch overlapassumptions may not always hold, as is the case when e.g. units select their\nown interventions and different subpopulations of units prefer different interventions a priori.\nWe shed light on this typically overlooked assumption, and we address this issue by\nincentivizing units with different preferences to take interventions they would not normally\nconsider. Specifically, we provide a SCM for incentivizing exploration in panel data settings\nwhich provides incentive-compatible intervention recommendations to units by leveraging\ntools from information design and online learning. Using our algorithm, we show how to\nobtain valid counterfactual estimates using SCMs without the need for an explicit overlap\nassumption on the unit outcomes.\n∗Denotes equal contribution.arXiv:2312.16307v1  [econ.EM]  26 Dec 2023Contents\n1 Introduction 1\n1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Setting and Background 4\n2.1 Our Panel Data Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Background on PCR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Recommendations and Beliefs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3 Incentivized Exploration for Synthetic Control 8\n3.1 On the Necessity of the Unit Overlap Assumption . . . . . . . . . . . . . . . . . . 10\n4 Extension to Synthetic Interventions 11\n5 Testing Whether the Unit Overlap Assumption Holds 13\n5.1 A Non-Asymptotic Hypothesis Test . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n5.2 An Asymptotic Hypothesis Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n6 Numerical Simulations 15\n7 Conclusion 17\nA Appendix for Section 3: Incentivized Exploration for Synthetic Control 22\nA.1 Causal Parameter Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nA.2 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nB Appendix for Section 4: Extension to Synthetic Interventions 27\nCAppendix for Section 5: Testing Whether the Unit Overlap Assumption Holds 291 Introduction\nA ubiquitous task in statistics, machine learning, and econometrics is to estimate counterfactual\noutcomes for a group of units(e.g. people, geographic regions, subpopulations) under different\ninterventions (e.g. medical treatments, weather patterns, legal regulations) over time. Such\nmulti-dimensional data are often referred to as panel data (orlongitudinal data ), where the\ndifferent units may be thought of as rows of a matrix, and the time-steps as columns. A\nprominent framework for counterfactual inference using panel data is that of synthetic control\n[1,2]. Synthetic control methods (SCMs) assume access to a pre-intervention time period, during\nwhich all units are under control(i.e. no treatment). After the pre-intervention time period,\nevery unit is given exactly one intervention from a set of possible interventions (which can\ninclude the control) and remains under the intervention for the remaining time-steps (i.e. the\npost-intervention time period). In order to estimate unit-specific counterfactuals under control,\nSCMs use the pre-intervention time period to learn a model to predict the outcomes for the test\nunit from the outcomes of the units who remained under control (i.e. the donorunits). Once\nthe model is learned, it is then extrapolated to the post-intervention time period in order to\npredict the counterfactual outcome for the test unit, had they remained under control. Since first\nbeing introduced in the field of economics over two decades ago, SCMs have become a popular\ntool for counterfactual inference and are routinel"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "Randomized Signature Methods in Optimal Portfolio Selection\nErdin¸ c Akyildirima,b, Matteo Gambaraa, Josef Teichmanna, Syang Zhoua\naDepartment of Mathematics, ETH, Zurich, Switzerland\nbDepartment of Banking and Finance, University of Zurich, Zurich, Switzerland\nAbstract\nWe present convincing empirical results on the application of Randomized Signature Methods\nfor non-linear, non-parametric drift estimation for a multi-variate financial market. Even\nthough drift estimation is notoriously ill defined due to small signal to noise ratio, one can still\ntry to learn optimal non-linear maps from data to future returns for the purposes of portfolio\noptimization. Randomized Signatures, in constrast to classical signatures, allow for high\ndimensional market dimension and provide features on the same scale. We do not contribute\nto the theory of Randomized Signatures here, but rather present our empirical findings on\nportfolio selection in real world settings including real market data and transaction costs.\nKeywords: Machine Learning, Randomized Signature, Drift estimation, Returns forecast,\nPortfolio Optimization, Path-dependent Signal\nJEL: C21, C22, G11, G14, G17\n1. Introduction\nOptimal portfolio construction is one of the most fundamental problems in quantitative\nfinance. It refers to selecting and allocating assets to achieve a balance between risk and\nreturn. An optimal portfolio aligns with an investor’s specific objectives, risk tolerance, and\ntime horizon. In that sense, optimal implies achieving the best trade-off between expected\nreturn and risk based on the fact that different investors will have different optimal portfolios\ndepending on their unique goals and risk tolerances. Notice that a priori neither the precise\noptimization problem nor the underlying model for the evolution of the market are known\nto the investor. The former needs a quantification of risk tolerance, time horizon, the latter\nneeds an estimation of model parameters.\nThere are several fundamental methods for constructing an optimal portfolio. Mod-\nern Portfolio Theory (MPT) which is developed by Harry Markowitz in his seminal work\nEmail addresses: erdinc.akyildirim@bf.uzh.ch (Erdin¸ c Akyildirim), matteo.gambara@gmail.com\n(Matteo Gambara), josef.teichmann@math.ethz.ch (Josef Teichmann), syang.zhou@math.ethz.ch\n(Syang Zhou)arXiv:2312.16448v1  [q-fin.PM]  27 Dec 2023(Markovitz, 1959) provides a foundational approach given a model. MPT uses mean-variance\noptimization to construct the portfolio that maximizes the expected return while minimizing\nthe associated risk (expressed in terms of variance or standard deviation), or, equivalently,\nminimizes the risk for a given level of expected return.\nThe Capital Asset Pricing Model (CAPM) (Treynor, 1961a,b; Sharpe, 1964; Lintner,\n1975; Mossin, 1966) is another cornerstone in portfolio management which helps investors\ndetermine the expected return of an investment, particularly for individual stocks or portfo-\nlios of stocks. The Capital Market Line (CML) and Security Market Line (SML) represent\nportfolios derived from MPT. The former shows the relationship between risk and return\nfor a portfolio of all possible investments and the latter illustrates the relationship between\nthe expected return and the systematic risk of an individual asset or portfolio of assets.\nAnother method called the Maximum Sharpe Ratio Portfolio finds the optimal weights by\nmaximizing the portfolio’s Sharpe ratio, where the Sharpe ratio measures excess portfolio\nreturn over the risk-free rate relative to its standard deviation.\nAs an enhancement to the traditional MPT, the Black-Litterman model (Black and\nLitterman, 1992) was developed to incorporate investors’ subjective views and market equi-\nlibrium returns into optimal asset allocation in a portfolio. Similarly, factor-based portfolio\noptimization is used to improve CAPM by systematically considering factors that are believed\nto affect asset returns. Recently, Auh and Cho (2023) show that a parsimonious factor model\nmitigates idiosyncratic noise in historical data for portfolio optimization. They also prove\nthat a combination of the factor model and forward-looking returns improves out-of-sample\nperformance. As opposed to the Black-Litterman model, Risk-Parity distributes portfolio\nrisk equally across assets and does not look at investor views or expected return projections.\nEspecially after the Global Financial Crisis in 2008, Risk-Parity became a widely followed\nstrategy (Roncalli and Weisang, 2016; Costa and Kwon, 2019; Bai et al., 2016).\nAnother recent yet classical approach for optimal portfolio allocation is using Monte Carlo\nsimulations. This involves generating a large number of random scenarios to model the range\nof possible future returns for different assets. Of course this again is based on a given model.\nBy repeatedly simulating portfolio performance under various market conditions, investors\ncan assess the distribution of potential outcomes and make informe"}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "T cell receptor binding prediction: A machine\nlearning revolution\nAnna Weber * Aurélien Pélissier†María Rodríguez Martínez∗‡\nDecember 29, 2023\nAbstract\nRecent advancements in immune sequencing and experimental techniques are generat-\ning extensive T cell receptor (TCR) repertoire data, enabling the development of mod-\nels to predict TCR binding specificity. Despite the computational challenges due to\nthe vast diversity of TCRs and epitopes, significant progress has been made. This pa-\nper discusses the evolution of the computational models developed for this task, with\na focus on machine learning efforts, including the early unsupervised clustering ap-\nproaches, supervised models, and the more recent applications of Protein Language\nModels (PLMs). We critically assess the most prominent models in each category, and\ndiscuss recurrent challenges, such as the lack of generalization to new epitopes, dataset\nbiases, and biases in the validation design of the models.\nFurthermore, our paper discusses the transformative role of transformer-based pro-\ntein models in bioinformatics. These models, pretrained on extensive collections of\nunlabeled protein sequences, can convert amino acid sequences into vectorized em-\nbeddings that capture important biological properties. We discuss recent attempts to\nleverage PLMs to deliver very competitive performances in TCR-related tasks. Finally,\nwe address the pressing need for improved interpretability in these often opaque mod-\nels, proposing strategies to amplify their impact in the field.\nKeyword : Machine Learning; T cell Receptor; Specificity Prediction; Protein Lan-\nguage Models; Interpretability.\n*IBM Research Europe (Switzerland).\n†ZHAW, Life Sciences und Facility Management (Switzerland).\n‡Correspondence to maria.rodriguezmartinez@yale.edu.\n1arXiv:2312.16594v1  [q-bio.QM]  27 Dec 20231 Background\nT cells are an essential component of the adaptive immune system, due to their ability\nto orchestrate targeted, effective immune responses through cell-based and cytokine-\nrelease mechanisms. While T cell functions are diverse, their activation, differentiation,\nproliferation, and function are all governed by their T cell receptors (TCR), which\nenable them to recognize non-self antigens arising from infectious agents or diseased\ncells [1].\nTo face a diverse and ever-evolving array of antigens, the immune system has\nevolved the capability to generate a huge array of distinct TCRs. This diversity is\nachieved through a random process of DNA rearrangement, which involves the recom-\nbination of the germline V , D, and J gene segments and the deletion and insertion of\nnucleotides at the V(D)J junctions. While the theoretical diversity of different TCRs\nis estimated to be as high as 1019[2], the realized diversity in an individual is much\nsmaller, typically ranging between 106and 1010[3].\nAt the molecular level, TCRs interact with peptides presented on the major histo-\ncompatibility complex (MHC), a complex commonly referred to as pMHC. Although\nthe interaction between pMHC and TCR is highly specific, a single TCR can often rec-\nognize multiple pMHC complexes. Indeed, some TCRs have been shown to recognize\nup to a million different epitopes [4]. This multivalency is necessary to ensure that\nthe realized diversity in one individual can recognize a significantly broader array of\npotential antigens.\n2 T cell receptor specificity prediction.\nThe precise prediction of TCR-pMHC binding is essential for accurately quantifying\nand predicting immune responses. If effectively represented, it has the potential to\ntransform the field of personalized medicine. For instance, the accurate identification\nof epitopes recognized by expanded TCR clones can aid in identifying the auto-antigens\ninvolved in T-cell-associated autoimmune diseases [5], assessing responses to vaccines\nor identifying the pathogenic agents responsible for eliciting T-cell responses [6]. In\nthe context of cancer, an improved predictive power of TCR specificity can not only\naid in the design of more effective T cell-based therapies [7], but also minimize toxic\nside-effects produced by TCRs off-target binding [8].\nHowever, as experimental methods cannot encompass the vast space of potential\nTCRs and epitopes, significant emphasis has been placed on the development of reliable\ncomputational methods to predict TCR specificity. Existing methods can accurately\nclassify in-distribution samples, i.e. they can predict TCR binding to epitopes already\nencountered by the model [9]. However, the pivotal challenge is to develop models\nwith the capacity to generalize to novel epitopes. A major challenge stems from the\nscarcity of datasets containing experimentally validated TCR-epitope interactions, and\nin particular, regarding the diversity of sampled epitopes.\n23 Limitations of available datasets.\nTCR specificity data can be collected from various databases, such as the VDJdb [10],\nwith over ∼70,000 TCR sequences and ∼1100 different epitopes "}
{"date": "2023-12-31-11-12", "error": false, "url": "PDF", "text_blocks": "scRNA-seq Data Clustering by Cluster-aware Iterative\nContrastive Learning\nWeikang Jiang1∗, Jinxian Wang1∗, Jihong Guan2, Shuigeng Zhou1,†\n1Shanghai Key Lab of Intelligent Information Processing,\nand School of Computer Science, Fudan University\n2Department of Computer Science and Technology, Tongji University\n{sgzhou, 20210240030}@fudan.edu.cn\nwangjx22@m.fudan.edu.cn, jhguan@tongji.edu.cn\nAbstract\nSingle-cell RNA sequencing (scRNA-seq) enables researchers to analyze gene\nexpression at single-cell level. One important task in scRNA-seq data analysis is\nunsupervised clustering, which helps identify distinct cell types, laying down the\nfoundation for other downstream analysis tasks. In this paper, we propose a novel\nmethod called Cluster-aware Iterative Contrastive Learning (CICL in short) for\nscRNA-seq data clustering, which utilizes an iterative representation learning and\nclustering framework to progressively learn the clustering structure of scRNA-seq\ndata with a cluster-aware contrastive loss. CICL consists of a Transformer encoder,\na clustering head, a projection head and a contrastive loss module. First, CICL\nextracts the feature vectors of the original and augmented data by the Transformer-\nencoder. Then, it computes the clustering centroids by K-means and employs the\nstudent’s t-distribution to assign pseudo-labels to all cells in the clustering head.\nThe projection-head uses a Multi-Layer Perceptron (MLP) to obtain projections\nof the augmented data. At last, both pseudo-labels and projections are used in the\ncontrastive loss to guide the model training. Such a process goes iteratively so that\nthe clustering result becomes better and better. Extensive experiments on 25 real-\nworld scRNA-seq datasets show that CICL outperforms the state-of-the-art (SOTA)\nmethods. Concretely, CICL surpasses the existing methods by from 14% to 280%,\nand from 5% to 133% on average in terms of performance metrics ARI and NMI\nrespectively. Source code is available at https://github.com/Alunethy/CICL.\n1 Introduction\nEach cell possesses unique characteristics and biological functions defined by its gene transcription\nactivities. Conventional bulk RNA sequencing measures the average transcription levels of a multitude\nof cells, thereby obscuring the heterogeneity among individual cells. In the past decade, the rapid\nprogress of single-cell RNA sequencing (scRNA-seq) technologies [Tang et al., 2009] enables\ntranscriptome-wide gene expression measurement in individual cells, which greatly helps deepen\nour understanding of cellular heterogeneity and propels the research on cell biology, immunology,\nand complex diseases [van Galen et al., 2019]. Identifying cell types is a fundamental step in\nunraveling complex biological processes such as cellular differentiation, lineage commitment, and\ngene regulation [Deng et al., 2021]. As such, cell clustering becomes an important task in scRNA-seq\nanalysis. However, the inherent high-dimensionality, noise, and sparsity of scRNA-seq data present\nsevere challenges for scRNA-seq data clustering analysis [Kiselev et al., 2019, Stegle et al., 2015].\n∗Co-first authors who contribute equally to this work.\n†Corresponding author.\nPreprint. Under review.arXiv:2312.16600v1  [q-bio.GN]  27 Dec 2023Up to now, many models or algorithms have been developed for scRNA-seq data clustering. Early\nscRNA-seq clustering methods mainly rely on traditional dimensionality reduction and clustering\nmethods. For example, pcaReduce [Žurauskien ˙e and Yau, 2016] combines PCA and K-means,\niteratively merging cluster pairs based on related probability density function. Recognizing the\nimportance of similarity metrics in the clustering task, SIMLR [Wang et al., 2018a] amalgamates\nmultiple kernels to learn sample similarity and perform spectral clustering. Seurat [Satija et al., 2015]\nemploys a graph-based community detection algorithm, while Louvain [Blondel et al., 2008] is based\non the shared nearest neighbor graph to identify cell types.\nIn the past decade, with the rapid development of deep learning, deep neural networks (DNN) have\nbeen extensively applied to scRNA-seq data clustering to address the limitations of conventional\nmethods [Flores et al., 2022]. DEC [Xie et al., 2016] and IDEC [Guo et al., 2017], based on\nautoencoders (AE), use KL divergence as the clustering loss, achieving simultaneous learning of\nfeature representations and cluster assignments. To address the pervasive dropout events in scRNA-\nseq data, DCA [Eraslan et al., 2019] proposes a zero-inflated negative binomial (ZINB) model to better\ncharacterize the distribution of scRNA-seq data, and uses the negative likelihood as the reconstruction\nloss instead of the frequently-used mean-square error (MSE) loss in autoencoders. scVI [Lopez et al.,\n2018] is a deep generative model based on variational autoencoders, which can do various scRNA-seq\ndata analyses such as data imputation, clustering, and visualization. scDeepCluster [Tian et al., 2019]\nintroduce"}
