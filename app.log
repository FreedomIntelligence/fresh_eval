root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.3
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.05
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.19
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.19
root - INFO - start up
root - INFO - crawler rq_github done, time_used:61.57
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.17
root - INFO - start up
root - INFO - crawler rq_github done, time_used:330.39
root - INFO - crawler wr_wiki done, time_used:184.02
root - INFO - start up
root - ERROR - crawler wr_quora failed, time_used:49.66, error:Timeout 30000ms exceeded.
root - INFO - crawler rq_arxiv done, time_used:195.96
root - INFO - crawler praw_reddit done, time_used:18.04
root - INFO - crawler wr_bbc done, time_used:131.67
root - ERROR - crawler rq_wattpad failed, time_used:125.15, error:HTTPSConnectionPool(host='www.wattpad.com', port=443): Max retries exceeded with url: /api/v3/stories/254083596?drafts=0&mature=1&include_deleted=1&fields=id,title,createDate,modifyDate,description,url,firstPublishedPart,cover,language,user(name,username,avatar,location,numStoriesPublished,numFollowing,numFollowers,twitter),completed,numParts,lastPublishedPart,parts(id,title,length,url,deleted,draft,createDate),tags,storyLanguage,copyright (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))
root - INFO - crawler wr_Yahoo done, time_used:726.7
root - INFO - crawler rq_github done, time_used:306.61
root - INFO - start up
root - ERROR - crawler rq_wattpad failed, time_used:182.54, error:HTTPSConnectionPool(host='www.wattpad.com', port=443): Max retries exceeded with url: /api/v3/stories/319332536?drafts=0&mature=1&include_deleted=1&fields=id,title,createDate,modifyDate,description,url,firstPublishedPart,cover,language,user(name,username,avatar,location,numStoriesPublished,numFollowing,numFollowers,twitter),completed,numParts,lastPublishedPart,parts(id,title,length,url,deleted,draft,createDate),tags,storyLanguage,copyright (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))
root - INFO - start up
root - INFO - crawler rq_wattpad done, time_used:44.83
root - INFO - start up
root - INFO - crawler rq_wattpad done, time_used:44.51
root - INFO - start up
root - ERROR - crawler wr_quora failed, time_used:50.88, error:Timeout 30000ms exceeded.
root - INFO - crawler rq_arxiv done, time_used:140.61
root - INFO - crawler praw_reddit done, time_used:15.53
root - INFO - crawler wr_bbc done, time_used:79.55
root - INFO - crawler wr_Yahoo done, time_used:307.87
root - INFO - crawler rq_github done, time_used:377.34
root - INFO - crawler wr_wiki done, time_used:109.4
root - INFO - start up
root - INFO - crawler wr_bbc done, time_used:67.12
root - ERROR - crawler wr_quora failed, time_used:48.88, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_quora failed, time_used:44.79, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_quora failed, time_used:48.13, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_quora failed, time_used:256.76, error:Timeout 30000ms exceeded.
root - INFO - start up
root - INFO - crawler wr_quora done, time_used:193.46
root - INFO - start up
root - INFO - crawler wr_quora done, time_used:394.33
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:154.52
root - INFO - crawler praw_reddit done, time_used:12.77
root - INFO - crawler wr_bbc done, time_used:58.99
root - INFO - crawler wr_Yahoo done, time_used:251.13
root - INFO - start up
root - INFO - crawler wr_wiki done, time_used:94.9
root - INFO - crawler rq_wattpad done, time_used:39.27
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:573.16
root - ERROR - crawler rq_arxiv failed, time_used:613.1, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - ERROR - crawler rq_arxiv failed, time_used:653.08, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:57.55, error:[Errno 2] No such file or directory: './data/2024-01-30_2022/rq_arxiv_computer_science.jsonl'
root - ERROR - crawler rq_arxiv failed, time_used:122.87, error:[Errno 2] No such file or directory: './data/2024-01-30_2023/rq_arxiv_computer_science.jsonl'
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:44.94, error:[Errno 2] No such file or directory: './data/2024-01-30_2023/rq_arxiv_computer_science.jsonl'
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:39.87, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:109.44, error:[Errno 2] No such file or directory: './data/2024-01-30_2023/rq_arxiv_computer_science.jsonl'
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:'save_path'
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:'save_path'
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:92.35, error:("Connection broken: InvalidChunkLength(got length b'', 0 bytes read)", InvalidChunkLength(got length b'', 0 bytes read))
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:279.91, error:('Connection broken: IncompleteRead(1378 bytes read, 8862 more expected)', IncompleteRead(1378 bytes read, 8862 more expected))
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:305.38, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:241.11
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:51.65, error:
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:176.21, error:
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:42.75, error:
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:280.38
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:159.84
root - INFO - crawler rq_arxiv done, time_used:313.22
PyPDF2._reader - WARNING - XRef object at 1608692 can not be read, some object may be missing
root - INFO - crawler rq_arxiv done, time_used:495.37
root - ERROR - crawler rq_arxiv failed, time_used:699.04, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - ERROR - crawler rq_arxiv failed, time_used:738.97, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - ERROR - crawler rq_arxiv failed, time_used:778.99, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - INFO - start up
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x7a for key /Subtype
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x1af for key /Subtype
root - ERROR - crawler rq_arxiv failed, time_used:324.91, error:('Connection aborted.', TimeoutError(10060, 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½', None, 10060, None))
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:48.47, error:list index out of range
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x7a for key /Subtype
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x1af for key /Subtype
root - ERROR - crawler rq_arxiv failed, time_used:132.79, error:list index out of range
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:370.88
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:151.25
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - INFO - start up
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - ERROR - crawler rq_arxiv failed, time_used:0.0, error:can only concatenate str (not "int") to str
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:265.63
root - INFO - crawler rq_arxiv done, time_used:571.81
root - INFO - crawler rq_arxiv done, time_used:802.12
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x3f307 for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x3f3d9 for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x3f4ab for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x40555 for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x40675 for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x3f307 for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x3f3d9 for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x3f4ab for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x40555 for key /Rotate
PyPDF2.generic._data_structures - WARNING - Multiple definitions in dictionary at byte 0x40675 for key /Rotate
root - INFO - crawler rq_arxiv done, time_used:977.06
root - INFO - crawler rq_arxiv done, time_used:1199.41
root - INFO - crawler rq_arxiv done, time_used:1492.51
root - INFO - crawler rq_arxiv done, time_used:1736.66
root - INFO - crawler rq_arxiv done, time_used:1977.94
root - INFO - crawler rq_arxiv done, time_used:2268.94
root - INFO - crawler rq_arxiv done, time_used:2552.69
root - INFO - crawler rq_arxiv done, time_used:2866.21
root - INFO - start up
root - INFO - crawler rq_arxiv done, time_used:357.99
root - INFO - start up
root - ERROR - crawler wr_quora failed, time_used:214.25, error:Timeout 30000ms exceeded.
root - INFO - crawler rq_arxiv done, time_used:422.79
root - INFO - crawler praw_reddit done, time_used:3.72
root - ERROR - crawler wr_bbc failed, time_used:31.32, error:Timeout 30000ms exceeded.
root - ERROR - crawler wr_Yahoo failed, time_used:30.69, error:Timeout 30000ms exceeded.
root - INFO - crawler rq_github done, time_used:291.16
root - INFO - crawler wr_wiki done, time_used:257.51
root - INFO - crawler rq_wattpad done, time_used:114.84
root - INFO - start up
root - INFO - crawler wr_quora done, time_used:124.2
root - ERROR - crawler wr_bbc failed, time_used:30.73, error:Timeout 30000ms exceeded.
root - ERROR - crawler wr_Yahoo failed, time_used:30.69, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_bbc failed, time_used:62.17, error:Timeout 30000ms exceeded.
root - ERROR - crawler wr_Yahoo failed, time_used:30.7, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:30.7, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:30.71, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:30.7, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:30.99, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:30.97, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:31.07, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:30.99, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:34.33, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:31.22, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:31.28, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:31.07, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:31.11, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:61.31, error:Timeout 60000ms exceeded.
root - INFO - start up
root - INFO - crawler wr_Yahoo done, time_used:257.5

<<<<<<< HEAD
root - INFO - start up
root - ERROR - crawler wr_quora failed, time_used:98.27, error:Timeout 30000ms exceeded.
root - INFO - crawler rq_arxiv done, time_used:514.94
root - INFO -   crawler praw_reddit done, time_used:4.14
root - INFO - crawler wr_bbc done, time_used:110.93
root - INFO - crawler wr_Yahoo done, time_used:160.26
root - INFO - crawler rq_github done, time_used:1863.25
root - INFO - crawler wr_wiki done, time_used:95.3
root - INFO - crawler rq_wattpad done, time_used:76.68
=======
    root - ERROR - crawler rq_wattpad failed, time_used:0.05, error:check_hostname requires server_hostname,trackback:Traceback (most recent call last):
    File ".\cmd_crawl.py", line 44, in <module>
        crawler(config)
    File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 138, in rq_wattpad
        story_urls=get_new_story_urls()
    File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 120, in get_new_story_urls
        response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})
    File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
        return request('get', url, params=params, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
        return session.request(method=method, url=url, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
        resp = self.send(prep, **send_kwargs)
    File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
        r = adapter.send(request, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
        resp = conn.urlopen(
    File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 696, in urlopen
        self._prepare_proxy(conn)
    File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 964, in _prepare_proxy
        conn.connect()
    File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 359, in connect
        conn = self._connect_tls_proxy(hostname, conn)
    File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 500, in _connect_tls_proxy
        return ssl_wrap_socket(
    File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 432, in ssl_wrap_socket
        ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
    File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 474, in _ssl_wrap_socket_impl
        return ssl_context.wrap_socket(sock)
    File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
        return self.sslsocket_class._create(
    File "F:\anaconda\lib\ssl.py", line 997, in _create
        raise ValueError("check_hostname requires server_hostname")
    ValueError: check_hostname requires server_hostname

2024-02-14 23:21:20,462 - root - INFO - start up
2024-02-14 23:21:20,637 - root - ERROR - crawler praw_reddit failed, time_used:0.17, error:check_hostname requires server_hostname,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\reddit\praw.py", line 15, in praw_reddit
    reddit = praw.Reddit(
  File "F:\anaconda\lib\site-packages\praw\util\deprecate_args.py", line 43, in wrapped
    return func(**dict(zip(_old_args, args)), **kwargs)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 268, in __init__
    self._check_for_update()
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 470, in _check_for_update
    update_check(__package__, __version__)
  File "F:\anaconda\lib\site-packages\update_checker.py", line 184, in update_check
    result = checker.check(package_name, package_version)
  File "F:\anaconda\lib\site-packages\update_checker.py", line 62, in wrapped
    retval = function(obj, package_name, package_version, **extra_data)
  File "F:\anaconda\lib\site-packages\update_checker.py", line 139, in check
    data = query_pypi(
  File "F:\anaconda\lib\site-packages\update_checker.py", line 74, in query_pypi
    response = requests.get(f"https://pypi.org/pypi/{package}/json", timeout=1)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 359, in connect
    conn = self._connect_tls_proxy(hostname, conn)
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 500, in _connect_tls_proxy
    return ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 432, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 474, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 997, in _create
    raise ValueError("check_hostname requires server_hostname")
ValueError: check_hostname requires server_hostname

2024-02-14 23:21:20,658 - root - ERROR - crawler rq_wattpad failed, time_used:0.02, error:check_hostname requires server_hostname,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 138, in rq_wattpad
    story_urls=get_new_story_urls()
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 120, in get_new_story_urls
    response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 359, in connect
    conn = self._connect_tls_proxy(hostname, conn)
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 500, in _connect_tls_proxy
    return ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 432, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 474, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 997, in _create
    raise ValueError("check_hostname requires server_hostname")
ValueError: check_hostname requires server_hostname

2024-02-14 23:22:52,566 - root - INFO - start up
2024-02-14 23:22:54,289 - prawcore - WARNING - Retrying due to ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None))) status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:22:56,172 - prawcore - WARNING - Retrying due to ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None))) status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:22:59,569 - root - ERROR - crawler praw_reddit failed, time_used:6.88, error:error with request ('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None)),trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\reddit\praw.py", line 28, in praw_reddit
    for post in posts:
  File "F:\anaconda\lib\site-packages\praw\models\listing\generator.py", line 63, in __next__
    self._next_batch()
  File "F:\anaconda\lib\site-packages\praw\models\listing\generator.py", line 89, in _next_batch
    self._listing = self._reddit.get(self.url, params=self.params)
  File "F:\anaconda\lib\site-packages\praw\util\deprecate_args.py", line 43, in wrapped
    return func(**dict(zip(_old_args, args)), **kwargs)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 712, in get
    return self._objectify_request(method="GET", params=params, path=path)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 517, in _objectify_request
    self.request(
  File "F:\anaconda\lib\site-packages\praw\util\deprecate_args.py", line 43, in wrapped
    return func(**dict(zip(_old_args, args)), **kwargs)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 941, in request
    return self._core.request(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 328, in request
    return self._request_with_retries(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 254, in _request_with_retries
    return self._do_retry(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 162, in _do_retry
    return self._request_with_retries(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 254, in _request_with_retries
    return self._do_retry(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 162, in _do_retry
    return self._request_with_retries(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 234, in _request_with_retries
    response, saved_exception = self._make_request(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 186, in _make_request
    response = self._rate_limiter.call(
  File "F:\anaconda\lib\site-packages\prawcore\rate_limit.py", line 46, in call
    kwargs["headers"] = set_header_callback()
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 282, in _set_header_callback
    self._authorizer.refresh()
  File "F:\anaconda\lib\site-packages\prawcore\auth.py", line 378, in refresh
    self._request_token(grant_type="client_credentials", **additional_kwargs)
  File "F:\anaconda\lib\site-packages\prawcore\auth.py", line 155, in _request_token
    response = self._authenticator._post(url=url, **data)
  File "F:\anaconda\lib\site-packages\prawcore\auth.py", line 51, in _post
    response = self._requestor.request(
  File "F:\anaconda\lib\site-packages\prawcore\requestor.py", line 70, in request
    raise RequestException(exc, args, kwargs) from None
prawcore.exceptions.RequestException: error with request ('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None))

2024-02-14 23:23:53,687 - root - INFO - start up
2024-02-14 23:24:05,028 - prawcore - WARNING - Retrying due to 500 status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:24:07,689 - prawcore - WARNING - Retrying due to 500 status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:24:24,139 - root - INFO - crawler praw_reddit done, time_used:30.45
2024-02-14 23:24:30,293 - root - ERROR - crawler rq_wattpad failed, time_used:6.15, error:request() got an unexpected keyword argument 'server_hostname',trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 142, in rq_wattpad
    entry=main(urls)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 109, in main
    entry=save_html_file(html_file_name, story_name, author, cover, tags, summary, chapters)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 58, in save_html_file
    chapter_content = download_webpage(chapter_url)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 32, in download_webpage
    res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, server_hostname=url)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
TypeError: request() got an unexpected keyword argument 'server_hostname'

2024-02-14 23:25:33,028 - root - INFO - start up
2024-02-14 23:25:54,489 - root - INFO - crawler praw_reddit done, time_used:21.46
2024-02-14 23:27:45,067 - root - INFO - crawler rq_wattpad done, time_used:110.58
2024-02-28 15:01:06,045 - root - INFO - start up
2024-02-28 15:03:16,882 - root - INFO - crawler rq_github done, time_used:130.84
2024-02-28 15:06:02,359 - root - INFO - crawler wr_wiki done, time_used:165.48
2024-02-28 15:07:00,676 - root - ERROR - crawler wr_quora failed, time_used:58.32, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 45, in <module>
    try:
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 113, in wr_quora
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 65, in run
    element=page.get_by_role("link", name=random.choice(choose_list)).click()
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 15864, in click
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_locator.py", line 158, in click
    return await self._frame.click(self._selector, strict=True, **params)
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 494, in click
    await self._channel.send("click", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:17:45,683 - root - INFO - crawler rq_arxiv done, time_used:645.01
2024-02-28 15:18:01,186 - root - INFO - crawler praw_reddit done, time_used:15.5
2024-02-28 15:19:05,985 - root - ERROR - crawler wr_bbc failed, time_used:64.72, error:Timeout 60000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 45, in <module>
    try:
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 92, in wr_bbc
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 38, in run
    page.goto("https://www.bbc.com/news",timeout=60000)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 60000ms exceeded.

2024-02-28 15:19:51,251 - root - INFO - crawler rq_wattpad done, time_used:45.27
2024-02-28 15:21:34,144 - root - ERROR - crawler wr_Yahoo failed, time_used:102.89, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 45, in <module>
    try:
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 115, in wr_Yahoo
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 71, in run
    page.goto(full_url)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:28:38,234 - root - INFO - start up
2024-02-28 15:28:53,449 - root - INFO - crawler wr_Yahoo done, time_used:15.21
2024-02-28 15:29:12,054 - root - INFO - start up
2024-02-28 15:29:54,517 - root - INFO - crawler wr_Yahoo done, time_used:42.46
2024-02-28 15:30:57,014 - root - ERROR - crawler wr_bbc failed, time_used:62.5, error:Timeout 60000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 92, in wr_bbc
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 38, in run
    page.goto("https://www.bbc.com/news",timeout=60000)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 60000ms exceeded.

2024-02-28 15:31:48,556 - root - ERROR - crawler wr_quora failed, time_used:51.54, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 113, in wr_quora
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 65, in run
    element=page.get_by_role("link", name=random.choice(choose_list)).click()
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 15864, in click
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_locator.py", line 158, in click
    return await self._frame.click(self._selector, strict=True, **params)
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 494, in click
    await self._channel.send("click", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:32:34,509 - root - INFO - start up
2024-02-28 15:33:24,487 - root - ERROR - crawler wr_bbc failed, time_used:49.98, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 92, in wr_bbc
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 53, in run
    page.goto(full_url)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:36:35,475 - root - ERROR - crawler wr_quora failed, time_used:190.99, error:Target page, context or browser has been closed,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 113, in wr_quora
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 82, in run
    box.click()
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 2110, in click
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_element_handle.py", line 137, in click
    await self._channel.send("click", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TargetClosedError: Target page, context or browser has been closed

2024-02-28 15:39:38,987 - root - INFO - start up
2024-02-28 15:42:40,881 - root - INFO - crawler wr_bbc done, time_used:181.89
2024-02-28 15:49:46,437 - root - INFO - crawler wr_quora done, time_used:425.56
2024-02-28 15:50:22,217 - root - INFO - crawler wr_Yahoo done, time_used:35.78
2024-02-28 16:43:02,928 - root - INFO - start up
2024-02-28 16:58:35,392 - root - INFO - start up
2024-02-28 17:12:00,260 - root - ERROR - crawler wr_Yahoo failed, time_used:804.87, error:Cannot run the event loop while another loop is running,trackback:Traceback (most recent call last):
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 124, in wr_Yahoo
    #     run(playwright)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 70, in run
    '''//*[@id="nimbus-app"]/section/section/section/article/section[6]/div/div[1]/div/div/ul/li[3]/section/div/a'''
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 70, in run
    '''//*[@id="nimbus-app"]/section/section/section/article/section[6]/div/div[1]/div/div/ul/li[3]/section/div/a'''
  File "F:\anaconda\lib\bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "F:\anaconda\lib\bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ".\cmd_crawl.py", line 49, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 124, in wr_Yahoo
    #     run(playwright)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_context_manager.py", line 90, in __exit__
    self._connection.stop_sync()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 283, in stop_sync
    self._loop.run_until_complete(self._transport.wait_until_stopped())
  File "F:\anaconda\lib\asyncio\base_events.py", line 592, in run_until_complete
    self._check_running()
  File "F:\anaconda\lib\asyncio\base_events.py", line 554, in _check_running
    raise RuntimeError(
RuntimeError: Cannot run the event loop while another loop is running

2024-02-28 17:12:09,123 - root - INFO - start up
2024-02-28 17:18:19,079 - root - ERROR - crawler wr_Yahoo failed, time_used:369.95, error:,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 49, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 131, in wr_Yahoo
    config={}
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 57, in run
    # news_links = page.query_selector_all("a:has(> u.StretchedBox)")
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 57, in run
    # news_links = page.query_selector_all("a:has(> u.StretchedBox)")
  File "F:\anaconda\lib\bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "F:\anaconda\lib\bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

2024-02-28 17:18:27,245 - root - INFO - start up
2024-02-28 17:19:30,138 - root - INFO - crawler wr_Yahoo done, time_used:62.89
2024-02-28 17:19:36,051 - root - INFO - start up
2024-02-28 17:28:19,740 - root - INFO - crawler wr_Yahoo done, time_used:523.69
2024-02-28 17:30:12,804 - root - INFO - start up
2024-02-28 17:42:58,575 - root - ERROR - crawler rq_arxiv failed, time_used:765.54, error:('Connection aborted.', TimeoutError(10060, 'ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£', None, 10060, None)),trackback:Traceback (most recent call last):
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 670, in urlopen
    httplib_response = self._make_request(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 381, in _make_request
    self._validate_conn(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 978, in _validate_conn
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 362, in connect
    self.sock = ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 386, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 1040, in _create
    self.do_handshake()
  File "F:\anaconda\lib\ssl.py", line 1309, in do_handshake
    self._sslobj.do_handshake()
TimeoutError: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 726, in urlopen
    retries = retries.increment(
  File "F:\anaconda\lib\site-packages\urllib3\util\retry.py", line 410, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "F:\anaconda\lib\site-packages\urllib3\packages\six.py", line 734, in reraise
    raise value.with_traceback(tb)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 670, in urlopen
    httplib_response = self._make_request(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 381, in _make_request
    self._validate_conn(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 978, in _validate_conn
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 362, in connect
    self.sock = ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 386, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 1040, in _create
    self.do_handshake()
  File "F:\anaconda\lib\ssl.py", line 1309, in do_handshake
    self._sslobj.do_handshake()
urllib3.exceptions.ProtocolError: ('Connection aborted.', TimeoutError(10060, 'ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£', None, 10060, None))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ".\cmd_crawl.py", line 50, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\arxiv\rq.py", line 162, in rq_arxiv
    response = requests.get(url)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', TimeoutError(10060, 'ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£', None, 10060, None))

2024-02-28 18:16:16,113 - root - INFO - start up
2024-02-28 18:32:10,440 - root - INFO - crawler rq_arxiv done, time_used:954.33
>>>>>>> 75f624ed0fbc4feebfdd7533f0e1c53c4dd56a04
