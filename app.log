root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.3
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.05
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.19
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.19
root - INFO - start up
root - INFO - crawler rq_github done, time_used:61.57
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.17
root - INFO - start up
root - INFO - crawler rq_github done, time_used:17.26
root - INFO - start up
root - INFO - crawler rq_github done, time_used:8.54
root - ERROR - crawler wr_wiki failed, time_used:1.37, error:net::ERR_CERT_COMMON_NAME_INVALID at https://en.wikipedia.org/wiki/Special:NewPagesFeed
root - ERROR - crawler wr_quora failed, time_used:0.84, error:net::ERR_ADDRESS_UNREACHABLE at https://www.quora.com/
root - ERROR - crawler rq_arxiv failed, time_used:1033.64, error:HTTPSConnectionPool(host='arxiv.org', port=443): Read timed out. (read timeout=None)
root - ERROR - crawler praw_reddit failed, time_used:0.0, error:No module named 'praw'
root - ERROR - crawler wr_bbc failed, time_used:31.05, error:Timeout 30000ms exceeded.
root - ERROR - crawler rq_wattpad failed, time_used:0.33, error:('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
root - INFO - crawler wr_Yahoo done, time_used:8.14
root - INFO - start up
root - INFO - crawler rq_github done, time_used:18.37
root - ERROR - crawler wr_wiki failed, time_used:52.18, error:net::ERR_CONNECTION_TIMED_OUT at https://en.wikipedia.org/wiki/Special:NewPagesFeed
root - ERROR - crawler wr_quora failed, time_used:35.59, error:net::ERR_CONNECTION_TIMED_OUT at https://www.quora.com/
root - INFO - crawler rq_arxiv done, time_used:151.46
root - ERROR - crawler praw_reddit failed, time_used:0.0, error:No module named 'praw'
root - ERROR - crawler wr_bbc failed, time_used:25.29, error:net::ERR_CONNECTION_TIMED_OUT at https://www.bbc.com/news
root - ERROR - crawler rq_wattpad failed, time_used:42.14, error:HTTPSConnectionPool(host='www.wattpad.com', port=443): Max retries exceeded with url: /stories/new/new (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000225FDA39610>: Failed to establish a new connection: [WinError 10060] ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ó·ï¿½ï¿½ï¿½Ò»ï¿½ï¿½Ê±ï¿½ï¿½ï¿½Ã»ï¿½ï¿½ï¿½ï¿½È·ï¿½ð¸´»ï¿½ï¿½ï¿½ï¿½Óµï¿½ï¿½ï¿½ï¿½ï¿½Ã»ï¿½Ð·ï¿½Ó¦ï¿½ï¿½ï¿½ï¿½ï¿½Ó³ï¿½ï¿½ï¿½Ê§ï¿½Ü¡ï¿½'))
root - INFO - crawler wr_Yahoo done, time_used:8.95
root - INFO - start up
root - ERROR - crawler rq_github failed, time_used:0.02, error:check_hostname requires server_hostname
root - INFO - crawler wr_wiki done, time_used:139.99
root - INFO - crawler wr_quora done, time_used:408.16
root - ERROR - crawler praw_reddit failed, time_used:3.47, error:check_hostname requires server_hostname
root - ERROR - crawler wr_bbc failed, time_used:33.22, error:Timeout 30000ms exceeded.
root - ERROR - crawler rq_wattpad failed, time_used:0.01, error:check_hostname requires server_hostname
root - ERROR - crawler wr_Yahoo failed, time_used:56.03, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler praw_reddit failed, time_used:0.24, error:check_hostname requires server_hostname
root - ERROR - crawler wr_bbc failed, time_used:8.88, error:Timeout 6000ms exceeded.
root - ERROR - crawler rq_wattpad failed, time_used:0.02, error:check_hostname requires server_hostname
root - ERROR - crawler wr_Yahoo failed, time_used:11.4, error:Timeout 6000ms exceeded.
root - INFO - start up
root - ERROR - crawler praw_reddit failed, time_used:0.18, error:check_hostname requires server_hostname
root - ERROR - crawler wr_bbc failed, time_used:241.33, error:Timeout 30000ms exceeded.
root - ERROR - crawler rq_wattpad failed, time_used:0.02, error:check_hostname requires server_hostname
root - ERROR - crawler wr_Yahoo failed, time_used:33.3, error:net::ERR_TIMED_OUT at https://finance.yahoo.com/
root - INFO - start up
root - ERROR - crawler wr_Yahoo failed, time_used:344.5, error:Timeout 30000ms exceeded.
root - INFO - start up
root - ERROR - crawler praw_reddit failed, time_used:0.18, error:check_hostname requires server_hostname,trackback:None
asyncio - ERROR - Future exception was never retrieved
future: <Future finished exception=TargetClosedError('Target page, context or browser has been closed')>
playwright._impl._errors.TargetClosedError: Target page, context or browser has been closed
root - INFO - start up
root - ERROR - crawler praw_reddit failed, time_used:0.19, error:check_hostname requires server_hostname,trackback:None
root - ERROR - crawler rq_wattpad failed, time_used:0.03, error:check_hostname requires server_hostname,trackback:None
    root - INFO - start up
    root - ERROR - crawler praw_reddit failed, time_used:0.22, error:check_hostname requires server_hostname,trackback:Traceback (most recent call last):
    File ".\cmd_crawl.py", line 44, in <module>
        crawler(config)
    File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\reddit\praw.py", line 15, in praw_reddit
        reddit = praw.Reddit(
    File "F:\anaconda\lib\site-packages\praw\util\deprecate_args.py", line 43, in wrapped
        return func(**dict(zip(_old_args, args)), **kwargs)
    File "F:\anaconda\lib\site-packages\praw\reddit.py", line 268, in __init__
        self._check_for_update()
    File "F:\anaconda\lib\site-packages\praw\reddit.py", line 470, in _check_for_update
        update_check(__package__, __version__)
    File "F:\anaconda\lib\site-packages\update_checker.py", line 184, in update_check
        result = checker.check(package_name, package_version)
    File "F:\anaconda\lib\site-packages\update_checker.py", line 62, in wrapped
        retval = function(obj, package_name, package_version, **extra_data)
    File "F:\anaconda\lib\site-packages\update_checker.py", line 139, in check
        data = query_pypi(
    File "F:\anaconda\lib\site-packages\update_checker.py", line 74, in query_pypi
        response = requests.get(f"https://pypi.org/pypi/{package}/json", timeout=1)
    File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
        return request('get', url, params=params, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
        return session.request(method=method, url=url, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
        resp = self.send(prep, **send_kwargs)
    File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
        r = adapter.send(request, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
        resp = conn.urlopen(
    File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 696, in urlopen
        self._prepare_proxy(conn)
    File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 964, in _prepare_proxy
        conn.connect()
    File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 359, in connect
        conn = self._connect_tls_proxy(hostname, conn)
    File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 500, in _connect_tls_proxy
        return ssl_wrap_socket(
    File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 432, in ssl_wrap_socket
        ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
    File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 474, in _ssl_wrap_socket_impl
        return ssl_context.wrap_socket(sock)
    File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
        return self.sslsocket_class._create(
    File "F:\anaconda\lib\ssl.py", line 997, in _create
        raise ValueError("check_hostname requires server_hostname")
    ValueError: check_hostname requires server_hostname

    root - ERROR - crawler rq_wattpad failed, time_used:0.05, error:check_hostname requires server_hostname,trackback:Traceback (most recent call last):
    File ".\cmd_crawl.py", line 44, in <module>
        crawler(config)
    File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 138, in rq_wattpad
        story_urls=get_new_story_urls()
    File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 120, in get_new_story_urls
        response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})
    File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
        return request('get', url, params=params, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
        return session.request(method=method, url=url, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
        resp = self.send(prep, **send_kwargs)
    File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
        r = adapter.send(request, **kwargs)
    File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
        resp = conn.urlopen(
    File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 696, in urlopen
        self._prepare_proxy(conn)
    File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 964, in _prepare_proxy
        conn.connect()
    File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 359, in connect
        conn = self._connect_tls_proxy(hostname, conn)
    File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 500, in _connect_tls_proxy
        return ssl_wrap_socket(
    File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 432, in ssl_wrap_socket
        ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
    File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 474, in _ssl_wrap_socket_impl
        return ssl_context.wrap_socket(sock)
    File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
        return self.sslsocket_class._create(
    File "F:\anaconda\lib\ssl.py", line 997, in _create
        raise ValueError("check_hostname requires server_hostname")
    ValueError: check_hostname requires server_hostname

2024-02-14 23:21:20,462 - root - INFO - start up
2024-02-14 23:21:20,637 - root - ERROR - crawler praw_reddit failed, time_used:0.17, error:check_hostname requires server_hostname,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\reddit\praw.py", line 15, in praw_reddit
    reddit = praw.Reddit(
  File "F:\anaconda\lib\site-packages\praw\util\deprecate_args.py", line 43, in wrapped
    return func(**dict(zip(_old_args, args)), **kwargs)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 268, in __init__
    self._check_for_update()
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 470, in _check_for_update
    update_check(__package__, __version__)
  File "F:\anaconda\lib\site-packages\update_checker.py", line 184, in update_check
    result = checker.check(package_name, package_version)
  File "F:\anaconda\lib\site-packages\update_checker.py", line 62, in wrapped
    retval = function(obj, package_name, package_version, **extra_data)
  File "F:\anaconda\lib\site-packages\update_checker.py", line 139, in check
    data = query_pypi(
  File "F:\anaconda\lib\site-packages\update_checker.py", line 74, in query_pypi
    response = requests.get(f"https://pypi.org/pypi/{package}/json", timeout=1)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 359, in connect
    conn = self._connect_tls_proxy(hostname, conn)
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 500, in _connect_tls_proxy
    return ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 432, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 474, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 997, in _create
    raise ValueError("check_hostname requires server_hostname")
ValueError: check_hostname requires server_hostname

2024-02-14 23:21:20,658 - root - ERROR - crawler rq_wattpad failed, time_used:0.02, error:check_hostname requires server_hostname,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 138, in rq_wattpad
    story_urls=get_new_story_urls()
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 120, in get_new_story_urls
    response = requests.get(url,headers={'User-Agent': 'Mozilla/5.0'})
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 359, in connect
    conn = self._connect_tls_proxy(hostname, conn)
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 500, in _connect_tls_proxy
    return ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 432, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 474, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 997, in _create
    raise ValueError("check_hostname requires server_hostname")
ValueError: check_hostname requires server_hostname

2024-02-14 23:22:52,566 - root - INFO - start up
2024-02-14 23:22:54,289 - prawcore - WARNING - Retrying due to ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None))) status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:22:56,172 - prawcore - WARNING - Retrying due to ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None))) status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:22:59,569 - root - ERROR - crawler praw_reddit failed, time_used:6.88, error:error with request ('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None)),trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\reddit\praw.py", line 28, in praw_reddit
    for post in posts:
  File "F:\anaconda\lib\site-packages\praw\models\listing\generator.py", line 63, in __next__
    self._next_batch()
  File "F:\anaconda\lib\site-packages\praw\models\listing\generator.py", line 89, in _next_batch
    self._listing = self._reddit.get(self.url, params=self.params)
  File "F:\anaconda\lib\site-packages\praw\util\deprecate_args.py", line 43, in wrapped
    return func(**dict(zip(_old_args, args)), **kwargs)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 712, in get
    return self._objectify_request(method="GET", params=params, path=path)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 517, in _objectify_request
    self.request(
  File "F:\anaconda\lib\site-packages\praw\util\deprecate_args.py", line 43, in wrapped
    return func(**dict(zip(_old_args, args)), **kwargs)
  File "F:\anaconda\lib\site-packages\praw\reddit.py", line 941, in request
    return self._core.request(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 328, in request
    return self._request_with_retries(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 254, in _request_with_retries
    return self._do_retry(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 162, in _do_retry
    return self._request_with_retries(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 254, in _request_with_retries
    return self._do_retry(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 162, in _do_retry
    return self._request_with_retries(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 234, in _request_with_retries
    response, saved_exception = self._make_request(
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 186, in _make_request
    response = self._rate_limiter.call(
  File "F:\anaconda\lib\site-packages\prawcore\rate_limit.py", line 46, in call
    kwargs["headers"] = set_header_callback()
  File "F:\anaconda\lib\site-packages\prawcore\sessions.py", line 282, in _set_header_callback
    self._authorizer.refresh()
  File "F:\anaconda\lib\site-packages\prawcore\auth.py", line 378, in refresh
    self._request_token(grant_type="client_credentials", **additional_kwargs)
  File "F:\anaconda\lib\site-packages\prawcore\auth.py", line 155, in _request_token
    response = self._authenticator._post(url=url, **data)
  File "F:\anaconda\lib\site-packages\prawcore\auth.py", line 51, in _post
    response = self._requestor.request(
  File "F:\anaconda\lib\site-packages\prawcore\requestor.py", line 70, in request
    raise RequestException(exc, args, kwargs) from None
prawcore.exceptions.RequestException: error with request ('Connection aborted.', ConnectionResetError(10054, 'Ô¶³ÌÖ÷»úÇ¿ÆÈ¹Ø±ÕÁËÒ»¸öÏÖÓÐµÄÁ¬½Ó¡£', None, 10054, None))

2024-02-14 23:23:53,687 - root - INFO - start up
2024-02-14 23:24:05,028 - prawcore - WARNING - Retrying due to 500 status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:24:07,689 - prawcore - WARNING - Retrying due to 500 status: GET https://oauth.reddit.com/r/all/new
2024-02-14 23:24:24,139 - root - INFO - crawler praw_reddit done, time_used:30.45
2024-02-14 23:24:30,293 - root - ERROR - crawler rq_wattpad failed, time_used:6.15, error:request() got an unexpected keyword argument 'server_hostname',trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 44, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 142, in rq_wattpad
    entry=main(urls)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 109, in main
    entry=save_html_file(html_file_name, story_name, author, cover, tags, summary, chapters)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 58, in save_html_file
    chapter_content = download_webpage(chapter_url)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\wattpad\rq.py", line 32, in download_webpage
    res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, server_hostname=url)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
TypeError: request() got an unexpected keyword argument 'server_hostname'

2024-02-14 23:25:33,028 - root - INFO - start up
2024-02-14 23:25:54,489 - root - INFO - crawler praw_reddit done, time_used:21.46
2024-02-14 23:27:45,067 - root - INFO - crawler rq_wattpad done, time_used:110.58
2024-02-28 15:01:06,045 - root - INFO - start up
2024-02-28 15:03:16,882 - root - INFO - crawler rq_github done, time_used:130.84
2024-02-28 15:06:02,359 - root - INFO - crawler wr_wiki done, time_used:165.48
2024-02-28 15:07:00,676 - root - ERROR - crawler wr_quora failed, time_used:58.32, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 45, in <module>
    try:
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 113, in wr_quora
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 65, in run
    element=page.get_by_role("link", name=random.choice(choose_list)).click()
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 15864, in click
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_locator.py", line 158, in click
    return await self._frame.click(self._selector, strict=True, **params)
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 494, in click
    await self._channel.send("click", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:17:45,683 - root - INFO - crawler rq_arxiv done, time_used:645.01
2024-02-28 15:18:01,186 - root - INFO - crawler praw_reddit done, time_used:15.5
2024-02-28 15:19:05,985 - root - ERROR - crawler wr_bbc failed, time_used:64.72, error:Timeout 60000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 45, in <module>
    try:
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 92, in wr_bbc
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 38, in run
    page.goto("https://www.bbc.com/news",timeout=60000)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 60000ms exceeded.

2024-02-28 15:19:51,251 - root - INFO - crawler rq_wattpad done, time_used:45.27
2024-02-28 15:21:34,144 - root - ERROR - crawler wr_Yahoo failed, time_used:102.89, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 45, in <module>
    try:
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 115, in wr_Yahoo
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 71, in run
    page.goto(full_url)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:28:38,234 - root - INFO - start up
2024-02-28 15:28:53,449 - root - INFO - crawler wr_Yahoo done, time_used:15.21
2024-02-28 15:29:12,054 - root - INFO - start up
2024-02-28 15:29:54,517 - root - INFO - crawler wr_Yahoo done, time_used:42.46
2024-02-28 15:30:57,014 - root - ERROR - crawler wr_bbc failed, time_used:62.5, error:Timeout 60000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 92, in wr_bbc
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 38, in run
    page.goto("https://www.bbc.com/news",timeout=60000)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 60000ms exceeded.

2024-02-28 15:31:48,556 - root - ERROR - crawler wr_quora failed, time_used:51.54, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 113, in wr_quora
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 65, in run
    element=page.get_by_role("link", name=random.choice(choose_list)).click()
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 15864, in click
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_locator.py", line 158, in click
    return await self._frame.click(self._selector, strict=True, **params)
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 494, in click
    await self._channel.send("click", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:32:34,509 - root - INFO - start up
2024-02-28 15:33:24,487 - root - ERROR - crawler wr_bbc failed, time_used:49.98, error:Timeout 30000ms exceeded.,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 92, in wr_bbc
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\BBC\wr.py", line 53, in run
    page.goto(full_url)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 9327, in goto
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_page.py", line 484, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_frame.py", line 149, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TimeoutError: Timeout 30000ms exceeded.

2024-02-28 15:36:35,475 - root - ERROR - crawler wr_quora failed, time_used:190.99, error:Target page, context or browser has been closed,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 46, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 113, in wr_quora
    run(playwright,config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\quora\wr.py", line 82, in run
    box.click()
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_generated.py", line 2110, in click
    self._sync(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_sync_base.py", line 115, in _sync
    return task.result()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_element_handle.py", line 137, in click
    await self._channel.send("click", locals_to_params(locals()))
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 63, in send
    return await self._connection.wrap_api_call(
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 495, in wrap_api_call
    return await cb()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 101, in inner_send
    result = next(iter(done)).result()
playwright._impl._errors.TargetClosedError: Target page, context or browser has been closed

2024-02-28 15:39:38,987 - root - INFO - start up
2024-02-28 15:42:40,881 - root - INFO - crawler wr_bbc done, time_used:181.89
2024-02-28 15:49:46,437 - root - INFO - crawler wr_quora done, time_used:425.56
2024-02-28 15:50:22,217 - root - INFO - crawler wr_Yahoo done, time_used:35.78
2024-02-28 16:43:02,928 - root - INFO - start up
2024-02-28 16:58:35,392 - root - INFO - start up
2024-02-28 17:12:00,260 - root - ERROR - crawler wr_Yahoo failed, time_used:804.87, error:Cannot run the event loop while another loop is running,trackback:Traceback (most recent call last):
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 124, in wr_Yahoo
    #     run(playwright)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 70, in run
    '''//*[@id="nimbus-app"]/section/section/section/article/section[6]/div/div[1]/div/div/ul/li[3]/section/div/a'''
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 70, in run
    '''//*[@id="nimbus-app"]/section/section/section/article/section[6]/div/div[1]/div/div/ul/li[3]/section/div/a'''
  File "F:\anaconda\lib\bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "F:\anaconda\lib\bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ".\cmd_crawl.py", line 49, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 124, in wr_Yahoo
    #     run(playwright)
  File "F:\anaconda\lib\site-packages\playwright\sync_api\_context_manager.py", line 90, in __exit__
    self._connection.stop_sync()
  File "F:\anaconda\lib\site-packages\playwright\_impl\_connection.py", line 283, in stop_sync
    self._loop.run_until_complete(self._transport.wait_until_stopped())
  File "F:\anaconda\lib\asyncio\base_events.py", line 592, in run_until_complete
    self._check_running()
  File "F:\anaconda\lib\asyncio\base_events.py", line 554, in _check_running
    raise RuntimeError(
RuntimeError: Cannot run the event loop while another loop is running

2024-02-28 17:12:09,123 - root - INFO - start up
2024-02-28 17:18:19,079 - root - ERROR - crawler wr_Yahoo failed, time_used:369.95, error:,trackback:Traceback (most recent call last):
  File ".\cmd_crawl.py", line 49, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 131, in wr_Yahoo
    config={}
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 57, in run
    # news_links = page.query_selector_all("a:has(> u.StretchedBox)")
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\Yahoo\wr.py", line 57, in run
    # news_links = page.query_selector_all("a:has(> u.StretchedBox)")
  File "F:\anaconda\lib\bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "F:\anaconda\lib\bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

2024-02-28 17:18:27,245 - root - INFO - start up
2024-02-28 17:19:30,138 - root - INFO - crawler wr_Yahoo done, time_used:62.89
2024-02-28 17:19:36,051 - root - INFO - start up
2024-02-28 17:28:19,740 - root - INFO - crawler wr_Yahoo done, time_used:523.69
2024-02-28 17:30:12,804 - root - INFO - start up
2024-02-28 17:42:58,575 - root - ERROR - crawler rq_arxiv failed, time_used:765.54, error:('Connection aborted.', TimeoutError(10060, 'ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£', None, 10060, None)),trackback:Traceback (most recent call last):
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 670, in urlopen
    httplib_response = self._make_request(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 381, in _make_request
    self._validate_conn(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 978, in _validate_conn
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 362, in connect
    self.sock = ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 386, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 1040, in _create
    self.do_handshake()
  File "F:\anaconda\lib\ssl.py", line 1309, in do_handshake
    self._sslobj.do_handshake()
TimeoutError: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 726, in urlopen
    retries = retries.increment(
  File "F:\anaconda\lib\site-packages\urllib3\util\retry.py", line 410, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "F:\anaconda\lib\site-packages\urllib3\packages\six.py", line 734, in reraise
    raise value.with_traceback(tb)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 670, in urlopen
    httplib_response = self._make_request(
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 381, in _make_request
    self._validate_conn(conn)
  File "F:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 978, in _validate_conn
    conn.connect()
  File "F:\anaconda\lib\site-packages\urllib3\connection.py", line 362, in connect
    self.sock = ssl_wrap_socket(
  File "F:\anaconda\lib\site-packages\urllib3\util\ssl_.py", line 386, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File "F:\anaconda\lib\ssl.py", line 500, in wrap_socket
    return self.sslsocket_class._create(
  File "F:\anaconda\lib\ssl.py", line 1040, in _create
    self.do_handshake()
  File "F:\anaconda\lib\ssl.py", line 1309, in do_handshake
    self._sslobj.do_handshake()
urllib3.exceptions.ProtocolError: ('Connection aborted.', TimeoutError(10060, 'ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£', None, 10060, None))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ".\cmd_crawl.py", line 50, in <module>
    crawler(config)
  File "C:\Users\ZTZ\Desktop\Fresh\fresh_eval\src\arxiv\rq.py", line 162, in rq_arxiv
    response = requests.get(url)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "F:\anaconda\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "F:\anaconda\lib\site-packages\requests\adapters.py", line 501, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', TimeoutError(10060, 'ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£', None, 10060, None))

2024-02-28 18:16:16,113 - root - INFO - start up
2024-02-28 18:32:10,440 - root - INFO - crawler rq_arxiv done, time_used:954.33
